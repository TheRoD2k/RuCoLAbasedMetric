{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "solar-sauce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/bart/.local/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (4.64.0)\n",
      "Requirement already satisfied: ipywidgets in /home/bart/.local/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (7.7.0)\n",
      "Requirement already satisfied: numpy in /home/bart/.local/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (1.22.3)\n",
      "Requirement already satisfied: pandas in /home/bart/.local/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (1.4.2)\n",
      "Requirement already satisfied: torch in /home/bart/.local/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (1.11.0)\n",
      "Requirement already satisfied: sentencepiece in /home/bart/.local/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (0.1.96)\n",
      "Requirement already satisfied: datasets in /home/bart/.local/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (2.1.0)\n",
      "Requirement already satisfied: transformers in /home/bart/.local/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (4.18.0)\n",
      "Requirement already satisfied: sklearn in /home/bart/.local/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/bart/.local/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 7)) (2022.3.0)\n",
      "Requirement already satisfied: xxhash in /home/bart/.local/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 7)) (3.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/bart/.local/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 7)) (0.7.0)\n",
      "Requirement already satisfied: packaging in /home/bart/.local/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 7)) (21.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/bart/.local/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 7)) (2.27.1)\n",
      "Requirement already satisfied: multiprocess in /home/bart/.local/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 7)) (0.70.12.2)\n",
      "Requirement already satisfied: dill in /home/bart/.local/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 7)) (0.3.4)\n",
      "Requirement already satisfied: responses<0.19 in /home/bart/.local/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 7)) (0.18.0)\n",
      "Requirement already satisfied: aiohttp in /home/bart/.local/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 7)) (3.8.1)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /home/bart/.local/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 7)) (8.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/bart/.local/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets->-r requirements.txt (line 7)) (6.0)\n",
      "Requirement already satisfied: filelock in /home/bart/.local/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets->-r requirements.txt (line 7)) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/bart/.local/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets->-r requirements.txt (line 7)) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/bart/.local/lib/python3.9/site-packages (from packaging->datasets->-r requirements.txt (line 7)) (3.0.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/bart/.local/lib/python3.9/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 7)) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bart/.local/lib/python3.9/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 7)) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bart/.local/lib/python3.9/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 7)) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/bart/.local/lib/python3.9/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 7)) (1.26.9)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/bart/.local/lib/python3.9/site-packages (from ipywidgets->-r requirements.txt (line 2)) (5.1.1)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /home/bart/.local/lib/python3.9/site-packages (from ipywidgets->-r requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /home/bart/.local/lib/python3.9/site-packages (from ipywidgets->-r requirements.txt (line 2)) (5.4.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /home/bart/.local/lib/python3.9/site-packages (from ipywidgets->-r requirements.txt (line 2)) (1.1.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /home/bart/.local/lib/python3.9/site-packages (from ipywidgets->-r requirements.txt (line 2)) (6.13.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /home/bart/.local/lib/python3.9/site-packages (from ipywidgets->-r requirements.txt (line 2)) (8.3.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /home/bart/.local/lib/python3.9/site-packages (from ipywidgets->-r requirements.txt (line 2)) (3.6.0)\n",
      "Requirement already satisfied: nest-asyncio in /home/bart/.local/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 2)) (1.5.5)\n",
      "Requirement already satisfied: debugpy>=1.0 in /home/bart/.local/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 2)) (1.6.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/bart/.local/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 2)) (0.1.3)\n",
      "Requirement already satisfied: tornado>=6.1 in /home/bart/.local/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 2)) (6.1)\n",
      "Requirement already satisfied: psutil in /home/bart/.local/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 2)) (5.9.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/bart/.local/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 2)) (7.3.1)\n",
      "Requirement already satisfied: pickleshare in /home/bart/.local/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/bart/.local/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (62.1.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/bart/.local/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (4.8.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/bart/.local/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (2.12.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/bart/.local/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (0.18.1)\n",
      "Requirement already satisfied: stack-data in /home/bart/.local/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: backcall in /home/bart/.local/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: decorator in /home/bart/.local/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (5.1.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/bart/.local/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (3.0.29)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/bart/.local/lib/python3.9/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (0.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/bart/.local/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=22.3 in /home/bart/.local/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 2)) (22.3.0)\n",
      "Requirement already satisfied: entrypoints in /home/bart/.local/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 2)) (0.4)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in /home/bart/.local/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 2)) (4.10.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastjsonschema in /home/bart/.local/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets->-r requirements.txt (line 2)) (2.15.3)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /home/bart/.local/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets->-r requirements.txt (line 2)) (3.2.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/bart/.local/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->-r requirements.txt (line 2)) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/bart/.local/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->-r requirements.txt (line 2)) (0.18.1)\n",
      "Requirement already satisfied: six>=1.11.0 in /home/bart/.local/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/bart/.local/lib/python3.9/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/bart/.local/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (0.2.5)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /home/bart/.local/lib/python3.9/site-packages (from widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (6.4.11)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /home/bart/.local/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (1.8.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/bart/.local/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (0.13.3)\n",
      "Requirement already satisfied: jinja2 in /home/bart/.local/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (3.1.2)\n",
      "Requirement already satisfied: argon2-cffi in /home/bart/.local/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (21.3.0)\n",
      "Requirement already satisfied: nbconvert>=5 in /home/bart/.local/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (6.5.0)\n",
      "Requirement already satisfied: prometheus-client in /home/bart/.local/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (0.14.1)\n",
      "Requirement already satisfied: defusedxml in /home/bart/.local/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/bart/.local/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (0.2.2)\n",
      "Requirement already satisfied: bleach in /home/bart/.local/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (5.0.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/bart/.local/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (4.11.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/bart/.local/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /home/bart/.local/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (0.6.2)\n",
      "Requirement already satisfied: tinycss2 in /home/bart/.local/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (1.1.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /home/bart/.local/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (0.8.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/bart/.local/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (1.5.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/bart/.local/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 4)) (2022.1)\n",
      "Requirement already satisfied: scikit-learn in /home/bart/.local/lib/python3.9/site-packages (from sklearn->-r requirements.txt (line 9)) (1.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/bart/.local/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 8)) (2022.4.24)\n",
      "Requirement already satisfied: sacremoses in /home/bart/.local/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 8)) (0.0.53)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/bart/.local/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 8)) (0.12.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/bart/.local/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/bart/.local/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (1.7.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/bart/.local/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/bart/.local/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/bart/.local/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (6.0.2)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /home/bart/.local/lib/python3.9/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /home/bart/.local/lib/python3.9/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /home/bart/.local/lib/python3.9/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (2.21)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/bart/.local/lib/python3.9/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (2.3.2.post1)\n",
      "Requirement already satisfied: webencodings in /home/bart/.local/lib/python3.9/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 2)) (0.5.1)\n",
      "Requirement already satisfied: joblib in /home/bart/.local/lib/python3.9/site-packages (from sacremoses->transformers->-r requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: click in /home/bart/.local/lib/python3.9/site-packages (from sacremoses->transformers->-r requirements.txt (line 8)) (8.1.3)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/bart/.local/lib/python3.9/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 9)) (1.8.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/bart/.local/lib/python3.9/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 9)) (3.1.0)\n",
      "Requirement already satisfied: pure-eval in /home/bart/.local/lib/python3.9/site-packages (from stack-data->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (0.2.2)\n",
      "Requirement already satisfied: executing in /home/bart/.local/lib/python3.9/site-packages (from stack-data->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /home/bart/.local/lib/python3.9/site-packages (from stack-data->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (2.0.5)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt\n",
    "# !sudo jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e955b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2465b264",
   "metadata": {},
   "source": [
    "# Датасет RuCoLa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e61f5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8edabfa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>authored_by</th>\n",
       "      <th>error_type</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>В 1929 году Ньюкомб переехал на Мальту в качес...</td>\n",
       "      <td>1</td>\n",
       "      <td>Machine</td>\n",
       "      <td>0</td>\n",
       "      <td>WikiMatrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Вдруг решетка беззвучно поехала в сторону, и н...</td>\n",
       "      <td>1</td>\n",
       "      <td>Expert</td>\n",
       "      <td>0</td>\n",
       "      <td>Paducheva2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Этим летом не никуда ездили.</td>\n",
       "      <td>0</td>\n",
       "      <td>Expert</td>\n",
       "      <td>Syntax</td>\n",
       "      <td>Rusgram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>В 2011 году был выпущен документальный фильм «...</td>\n",
       "      <td>1</td>\n",
       "      <td>Machine</td>\n",
       "      <td>0</td>\n",
       "      <td>WikiMatrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Поговорительница Пресли поссорилась с поп-звез...</td>\n",
       "      <td>0</td>\n",
       "      <td>Machine</td>\n",
       "      <td>Morphology</td>\n",
       "      <td>WikiMatrix</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  is_correct authored_by  \\\n",
       "0  В 1929 году Ньюкомб переехал на Мальту в качес...           1     Machine   \n",
       "1  Вдруг решетка беззвучно поехала в сторону, и н...           1      Expert   \n",
       "2                       Этим летом не никуда ездили.           0      Expert   \n",
       "3  В 2011 году был выпущен документальный фильм «...           1     Machine   \n",
       "4  Поговорительница Пресли поссорилась с поп-звез...           0     Machine   \n",
       "\n",
       "   error_type         source  \n",
       "0           0     WikiMatrix  \n",
       "1           0  Paducheva2004  \n",
       "2      Syntax        Rusgram  \n",
       "3           0     WikiMatrix  \n",
       "4  Morphology     WikiMatrix  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = pd.read_csv(\"./rucola/train.tsv\", sep=\"\\t\", names=[\"text\", \"is_correct\", \"authored_by\", \"error_type\", \"source\"])\n",
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "924c530c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>authored_by</th>\n",
       "      <th>error_type</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Иван вчера не позвонил.</td>\n",
       "      <td>1</td>\n",
       "      <td>Expert</td>\n",
       "      <td>0</td>\n",
       "      <td>Paducheva2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>У многих туристов, кто посещают Кемер весной, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Expert</td>\n",
       "      <td>Syntax</td>\n",
       "      <td>USE8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Я бы изобрела транспорт, который бы с сильной ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Lexis</td>\n",
       "      <td>Tseitlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Когда он заходит на пол американского обмена в...</td>\n",
       "      <td>0</td>\n",
       "      <td>Machine</td>\n",
       "      <td>Hallucination</td>\n",
       "      <td>TED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Американский телесериал, посвященный Хэллоуину...</td>\n",
       "      <td>0</td>\n",
       "      <td>Machine</td>\n",
       "      <td>Hallucination</td>\n",
       "      <td>WikiMatrix</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  is_correct authored_by  \\\n",
       "0                            Иван вчера не позвонил.           1      Expert   \n",
       "1  У многих туристов, кто посещают Кемер весной, ...           0      Expert   \n",
       "2  Я бы изобрела транспорт, который бы с сильной ...           0     Natural   \n",
       "3  Когда он заходит на пол американского обмена в...           0     Machine   \n",
       "4  Американский телесериал, посвященный Хэллоуину...           0     Machine   \n",
       "\n",
       "      error_type         source  \n",
       "0              0  Paducheva2013  \n",
       "1         Syntax           USE8  \n",
       "2          Lexis       Tseitlin  \n",
       "3  Hallucination            TED  \n",
       "4  Hallucination     WikiMatrix  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = pd.read_csv(\"./rucola/dev.tsv\", sep=\"\\t\", names=[\"text\", \"is_correct\", \"authored_by\", \"error_type\", \"source\"])\n",
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e26830d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12917\n",
      "1612\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_LENGTH = len(train_dataset)\n",
    "print(TRAIN_DATA_LENGTH)\n",
    "TEST_DATA_LENGTH = len(test_dataset)\n",
    "print(TEST_DATA_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ede52e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Mitrenina', 'Lutikova', 'Paducheva2004', 'WikiMatrix', 'Tatoeba', 'CoRST', 'USE6', 'USE5', 'Paducheva2013', 'Seliverstova', 'USE7', 'Paducheva1996', 'USE8', 'Tseitlin', 'YandexCorpus', 'Rusgram', 'Testelets', 'TED'}\n"
     ]
    }
   ],
   "source": [
    "sources = set()\n",
    "for value in train_dataset[\"source\"]:\n",
    "    sources.add(value)\n",
    "for value in test_dataset[\"source\"]:\n",
    "    sources.add(value)\n",
    "print(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2da212db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.398\n",
      "0.396\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for value in train_dataset[\"is_correct\"]:\n",
    "    if value == 0:\n",
    "        counter += 1\n",
    "print(f\"{counter / TRAIN_DATA_LENGTH:.3f}\")\n",
    "\n",
    "counter = 0\n",
    "for value in test_dataset[\"is_correct\"]:\n",
    "    if value == 0:\n",
    "        counter += 1\n",
    "print(f\"{counter / TEST_DATA_LENGTH:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b981727",
   "metadata": {},
   "source": [
    "Итого предложений с ошибками 40%. Датасет в исходнои виде подходит для бинарной классификации грамматичности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11f0f86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Commonsense', 'Lexis', 'Morphology', 'Hallucination', 'Syntax', 'Semantics'}\n",
      "{'Commonsense', 'Lexis', 'Morphology', 'Hallucination', 'Syntax', 'Semantics'}\n"
     ]
    }
   ],
   "source": [
    "error_types = set()\n",
    "for value in train_dataset[\"error_type\"]:\n",
    "    error_types.update([*value.split(\"|\")])\n",
    "error_types.remove('0')\n",
    "print(error_types)\n",
    "\n",
    "error_types = set()\n",
    "for value in test_dataset[\"error_type\"]:\n",
    "    error_types.update([*value.split(\"|\")])\n",
    "error_types.remove('0')\n",
    "print(error_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dddaec3",
   "metadata": {},
   "source": [
    "Итого 6 типов ошибок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a2ce58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commonsense: 0.010\n",
      "Lexis: 0.085\n",
      "Morphology: 0.056\n",
      "Hallucination: 0.037\n",
      "Syntax: 0.190\n",
      "Semantics: 0.075\n",
      "\n",
      "\n",
      "Commonsense: 0.009\n",
      "Lexis: 0.085\n",
      "Morphology: 0.055\n",
      "Hallucination: 0.034\n",
      "Syntax: 0.192\n",
      "Semantics: 0.074\n"
     ]
    }
   ],
   "source": [
    "error_types_stats = {}\n",
    "for error_type in error_types:\n",
    "    error_types_stats[error_type] = 0\n",
    "\n",
    "for value in train_dataset[\"error_type\"]:\n",
    "    if value == '0':\n",
    "        continue\n",
    "    for error_type in value.split(\"|\"):\n",
    "        error_types_stats[error_type] += 1\n",
    "\n",
    "for error_type in error_types:\n",
    "    print(f\"{error_type}: {error_types_stats[error_type] / TRAIN_DATA_LENGTH:.3f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "    \n",
    "error_types_stats = {}\n",
    "for error_type in error_types:\n",
    "    error_types_stats[error_type] = 0\n",
    "\n",
    "for value in test_dataset[\"error_type\"]:\n",
    "    if value == '0':\n",
    "        continue\n",
    "    for error_type in value.split(\"|\"):\n",
    "        error_types_stats[error_type] += 1\n",
    "\n",
    "for error_type in error_types:\n",
    "    print(f\"{error_type}: {error_types_stats[error_type] / TEST_DATA_LENGTH:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1dd0167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.258\n",
      "0.257\n"
     ]
    }
   ],
   "source": [
    "machine_counter = 0\n",
    "for value in train_dataset[\"authored_by\"]:\n",
    "    if value == \"Machine\":\n",
    "        machine_counter += 1\n",
    "\n",
    "print(f\"{machine_counter / TRAIN_DATA_LENGTH:.3f}\")\n",
    "\n",
    "\n",
    "machine_counter = 0\n",
    "for value in test_dataset[\"authored_by\"]:\n",
    "    if value == \"Machine\":\n",
    "        machine_counter += 1\n",
    "\n",
    "print(f\"{machine_counter / TEST_DATA_LENGTH:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbe5083",
   "metadata": {},
   "source": [
    "26% предложений в датасете сгенерированы машиной"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899f0868",
   "metadata": {},
   "source": [
    "## Сводка по датасету"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b8045a",
   "metadata": {},
   "source": [
    "Длина тренировочной части датасета: 12917\n",
    "Длина тестовой части датасета: 1612\n",
    "\n",
    "\n",
    "Доля предложений с ошибкой: 0.4\n",
    "\n",
    "Виды ошибок: {'Semantics', 'Commonsense', 'Morphology', 'Hallucination', 'Syntax', 'Lexis'}\n",
    "\n",
    "Доля каждого вида ошибок (взяты с тренировочной части; совпадают в пределах погрешности с тестовой частью):\n",
    "    Semantics: 0.075\n",
    "    Commonsense: 0.010\n",
    "    Morphology: 0.056\n",
    "    Hallucination: 0.037\n",
    "    Syntax: 0.190\n",
    "    Lexis: 0.085\n",
    "   \n",
    "Доля сгенерированных машиной предложений: 0.258"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694ef90a",
   "metadata": {},
   "source": [
    "# Классификаторы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4542b424",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.to_csv(\"./rucola/train.csv\")\n",
    "test_dataset.to_csv(\"./rucola/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d8d5a0",
   "metadata": {},
   "source": [
    "## Классификатор грамматичности"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e18d31",
   "metadata": {},
   "source": [
    "Основной классификатор, определяющий наличие ошибки любого рода в предложении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "161d849d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-081ceb0bf9c7c7cd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/bart/.cache/huggingface/datasets/csv/default-081ceb0bf9c7c7cd/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b837a768b1494d8144a3e5c91df097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71355624bace4b9682b7a519797c1d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/bart/.cache/huggingface/datasets/csv/default-081ceb0bf9c7c7cd/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6066d1bb2eca4c97afb0177fe56a1168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'Он перецеловал всех девушек.', 'label': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('csv', data_files={'train': './rucola/train.csv', 'test': './rucola/test.csv'})\n",
    "column_names = dataset[\"train\"].column_names\n",
    "column_names.remove(\"is_correct\")\n",
    "column_names.remove(\"text\")\n",
    "dataset[\"train\"] = dataset[\"train\"].remove_columns(column_names)\n",
    "dataset[\"test\"] = dataset[\"test\"].remove_columns(column_names)\n",
    "dataset[\"train\"] = dataset[\"train\"].rename_column(\"is_correct\", \"label\")\n",
    "dataset[\"test\"] = dataset[\"test\"].rename_column(\"is_correct\", \"label\")\n",
    "dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf0184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"]\n",
    "small_eval_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc8f025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cointegrated/rubert-tiny2\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f319a5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95c2eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b1d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", per_device_train_batch_size=2, per_device_eval_batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a646c77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df859361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d650ead",
   "metadata": {},
   "source": [
    "Epoch\t    Training Loss   Validation Loss    \tAccuracy\n",
    "\n",
    "1\t             0.820300\t       0.684045\t               0.688586\n",
    "\n",
    "2\t             0.793900\t       1.073257\t               0.683623\n",
    "\n",
    "3\t             0.750300\t       1.383223            \t   0.690447"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04c8bc7",
   "metadata": {},
   "source": [
    "Очевидно, модель переобучилась. Попробуем исправить это недоразумение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef24102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('csv', data_files={'train': './rucola/train.csv', 'test': './rucola/test.csv'})\n",
    "\n",
    "column_names = dataset[\"train\"].column_names\n",
    "column_names.remove(\"is_correct\")\n",
    "column_names.remove(\"text\")\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].remove_columns(column_names)\n",
    "dataset[\"test\"] = dataset[\"test\"].remove_columns(column_names)\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].rename_column(\"is_correct\", \"label\")\n",
    "dataset[\"test\"] = dataset[\"test\"].rename_column(\"is_correct\", \"label\")\n",
    "\n",
    "dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e505fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "from transformers import TrainingArguments, Trainer, AutoTokenizer\n",
    "\n",
    "\n",
    "# First make the kfold object\n",
    "folds = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Then get the dataset\n",
    "datasets = dataset\n",
    "\n",
    "# Now make our splits based off of the labels. \n",
    "# We can use `np.zeros()` here since it only works off of indices, we really care about the labels\n",
    "splits = folds.split(np.zeros(datasets[\"train\"].num_rows), datasets[\"train\"][\"label\"])\n",
    "\n",
    "# Finally, do what you want with it\n",
    "# In this case I'm overriding the train/val/test\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cointegrated/rubert-tiny2\", num_labels=2, attention_probs_dropout_prob=0.3,\n",
    "        hidden_dropout_prob=0.3)\n",
    "f1_metric = load_metric(\"f1\")\n",
    "precision_metric = load_metric(\"precision\")\n",
    "recall_metric = load_metric(\"recall\")\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer_cross_val\", evaluation_strategy=\"epoch\", learning_rate=3e-6, num_train_epochs=3, per_device_train_batch_size=2, per_device_eval_batch_size=2)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "\n",
    "for train_idxs, val_idxs in splits:\n",
    "    datasets = copy.deepcopy(dataset)\n",
    "    #datasets[\"test\"] = datasets[\"validation\"]\n",
    "    datasets[\"test\"] = datasets[\"train\"].select(val_idxs)\n",
    "    datasets[\"train\"] = datasets[\"train\"].select(train_idxs)\n",
    "\n",
    "    tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "    small_train_dataset = tokenized_datasets[\"train\"]\n",
    "    small_eval_dataset = tokenized_datasets[\"test\"]\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=small_train_dataset,\n",
    "        eval_dataset=small_eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fe8696",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(iter(splits))\n",
    "print(a[0].shape)\n",
    "print(a[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bba6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "div#notebook-container { width: 95%; }\n",
    "div#menubar-container { width: 65%; }\n",
    "div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4394b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "from transformers import TrainingArguments, Trainer, AutoTokenizer\n",
    "from transformers.utils import logging\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# First make the kfold object\n",
    "# folds = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Then get the dataset\n",
    "!export TRANSFORMERS_NO_ADVISORY_WARNINGS=1\n",
    "datasets = dataset\n",
    "\n",
    "# Now make our splits based off of the labels. \n",
    "# We can use `np.zeros()` here since it only works off of indices, we really care about the labels\n",
    "# splits = folds.split(np.zeros(datasets[\"train\"].num_rows), datasets[\"train\"][\"label\"])\n",
    "\n",
    "# Finally, do what you want with it\n",
    "# In this case I'm overriding the train/val/test\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"cointegrated/rubert-tiny2\",\n",
    "    num_labels=2,\n",
    "    attention_probs_dropout_prob=0.3,\n",
    "    hidden_dropout_prob=0.3,\n",
    "    classifier_dropout=0.3\n",
    ")\n",
    "\n",
    "f1_metric = load_metric(\"f1\")\n",
    "precision_metric = load_metric(\"precision\")\n",
    "recall_metric = load_metric(\"recall\")\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer_cross_val\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=18,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=3e-7,\n",
    "    # log_level=\"error\",\n",
    "    logging_first_step=True,\n",
    ")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "\n",
    "\n",
    "datasets = copy.deepcopy(dataset)\n",
    "# #datasets[\"test\"] = datasets[\"validation\"]\n",
    "# datasets[\"test\"] = datasets[\"test\"]\n",
    "# datasets[\"train\"] = datasets[\"train\"]\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"]\n",
    "small_eval_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Start training\")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abdc338",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer_cross_val\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=1e-7,\n",
    "    # log_level=\"error\",\n",
    "    logging_first_step=True,\n",
    ")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "\n",
    "\n",
    "datasets = copy.deepcopy(dataset)\n",
    "# #datasets[\"test\"] = datasets[\"validation\"]\n",
    "# datasets[\"test\"] = datasets[\"test\"]\n",
    "# datasets[\"train\"] = datasets[\"train\"]\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"]\n",
    "small_eval_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Start training\")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838f202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-study",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers sentencepiece\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "model.cuda()  # uncomment it if you have a GPU\n",
    "\n",
    "def embed_bert_cls(text, model, tokenizer):\n",
    "    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    print(t)\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings[0].cpu().numpy()\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(embed_bert_cls('привет, мир', model, tokenizer) == embed_bert_cls('привет мир', model, tokenizer))\n",
    "# (312,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e6e82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc394c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_error_model = AutoModelForSequenceClassification.from_pretrained(\"./test_trainer_cross_val/checkpoint-26000\")\n",
    "contains_error_tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47d5283",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContainsErrorClassifier:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        print(f\"Model device: {self.device}\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        \n",
    "    def _gen_batch(self, inputs, batch_size):\n",
    "        batch_start = 0\n",
    "        while batch_start < len(inputs):\n",
    "            yield inputs[batch_start: batch_start + batch_size]\n",
    "            batch_start += batch_size\n",
    "\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        input_records,\n",
    "        max_source_tokens_count=600,\n",
    "        batch_size=4,\n",
    "        print_batch_results=False,\n",
    "        save_in_file=False,\n",
    "        output_file=\"MBartSummarizerOutput.txt\"\n",
    "    ):\n",
    "        labels = []\n",
    "        for input_record in input_records:\n",
    "            inputs = self.tokenizer(input_record, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(**inputs).logits\n",
    "            # print(logits)\n",
    "\n",
    "            predicted_class_id = logits.argmax().item()\n",
    "            # print(predicted_class_id)\n",
    "            labels.append(1 if self.model.config.id2label[predicted_class_id] == 'LABEL_1' else 0)\n",
    "        \n",
    "        return labels\n",
    "    \n",
    "    def save(self, predictions, output_file, output_mode=\"w\"):\n",
    "        with open(output_file, output_mode) as w:\n",
    "            for p in predictions:\n",
    "                w.write(p.strip().replace(\"\\n\", \" \") + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55af28ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_error_model = AutoModelForSequenceClassification.from_pretrained(\"./test_trainer_cross_val/checkpoint-26000\", num_labels=2)\n",
    "contains_error_model.cuda()\n",
    "contains_error_tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe433f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "HasErrorClassifier = ContainsErrorClassifier(contains_error_model, contains_error_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f7bc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"][\"text\"][0:10], datasets[\"train\"][\"label\"][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9042fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(HasErrorClassifier.predict(datasets[\"test\"][\"text\"][0:100]))\n",
    "print(datasets[\"test\"][\"label\"][0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1281110",
   "metadata": {},
   "source": [
    "# Syntax Error Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09134b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-081ceb0bf9c7c7cd\n",
      "Reusing dataset csv (/home/bart/.cache/huggingface/datasets/csv/default-081ceb0bf9c7c7cd/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28660e0ce94340b2ac2fa85739ce8caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('csv', data_files={'train': './rucola/train.csv', 'test': './rucola/test.csv'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab80544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_type = \"Syntax\"\n",
    "syntax_train_column = []\n",
    "syntax_test_column = []\n",
    "for row in dataset[\"train\"]:\n",
    "    if error_type in row[\"error_type\"]:\n",
    "        syntax_train_column.append(1)\n",
    "    else:\n",
    "        syntax_train_column.append(0)\n",
    "        \n",
    "for row in dataset[\"test\"]:\n",
    "    if error_type in row[\"error_type\"]:\n",
    "        syntax_test_column.append(1)\n",
    "    else:\n",
    "        syntax_test_column.append(0)\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].add_column(\"label\", syntax_train_column)\n",
    "dataset[\"test\"] = dataset[\"test\"].add_column(\"label\", syntax_test_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77030348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 10,\n",
       " 'text': 'Мне предоставилась возможность все видеть, сам оставаясь незамеченным.',\n",
       " 'is_correct': 0,\n",
       " 'authored_by': 'Expert',\n",
       " 'error_type': 'Syntax',\n",
       " 'source': 'Testelets',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bcc71ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Мне предоставилась возможность все видеть, сам оставаясь незамеченным.',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = dataset[\"train\"].column_names\n",
    "column_names.remove(\"label\")\n",
    "column_names.remove(\"text\")\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].remove_columns(column_names)\n",
    "dataset[\"test\"] = dataset[\"test\"].remove_columns(column_names)\n",
    "\n",
    "dataset[\"train\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aa2262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "from transformers import TrainingArguments, Trainer, AutoTokenizer\n",
    "from transformers.utils import logging\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# First make the kfold object\n",
    "# folds = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Then get the dataset\n",
    "!export TRANSFORMERS_NO_ADVISORY_WARNINGS=1\n",
    "datasets = dataset\n",
    "\n",
    "# Now make our splits based off of the labels. \n",
    "# We can use `np.zeros()` here since it only works off of indices, we really care about the labels\n",
    "# splits = folds.split(np.zeros(datasets[\"train\"].num_rows), datasets[\"train\"][\"label\"])\n",
    "\n",
    "# Finally, do what you want with it\n",
    "# In this case I'm overriding the train/val/test\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"cointegrated/rubert-tiny2\",\n",
    "    num_labels=2,\n",
    "    attention_probs_dropout_prob=0.3,\n",
    "    hidden_dropout_prob=0.3,\n",
    "    classifier_dropout=0.3\n",
    ")\n",
    "\n",
    "f1_metric = load_metric(\"f1\")\n",
    "precision_metric = load_metric(\"precision\")\n",
    "recall_metric = load_metric(\"recall\")\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"syntax_error_classifier_checkpoints\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=18,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=1e-5,\n",
    "    # log_level=\"error\",\n",
    "    logging_first_step=True,\n",
    ")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "\n",
    "\n",
    "datasets = copy.deepcopy(dataset)\n",
    "# #datasets[\"test\"] = datasets[\"validation\"]\n",
    "# datasets[\"test\"] = datasets[\"test\"]\n",
    "# datasets[\"train\"] = datasets[\"train\"]\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"]\n",
    "small_eval_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Start training\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bda1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "from transformers import TrainingArguments, Trainer, AutoTokenizer\n",
    "from transformers.utils import logging\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# First make the kfold object\n",
    "# folds = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Then get the dataset\n",
    "!export TRANSFORMERS_NO_ADVISORY_WARNINGS=1\n",
    "datasets = dataset\n",
    "\n",
    "# Now make our splits based off of the labels. \n",
    "# We can use `np.zeros()` here since it only works off of indices, we really care about the labels\n",
    "# splits = folds.split(np.zeros(datasets[\"train\"].num_rows), datasets[\"train\"][\"label\"])\n",
    "\n",
    "# Finally, do what you want with it\n",
    "# In this case I'm overriding the train/val/test\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"cointegrated/rubert-tiny2\",\n",
    "    num_labels=2,\n",
    "    attention_probs_dropout_prob=0.3,\n",
    "    hidden_dropout_prob=0.3,\n",
    "    classifier_dropout=0.3\n",
    ")\n",
    "\n",
    "f1_metric = load_metric(\"f1\")\n",
    "precision_metric = load_metric(\"precision\")\n",
    "recall_metric = load_metric(\"recall\")\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"syntax_error_1e6_classifier_checkpoints\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=18,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=1e-6,\n",
    "    # log_level=\"error\",\n",
    "    logging_first_step=True,\n",
    ")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "\n",
    "\n",
    "datasets = copy.deepcopy(dataset)\n",
    "# #datasets[\"test\"] = datasets[\"validation\"]\n",
    "# datasets[\"test\"] = datasets[\"test\"]\n",
    "# datasets[\"train\"] = datasets[\"train\"]\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"]\n",
    "small_eval_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Start training\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5110637",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"syntax_error_1e6_classifier_checkpoints\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=40,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=1e-6,\n",
    "    # log_level=\"error\",\n",
    "    logging_first_step=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Continue training\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1c0e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71140591",
   "metadata": {},
   "source": [
    "Saving model checkpoint to syntax_error_1e6_classifier_checkpoints/checkpoint-58500\n",
    "Configuration saved in syntax_error_1e6_classifier_checkpoints/checkpoint-58500/config.json\n",
    "Model weights saved in syntax_error_1e6_classifier_checkpoints/checkpoint-58500/pytorch_model.bin\n",
    "\n",
    "Saving model checkpoint to syntax_error_1e6_classifier_checkpoints/checkpoint-52000\n",
    "Configuration saved in syntax_error_1e6_classifier_checkpoints/checkpoint-52000/config.json\n",
    "Model weights saved in syntax_error_1e6_classifier_checkpoints/checkpoint-52000/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "115268ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./syntax_error_1e6_classifier_checkpoints/checkpoint-52000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./syntax_error_1e6_classifier_checkpoints/checkpoint-52000\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.3,\n",
      "  \"classifier_dropout\": 0.3,\n",
      "  \"emb_size\": 312,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.3,\n",
      "  \"hidden_size\": 312,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 600,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 83828\n",
      "}\n",
      "\n",
      "loading weights file ./syntax_error_1e6_classifier_checkpoints/checkpoint-52000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./syntax_error_1e6_classifier_checkpoints/checkpoint-52000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/vocab.txt from cache at /home/bart/.cache/huggingface/transformers/16af2afaa4ceaa8d50b689bd4c2f7ef7fe3bfac06c0aac7d82a5c1c72298b62a.cc3312d07ccf88871a3c2b7cb3442138e6785101efead94d9f77e96301cf7f4a\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/tokenizer.json from cache at /home/bart/.cache/huggingface/transformers/cfa6d82dc8ecc7fe3f06deb449f38968cdd188bb84c8da0e06f0bbfddbede1e3.550ab7157d36210bf96c7c3b30e621933d37d635c5f2e290f7e88bd5f7c9198a\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/special_tokens_map.json from cache at /home/bart/.cache/huggingface/transformers/20317640533199c6b37a557395cd5ee5fcb8777be7c89bb1314bfd43058b35e9.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/tokenizer_config.json from cache at /home/bart/.cache/huggingface/transformers/e818c3e83969c6aa46da3b5b2eafe049b197e0e787503bf7c643ec64422a51fc.1cec470309dd45bda58f63ce3bb829fe84e2a93e1fc2857ceff76e77262d7944\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "from transformers import TrainingArguments, Trainer, AutoTokenizer\n",
    "from transformers.utils import logging\n",
    "\n",
    "contains_syntax_error_model = AutoModelForSequenceClassification.from_pretrained(\"./syntax_error_1e6_classifier_checkpoints/checkpoint-52000\", num_labels=2)\n",
    "contains_syntax_error_model.cuda()\n",
    "contains_syntax_error_tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f520c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/vocab.txt from cache at /home/bart/.cache/huggingface/transformers/16af2afaa4ceaa8d50b689bd4c2f7ef7fe3bfac06c0aac7d82a5c1c72298b62a.cc3312d07ccf88871a3c2b7cb3442138e6785101efead94d9f77e96301cf7f4a\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/tokenizer.json from cache at /home/bart/.cache/huggingface/transformers/cfa6d82dc8ecc7fe3f06deb449f38968cdd188bb84c8da0e06f0bbfddbede1e3.550ab7157d36210bf96c7c3b30e621933d37d635c5f2e290f7e88bd5f7c9198a\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/special_tokens_map.json from cache at /home/bart/.cache/huggingface/transformers/20317640533199c6b37a557395cd5ee5fcb8777be7c89bb1314bfd43058b35e9.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/tokenizer_config.json from cache at /home/bart/.cache/huggingface/transformers/e818c3e83969c6aa46da3b5b2eafe049b197e0e787503bf7c643ec64422a51fc.1cec470309dd45bda58f63ce3bb829fe84e2a93e1fc2857ceff76e77262d7944\n",
      "Loading cached processed dataset at /home/bart/.cache/huggingface/datasets/csv/default-081ceb0bf9c7c7cd/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-1d7cc18ab24d07c4.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f9a46978b0b43c4a85b9c1977680a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/bart/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12917\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 129180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='102473' max='129180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [102473/129180 5:35:52 < 1:27:32, 5.08 it/s, Epoch 15.86/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.642700</td>\n",
       "      <td>0.665467</td>\n",
       "      <td>0.826303</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.281553</td>\n",
       "      <td>0.383260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.676500</td>\n",
       "      <td>0.666299</td>\n",
       "      <td>0.826303</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.281553</td>\n",
       "      <td>0.383260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.671100</td>\n",
       "      <td>0.666341</td>\n",
       "      <td>0.826303</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.281553</td>\n",
       "      <td>0.383260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.570200</td>\n",
       "      <td>0.666733</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.602740</td>\n",
       "      <td>0.284790</td>\n",
       "      <td>0.386813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.608800</td>\n",
       "      <td>0.667123</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.602740</td>\n",
       "      <td>0.284790</td>\n",
       "      <td>0.386813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.605900</td>\n",
       "      <td>0.667563</td>\n",
       "      <td>0.826303</td>\n",
       "      <td>0.598639</td>\n",
       "      <td>0.284790</td>\n",
       "      <td>0.385965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.635600</td>\n",
       "      <td>0.668085</td>\n",
       "      <td>0.826303</td>\n",
       "      <td>0.598639</td>\n",
       "      <td>0.284790</td>\n",
       "      <td>0.385965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.669400</td>\n",
       "      <td>0.668414</td>\n",
       "      <td>0.826303</td>\n",
       "      <td>0.598639</td>\n",
       "      <td>0.284790</td>\n",
       "      <td>0.385965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.634400</td>\n",
       "      <td>0.668707</td>\n",
       "      <td>0.826303</td>\n",
       "      <td>0.598639</td>\n",
       "      <td>0.284790</td>\n",
       "      <td>0.385965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.685600</td>\n",
       "      <td>0.668980</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.601351</td>\n",
       "      <td>0.288026</td>\n",
       "      <td>0.389497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.618700</td>\n",
       "      <td>0.669151</td>\n",
       "      <td>0.826303</td>\n",
       "      <td>0.598639</td>\n",
       "      <td>0.284790</td>\n",
       "      <td>0.385965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.569700</td>\n",
       "      <td>0.669334</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.601351</td>\n",
       "      <td>0.288026</td>\n",
       "      <td>0.389497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.552400</td>\n",
       "      <td>0.669201</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.601351</td>\n",
       "      <td>0.288026</td>\n",
       "      <td>0.389497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.640100</td>\n",
       "      <td>0.669503</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.601351</td>\n",
       "      <td>0.288026</td>\n",
       "      <td>0.389497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.623000</td>\n",
       "      <td>0.669354</td>\n",
       "      <td>0.827543</td>\n",
       "      <td>0.604027</td>\n",
       "      <td>0.291262</td>\n",
       "      <td>0.393013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-1000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-1000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-1500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-1500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-2000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-2000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-2500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-2500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-3000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-3000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-3500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-3500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-4000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-4000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-4500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-4500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-5000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-5000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-5500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-5500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-6000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-6000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-6000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-6500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-6500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-7000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-7000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-7500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-7500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-8000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-8000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-8500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-8500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-9000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-9000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-9500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-9500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-10000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-10000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-10500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-10500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-11000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-11000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-11000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-11500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-11500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-11500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-12000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-12000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-12000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-12500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-12500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-12500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-13000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-13000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-13000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-13500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-13500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-13500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-14000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-14000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-14000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-14500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-14500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-14500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-15000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-15000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-15000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-15500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-15500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-15500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-16000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-16000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-16000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-16500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-16500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-16500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-17000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-17000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-17000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-17500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-17500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-17500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-18000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-18000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-18000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-18500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-18500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-18500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-19000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-19000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-19000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-19500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-19500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-19500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-20000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-20000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-20000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-20500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-20500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-20500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-21000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-21000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-21000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-21500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-21500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-21500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-22000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-22000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-22000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-22500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-22500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-22500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-23000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-23000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-23000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-23500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-23500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-23500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-24000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-24000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-24000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-24500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-24500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-24500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-25000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-25000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-25000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-25500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-25500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-25500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-26000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-26000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-26000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-26500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-26500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-26500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-27000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-27000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-27000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-27500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-27500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-27500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-28000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-28000/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-28000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-28500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-28500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-28500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-29000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-29000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-29000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-29500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-29500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-29500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-30000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-30000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-30000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-30500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-30500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-30500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-31000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-31000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-31000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-31500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-31500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-31500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-32000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-32000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-32000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-32500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-32500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-32500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-33000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-33000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-33000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-33500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-33500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-33500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-34000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-34000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-34000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-34500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-34500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-34500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-35000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-35000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-35000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-35500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-35500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-35500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-36000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-36000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-36000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-36500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-36500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-36500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-37000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-37000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-37000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-37500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-37500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-37500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-38000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-38000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-38000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-38500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-38500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-38500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-39000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-39000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-39000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-39500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-39500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-39500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-40000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-40000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-40000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-40500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-40500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-40500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-41000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-41000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-41000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-41500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-41500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-41500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-42000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-42000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-42000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-42500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-42500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-42500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-43000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-43000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-43000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-43500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-43500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-43500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-44000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-44000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-44000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-44500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-44500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-44500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-45000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-45000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-45000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-45500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-45500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-45500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-46000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-46000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-46000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-46500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-46500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-46500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-47000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-47000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-47000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-47500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-47500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-47500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-48000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-48000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-48000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-48500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-48500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-48500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-49000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-49000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-49000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-49500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-49500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-49500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-50000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-50000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-50000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-50500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-50500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-50500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-51000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-51000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-51000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-51500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-51500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-51500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-52000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-52000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-52000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-52500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-52500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-52500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-53000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-53000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-53000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-53500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-53500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-53500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-54000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-54000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-54000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-54500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-54500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-54500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-55000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-55000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-55000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-55500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-55500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-55500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-56000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-56000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-56000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-56500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-56500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-56500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-57000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-57000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-57000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-57500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-57500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-57500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-58000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-58000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-58000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-58500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-58500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-58500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-59000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-59000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-59000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-59500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-59500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-59500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-60000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-60000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-60000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-60500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-60500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-60500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-61000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-61000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-61000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-61500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-61500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-61500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-62000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-62000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-62000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-62500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-62500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-62500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-63000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-63000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-63000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-63500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-63500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-63500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-64000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-64000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-64000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-64500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-64500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-64500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-65000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-65000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-65000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-65500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-65500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-65500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-66000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-66000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-66000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-66500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-66500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-66500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-67000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-67000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-67000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-67500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-67500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-67500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-68000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-68000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-68000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-68500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-68500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-68500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-69000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-69000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-69000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-69500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-69500/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-69500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-70000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-70000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-70000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-70500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-70500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-70500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-71000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-71000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-71000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-71500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-71500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-71500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-72000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-72000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-72000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-72500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-72500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-72500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-73000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-73000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-73000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-73500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-73500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-73500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-74000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-74000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-74000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-74500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-74500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-74500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-75000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-75000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-75000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-75500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-75500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-75500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-76000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-76000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-76000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-76500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-76500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-76500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-77000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-77000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-77000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-77500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-77500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-77500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-78000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-78000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-78000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-78500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-78500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-78500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-79000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-79000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-79000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-79500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-79500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-79500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-80000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-80000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-80000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-80500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-80500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-80500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-81000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-81000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-81000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-81500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-81500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-81500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-82000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-82000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-82000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-82500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-82500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-82500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-83000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-83000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-83000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-83500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-83500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-83500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-84000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-84000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-84000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-84500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-84500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-84500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-85000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-85000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-85000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-85500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-85500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-85500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-86000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-86000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-86000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-86500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-86500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-86500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-87000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-87000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-87000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-87500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-87500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-87500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-88000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-88000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-88000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-88500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-88500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-88500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-89000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-89000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-89000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-89500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-89500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-89500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-90000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-90000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-90000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-90500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-90500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-90500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-91000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-91000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-91000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-91500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-91500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-91500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-92000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-92000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-92000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-92500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-92500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-92500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-93000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-93000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-93000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-93500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-93500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-93500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-94000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-94000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-94000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-94500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-94500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-94500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-95000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-95000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-95000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-95500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-95500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-95500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-96000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-96000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-96000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-96500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-96500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-96500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-97000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-97000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-97000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-97500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-97500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-97500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-98000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-98000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-98000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-98500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-98500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-98500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-99000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-99000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-99000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-99500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-99500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-99500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-100000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-100000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-100000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-100500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-100500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-100500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-101000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-101000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-101000/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-101500\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-101500/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-101500/pytorch_model.bin\n",
      "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-102000\n",
      "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-102000/config.json\n",
      "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-102000/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "from transformers import TrainingArguments, Trainer, AutoTokenizer\n",
    "from transformers.utils import logging\n",
    "\n",
    "\n",
    "\n",
    "# Then get the dataset\n",
    "!export TRANSFORMERS_NO_ADVISORY_WARNINGS=1\n",
    "datasets = dataset\n",
    "\n",
    "\n",
    "f1_metric = load_metric(\"f1\")\n",
    "precision_metric = load_metric(\"precision\")\n",
    "recall_metric = load_metric(\"recall\")\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"syntax_error_1e7_classifier_checkpoints\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=3e-8,\n",
    "    # log_level=\"error\",\n",
    "    logging_first_step=True,\n",
    ")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "\n",
    "\n",
    "datasets = copy.deepcopy(dataset)\n",
    "# #datasets[\"test\"] = datasets[\"validation\"]\n",
    "# datasets[\"test\"] = datasets[\"test\"]\n",
    "# datasets[\"train\"] = datasets[\"train\"]\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"]\n",
    "small_eval_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=contains_syntax_error_model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "print(\"Start training\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa51368",
   "metadata": {},
   "source": [
    "Saving model checkpoint to syntax_error_1e7_classifier_checkpoints/checkpoint-6500\n",
    "Configuration saved in syntax_error_1e7_classifier_checkpoints/checkpoint-6500/config.json\n",
    "Model weights saved in syntax_error_1e7_classifier_checkpoints/checkpoint-6500/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2062665c",
   "metadata": {},
   "source": [
    "# Semantics Error Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6409779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-081ceb0bf9c7c7cd\n",
      "Reusing dataset csv (/home/bart/.cache/huggingface/datasets/csv/default-081ceb0bf9c7c7cd/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d956844a31114f78bd732ed3f3beba03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('csv', data_files={'train': './rucola/train.csv', 'test': './rucola/test.csv'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af2483b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_type = \"Semantics\"\n",
    "syntax_train_column = []\n",
    "syntax_test_column = []\n",
    "for row in dataset[\"train\"]:\n",
    "    if error_type in row[\"error_type\"]:\n",
    "        syntax_train_column.append(1)\n",
    "    else:\n",
    "        syntax_train_column.append(0)\n",
    "        \n",
    "for row in dataset[\"test\"]:\n",
    "    if error_type in row[\"error_type\"]:\n",
    "        syntax_test_column.append(1)\n",
    "    else:\n",
    "        syntax_test_column.append(0)\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].add_column(\"label\", syntax_train_column)\n",
    "dataset[\"test\"] = dataset[\"test\"].add_column(\"label\", syntax_test_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88979615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Мне предоставилась возможность все видеть, сам оставаясь незамеченным.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = dataset[\"train\"].column_names\n",
    "column_names.remove(\"label\")\n",
    "column_names.remove(\"text\")\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].remove_columns(column_names)\n",
    "dataset[\"test\"] = dataset[\"test\"].remove_columns(column_names)\n",
    "\n",
    "dataset[\"train\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46c9637c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Они хотят, чтобы я разговаривал с вами.', 'label': 0}\n",
      "{'text': 'Через четверть часа он уже сидел в ресторане.', 'label': 0}\n",
      "{'text': 'Как я заметил еще из окна, каждый из них что-то нес.', 'label': 0}\n",
      "{'text': 'Придет он или нет, зависит от ряда обстоятельств.', 'label': 0}\n",
      "{'text': 'Ценность сведений установилась на основе предпочтений.', 'label': 1}\n",
      "{'text': 'Они приняли во внимание слухи, что Аня предаст Катю.', 'label': 0}\n",
      "{'text': 'На повороте дороги показалась коляска.', 'label': 0}\n",
      "{'text': 'Ученые заявили об истощении людей до степени худобы при отсутствии еды и воды.', 'label': 1}\n",
      "{'text': 'Он немедленно не ушел.', 'label': 1}\n",
      "{'text': 'Пиво он любит темное.', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(dataset[\"train\"][30 + i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a191f0c5",
   "metadata": {},
   "source": [
    "Попробуем обучить классификатор без манипуляций с данными (спойлер - не учится)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "eb8a2eca",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afeee55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, concatenate_datasets, load_dataset\n",
    "\n",
    "new_part = {\n",
    "    \"text\": [],\n",
    "    \"label\": []\n",
    "}\n",
    "\n",
    "for row in dataset[\"train\"]:\n",
    "    if row[\"label\"] == 1: \n",
    "        for i in range(3):\n",
    "            new_part[\"text\"].append(row[\"text\"])\n",
    "            new_part[\"label\"].append(row[\"label\"])\n",
    "\n",
    "dataset[\"train\"] = concatenate_datasets([dataset[\"train\"], Dataset.from_dict(new_part)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be760853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15833\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "599e6e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2455630644855681\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "for row in dataset[\"train\"]:\n",
    "    if row[\"label\"] == 1: \n",
    "        counter += 1\n",
    "\n",
    "print(counter/len(dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10b3bdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/config.json from cache at /home/bart/.cache/huggingface/transformers/726139048d10597682731a7a4c0b8ef0382927911bc0f6f050a4f7f0afb04c2a.149cdc07694f3925e290abc5528c57a543bcbc9af955c0202b1028584ad15cb4\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"cointegrated/rubert-tiny2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.3,\n",
      "  \"classifier_dropout\": 0.3,\n",
      "  \"emb_size\": 312,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.3,\n",
      "  \"hidden_size\": 312,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 600,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 83828\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/pytorch_model.bin from cache at /home/bart/.cache/huggingface/transformers/a04241e554166e7b53f251768f0155f1b8a3f631c8e61b632fb38b3fa4a11314.6ea1ba06b786dd5a8753d0d1da827629b627d560c1d01af937410f09ce3e3983\n",
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/vocab.txt from cache at /home/bart/.cache/huggingface/transformers/16af2afaa4ceaa8d50b689bd4c2f7ef7fe3bfac06c0aac7d82a5c1c72298b62a.cc3312d07ccf88871a3c2b7cb3442138e6785101efead94d9f77e96301cf7f4a\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/tokenizer.json from cache at /home/bart/.cache/huggingface/transformers/cfa6d82dc8ecc7fe3f06deb449f38968cdd188bb84c8da0e06f0bbfddbede1e3.550ab7157d36210bf96c7c3b30e621933d37d635c5f2e290f7e88bd5f7c9198a\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/special_tokens_map.json from cache at /home/bart/.cache/huggingface/transformers/20317640533199c6b37a557395cd5ee5fcb8777be7c89bb1314bfd43058b35e9.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/tokenizer_config.json from cache at /home/bart/.cache/huggingface/transformers/e818c3e83969c6aa46da3b5b2eafe049b197e0e787503bf7c643ec64422a51fc.1cec470309dd45bda58f63ce3bb829fe84e2a93e1fc2857ceff76e77262d7944\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9caec73ebfd470cb87727b26bddd390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3293dc29d967449fa23ab955b4ed6e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "from transformers import TrainingArguments, Trainer, AutoTokenizer\n",
    "from transformers.utils import logging\n",
    "\n",
    "\n",
    "# Then get the dataset\n",
    "!export TRANSFORMERS_NO_ADVISORY_WARNINGS=1\n",
    "datasets = dataset\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"cointegrated/rubert-tiny2\",\n",
    "    num_labels=2,\n",
    "    attention_probs_dropout_prob=0.3,\n",
    "    hidden_dropout_prob=0.3,\n",
    "    classifier_dropout=0.3\n",
    ")\n",
    "\n",
    "f1_metric = load_metric(\"f1\")\n",
    "precision_metric = load_metric(\"precision\")\n",
    "recall_metric = load_metric(\"recall\")\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"semantics_error_classifier_checkpoints\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=18,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=1e-7,\n",
    "    # log_level=\"error\",\n",
    "    logging_first_step=True,\n",
    ")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "\n",
    "\n",
    "datasets = copy.deepcopy(dataset)\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"]\n",
    "small_eval_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb606c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/bart/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15833\n",
      "  Num Epochs = 18\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 142506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='142506' max='142506' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [142506/142506 7:43:13, Epoch 18/18]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.631100</td>\n",
       "      <td>0.565289</td>\n",
       "      <td>0.925558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.577200</td>\n",
       "      <td>0.473557</td>\n",
       "      <td>0.925558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.551800</td>\n",
       "      <td>0.402639</td>\n",
       "      <td>0.925558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.533000</td>\n",
       "      <td>0.350430</td>\n",
       "      <td>0.925558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.541000</td>\n",
       "      <td>0.314479</td>\n",
       "      <td>0.925558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.550600</td>\n",
       "      <td>0.291373</td>\n",
       "      <td>0.925558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.598000</td>\n",
       "      <td>0.276761</td>\n",
       "      <td>0.925558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.642300</td>\n",
       "      <td>0.268387</td>\n",
       "      <td>0.925558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.685200</td>\n",
       "      <td>0.264257</td>\n",
       "      <td>0.925558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.557600</td>\n",
       "      <td>0.262681</td>\n",
       "      <td>0.925558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.708100</td>\n",
       "      <td>0.262738</td>\n",
       "      <td>0.925558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.624300</td>\n",
       "      <td>0.263568</td>\n",
       "      <td>0.925558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.682800</td>\n",
       "      <td>0.264730</td>\n",
       "      <td>0.925558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.704400</td>\n",
       "      <td>0.265882</td>\n",
       "      <td>0.925558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.731000</td>\n",
       "      <td>0.266777</td>\n",
       "      <td>0.925558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.713400</td>\n",
       "      <td>0.267431</td>\n",
       "      <td>0.925558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.731000</td>\n",
       "      <td>0.267810</td>\n",
       "      <td>0.925558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.682300</td>\n",
       "      <td>0.267901</td>\n",
       "      <td>0.925558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-1000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-1000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-1500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-1500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-2000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-2000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-2500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-2500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-3000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-3000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-3500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-3500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-4000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-4000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-4500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-4500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-5000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-5000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-5500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-5500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-6000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-6000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-6500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-6500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-7000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-7000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-7500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-7500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-7500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-8000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-8000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-8500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-8500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-9000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-9000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-9500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-9500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-10000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-10000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-10500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-10500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-11000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-11000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-11000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-11500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-11500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-11500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-12000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-12000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-12000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-12500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-12500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-12500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-13000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-13000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-13000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-13500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-13500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-13500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-14000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-14000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-14000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-14500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-14500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-14500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-15000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-15000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-15000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-15500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-15500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-15500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-16000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-16000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-16000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-16500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-16500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-16500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-17000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-17000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-17000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-17500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-17500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-17500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-18000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-18000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-18000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-18500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-18500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-18500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-19000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-19000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-19000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-19500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-19500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-19500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-20000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-20000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-20000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-20500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-20500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-20500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-21000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-21000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-21000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-21500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-21500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-21500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-22000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-22000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-22000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-22500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-22500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-22500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-23000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-23000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-23000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-23500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-23500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-23500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-24000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-24000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-24000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-24500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-24500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-24500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-25000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-25000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-25000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-25500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-25500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-25500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-26000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-26000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-26000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-26500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-26500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-26500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-27000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-27000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-27000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-27500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-27500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-27500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-28000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-28000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-28000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-28500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-28500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-28500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-29000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-29000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-29000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-29500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-29500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-29500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-30000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-30000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-30000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-30500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-30500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-30500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-31000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-31000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-31000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-31500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-31500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-31500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-32000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-32000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-32000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-32500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-32500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-32500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-33000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-33000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-33000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-33500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-33500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-33500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-34000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-34000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-34000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-34500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-34500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-34500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-35000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-35000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-35000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-35500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-35500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-35500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-36000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-36000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-36000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-36500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-36500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-36500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-37000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-37000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-37000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-37500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-37500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-37500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-38000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-38000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-38000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-38500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-38500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-38500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-39000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-39000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-39000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-39500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-39500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-39500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-40000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-40000/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-40000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-40500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-40500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-40500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-41000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-41000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-41000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-41500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-41500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-41500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-42000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-42000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-42000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-42500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-42500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-42500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-43000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-43000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-43000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-43500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-43500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-43500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-44000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-44000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-44000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-44500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-44500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-44500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-45000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-45000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-45000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-45500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-45500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-45500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-46000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-46000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-46000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-46500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-46500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-46500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-47000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-47000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-47000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-47500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-47500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-47500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-48000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-48000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-48000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-48500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-48500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-48500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-49000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-49000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-49000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-49500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-49500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-49500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-50000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-50000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-50000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-50500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-50500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-50500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-51000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-51000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-51000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-51500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-51500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-51500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-52000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-52000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-52000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-52500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-52500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-52500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-53000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-53000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-53000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-53500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-53500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-53500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-54000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-54000/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-54000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-54500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-54500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-54500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-55000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-55000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-55000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-55500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-55500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-55500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-56000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-56000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-56000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-56500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-56500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-56500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-57000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-57000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-57000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-57500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-57500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-57500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-58000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-58000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-58000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-58500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-58500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-58500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-59000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-59000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-59000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-59500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-59500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-59500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-60000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-60000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-60000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-60500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-60500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-60500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-61000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-61000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-61000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-61500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-61500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-61500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-62000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-62000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-62000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-62500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-62500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-62500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-63000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-63000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-63000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-63500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-63500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-63500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-64000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-64000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-64000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-64500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-64500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-64500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-65000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-65000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-65000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-65500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-65500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-65500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-66000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-66000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-66000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-66500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-66500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-66500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-67000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-67000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-67000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-67500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-67500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-67500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-68000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-68000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-68000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-68500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-68500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-68500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-69000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-69000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-69000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-69500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-69500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-69500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-70000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-70000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-70000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-70500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-70500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-70500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-71000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-71000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-71000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-71500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-71500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-71500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-72000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-72000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-72000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-72500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-72500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-72500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-73000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-73000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-73000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-73500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-73500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-73500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-74000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-74000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-74000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-74500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-74500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-74500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-75000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-75000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-75000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-75500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-75500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-75500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-76000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-76000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-76000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-76500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-76500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-76500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-77000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-77000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-77000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-77500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-77500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-77500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-78000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-78000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-78000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-78500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-78500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-78500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-79000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-79000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-79000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-79500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-79500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-79500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-80000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-80000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-80000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-80500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-80500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-80500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-81000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-81000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-81000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-81500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-81500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-81500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-82000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-82000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-82000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-82500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-82500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-82500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-83000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-83000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-83000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-83500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-83500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-83500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-84000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-84000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-84000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-84500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-84500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-84500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-85000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-85000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-85000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-85500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-85500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-85500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-86000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-86000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-86000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-86500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-86500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-86500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-87000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-87000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-87000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-87500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-87500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-87500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-88000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-88000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-88000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-88500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-88500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-88500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-89000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-89000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-89000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-89500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-89500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-89500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-90000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-90000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-90000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-90500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-90500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-90500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-91000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-91000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-91000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-91500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-91500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-91500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-92000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-92000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-92000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-92500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-92500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-92500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-93000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-93000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-93000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-93500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-93500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-93500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-94000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-94000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-94000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-94500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-94500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-94500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-95000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-95000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-95000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-95500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-95500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-95500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-96000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-96000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-96000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-96500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-96500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-96500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-97000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-97000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-97000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-97500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-97500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-97500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-98000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-98000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-98000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-98500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-98500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-98500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-99000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-99000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-99000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-99500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-99500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-99500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-100000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-100000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-100000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-100500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-100500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-100500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-101000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-101000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-101000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-101500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-101500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-101500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-102000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-102000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-102000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-102500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-102500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-102500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-103000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-103000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-103000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-103500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-103500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-103500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-104000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-104000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-104000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-104500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-104500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-104500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-105000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-105000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-105000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-105500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-105500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-105500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-106000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-106000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-106000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-106500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-106500/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-106500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-107000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-107000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-107000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-107500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-107500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-107500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-108000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-108000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-108000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-108500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-108500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-108500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-109000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-109000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-109000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-109500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-109500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-109500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-110000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-110000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-110000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-110500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-110500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-110500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-111000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-111000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-111000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-111500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-111500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-111500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-112000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-112000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-112000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-112500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-112500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-112500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-113000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-113000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-113000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-113500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-113500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-113500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-114000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-114000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-114000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-114500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-114500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-114500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-115000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-115000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-115000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-115500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-115500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-115500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-116000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-116000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-116000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-116500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-116500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-116500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-117000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-117000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-117000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-117500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-117500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-117500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-118000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-118000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-118000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-118500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-118500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-118500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-119000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-119000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-119000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-119500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-119500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-119500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-120000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-120000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-120000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-120500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-120500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-120500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-121000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-121000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-121000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-121500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-121500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-121500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-122000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-122000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-122000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-122500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-122500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-122500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-123000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-123000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-123000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-123500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-123500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-123500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-124000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-124000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-124000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-124500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-124500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-124500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-125000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-125000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-125000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-125500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-125500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-125500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-126000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-126000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-126000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-126500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-126500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-126500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-127000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-127000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-127000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-127500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-127500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-127500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-128000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-128000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-128000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-128500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-128500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-128500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-129000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-129000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-129000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-129500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-129500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-129500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-130000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-130000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-130000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-130500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-130500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-130500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-131000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-131000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-131000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-131500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-131500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-131500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-132000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-132000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-132000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-132500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-132500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-132500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-133000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-133000/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-133000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-133500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-133500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-133500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-134000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-134000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-134000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-134500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-134500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-134500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-135000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-135000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-135000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-135500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-135500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-135500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-136000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-136000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-136000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-136500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-136500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-136500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-137000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-137000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-137000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-137500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-137500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-137500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-138000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-138000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-138000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-138500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-138500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-138500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-139000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-139000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-139000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-139500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-139500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-139500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-140000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-140000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-140000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-140500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-140500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-140500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-141000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-141000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-141000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-141500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-141500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-141500/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-142000\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-142000/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-142000/pytorch_model.bin\n",
      "Saving model checkpoint to semantics_error_classifier_checkpoints/checkpoint-142500\n",
      "Configuration saved in semantics_error_classifier_checkpoints/checkpoint-142500/config.json\n",
      "Model weights saved in semantics_error_classifier_checkpoints/checkpoint-142500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=142506, training_loss=0.6484119539863377, metrics={'train_runtime': 27794.1654, 'train_samples_per_second': 10.254, 'train_steps_per_second': 5.127, 'total_flos': 8406418971672576.0, 'train_loss': 0.6484119539863377, 'epoch': 18.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Start training\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e449202",
   "metadata": {},
   "source": [
    "# Machine Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33f162d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-081ceb0bf9c7c7cd\n",
      "Reusing dataset csv (/home/bart/.cache/huggingface/datasets/csv/default-081ceb0bf9c7c7cd/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63006b9d558545aeaef37f89412bd096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('csv', data_files={'train': './rucola/train.csv', 'test': './rucola/test.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eca9628",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_type = \"Machine\"\n",
    "syntax_train_column = []\n",
    "syntax_test_column = []\n",
    "for row in dataset[\"train\"]:\n",
    "    if error_type in row[\"authored_by\"]:\n",
    "        syntax_train_column.append(1)\n",
    "    else:\n",
    "        syntax_train_column.append(0)\n",
    "        \n",
    "for row in dataset[\"test\"]:\n",
    "    if error_type in row[\"authored_by\"]:\n",
    "        syntax_test_column.append(1)\n",
    "    else:\n",
    "        syntax_test_column.append(0)\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].add_column(\"label\", syntax_train_column)\n",
    "dataset[\"test\"] = dataset[\"test\"].add_column(\"label\", syntax_test_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5d92b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Мне предоставилась возможность все видеть, сам оставаясь незамеченным.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = dataset[\"train\"].column_names\n",
    "column_names.remove(\"label\")\n",
    "column_names.remove(\"text\")\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].remove_columns(column_names)\n",
    "dataset[\"test\"] = dataset[\"test\"].remove_columns(column_names)\n",
    "\n",
    "dataset[\"train\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff782b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Они хотят, чтобы я разговаривал с вами.', 'label': 1}\n",
      "{'text': 'Через четверть часа он уже сидел в ресторане.', 'label': 0}\n",
      "{'text': 'Как я заметил еще из окна, каждый из них что-то нес.', 'label': 0}\n",
      "{'text': 'Придет он или нет, зависит от ряда обстоятельств.', 'label': 0}\n",
      "{'text': 'Ценность сведений установилась на основе предпочтений.', 'label': 0}\n",
      "{'text': 'Они приняли во внимание слухи, что Аня предаст Катю.', 'label': 0}\n",
      "{'text': 'На повороте дороги показалась коляска.', 'label': 0}\n",
      "{'text': 'Ученые заявили об истощении людей до степени худобы при отсутствии еды и воды.', 'label': 1}\n",
      "{'text': 'Он немедленно не ушел.', 'label': 0}\n",
      "{'text': 'Пиво он любит темное.', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(dataset[\"train\"][30 + i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b4a8996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2582643028567005\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "for row in dataset[\"train\"]:\n",
    "    if row[\"label\"] == 1: \n",
    "        counter += 1\n",
    "\n",
    "print(counter/len(dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ece4b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/config.json from cache at /home/bart/.cache/huggingface/transformers/726139048d10597682731a7a4c0b8ef0382927911bc0f6f050a4f7f0afb04c2a.149cdc07694f3925e290abc5528c57a543bcbc9af955c0202b1028584ad15cb4\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"cointegrated/rubert-tiny2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.3,\n",
      "  \"classifier_dropout\": 0.3,\n",
      "  \"emb_size\": 312,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.3,\n",
      "  \"hidden_size\": 312,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 600,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 83828\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/pytorch_model.bin from cache at /home/bart/.cache/huggingface/transformers/a04241e554166e7b53f251768f0155f1b8a3f631c8e61b632fb38b3fa4a11314.6ea1ba06b786dd5a8753d0d1da827629b627d560c1d01af937410f09ce3e3983\n",
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/vocab.txt from cache at /home/bart/.cache/huggingface/transformers/16af2afaa4ceaa8d50b689bd4c2f7ef7fe3bfac06c0aac7d82a5c1c72298b62a.cc3312d07ccf88871a3c2b7cb3442138e6785101efead94d9f77e96301cf7f4a\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/tokenizer.json from cache at /home/bart/.cache/huggingface/transformers/cfa6d82dc8ecc7fe3f06deb449f38968cdd188bb84c8da0e06f0bbfddbede1e3.550ab7157d36210bf96c7c3b30e621933d37d635c5f2e290f7e88bd5f7c9198a\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/special_tokens_map.json from cache at /home/bart/.cache/huggingface/transformers/20317640533199c6b37a557395cd5ee5fcb8777be7c89bb1314bfd43058b35e9.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/tokenizer_config.json from cache at /home/bart/.cache/huggingface/transformers/e818c3e83969c6aa46da3b5b2eafe049b197e0e787503bf7c643ec64422a51fc.1cec470309dd45bda58f63ce3bb829fe84e2a93e1fc2857ceff76e77262d7944\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773b223c606640ec81fc12729a82824c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b09f0891b54bb8bffb9ba7987d4a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "from transformers import TrainingArguments, Trainer, AutoTokenizer\n",
    "from transformers.utils import logging\n",
    "\n",
    "\n",
    "# Then get the dataset\n",
    "!export TRANSFORMERS_NO_ADVISORY_WARNINGS=1\n",
    "datasets = dataset\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"cointegrated/rubert-tiny2\",\n",
    "    num_labels=2,\n",
    "    attention_probs_dropout_prob=0.3,\n",
    "    hidden_dropout_prob=0.3,\n",
    "    classifier_dropout=0.3\n",
    ")\n",
    "\n",
    "f1_metric = load_metric(\"f1\")\n",
    "precision_metric = load_metric(\"precision\")\n",
    "recall_metric = load_metric(\"recall\")\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"Machine_text_classifier_checkpoints\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=18,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=1e-6,\n",
    "    # log_level=\"error\",\n",
    "    logging_first_step=True,\n",
    ")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "\n",
    "\n",
    "datasets = copy.deepcopy(dataset)\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"]\n",
    "small_eval_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e9183a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/bart/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12917\n",
      "  Num Epochs = 18\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 116262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='47566' max='116262' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 47566/116262 2:38:28 < 3:48:53, 5.00 it/s, Epoch 7.36/18]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.580300</td>\n",
       "      <td>0.613922</td>\n",
       "      <td>0.743176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.577272</td>\n",
       "      <td>0.810794</td>\n",
       "      <td>0.865772</td>\n",
       "      <td>0.311594</td>\n",
       "      <td>0.458259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.577600</td>\n",
       "      <td>0.589346</td>\n",
       "      <td>0.831886</td>\n",
       "      <td>0.784861</td>\n",
       "      <td>0.475845</td>\n",
       "      <td>0.592481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.553200</td>\n",
       "      <td>0.626278</td>\n",
       "      <td>0.837469</td>\n",
       "      <td>0.783582</td>\n",
       "      <td>0.507246</td>\n",
       "      <td>0.615836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.637400</td>\n",
       "      <td>0.628822</td>\n",
       "      <td>0.841191</td>\n",
       "      <td>0.788321</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.627907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.554400</td>\n",
       "      <td>0.623779</td>\n",
       "      <td>0.842432</td>\n",
       "      <td>0.787770</td>\n",
       "      <td>0.528986</td>\n",
       "      <td>0.632948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.504600</td>\n",
       "      <td>0.632247</td>\n",
       "      <td>0.848015</td>\n",
       "      <td>0.796491</td>\n",
       "      <td>0.548309</td>\n",
       "      <td>0.649499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-1000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-1000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-1500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-1500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-2000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-2000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-2500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-2500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-3000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-3000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-3500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-3500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-4000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-4000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-4500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-4500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-5000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-5000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-5500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-5500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-6000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-6000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-6000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-6500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-6500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-7000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-7000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-7500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-7500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-8000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-8000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-8500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-8500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-9000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-9000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-9500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-9500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-10000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-10000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-10500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-10500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-11000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-11000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-11000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-11500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-11500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-11500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-12000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-12000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-12000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-12500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-12500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-12500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-13000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-13000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-13000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-13500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-13500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-13500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-14000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-14000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-14000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-14500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-14500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-14500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-15000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-15000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-15000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-15500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-15500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-15500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-16000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-16000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-16000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-16500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-16500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-16500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-17000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-17000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-17000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-17500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-17500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-17500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-18000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-18000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-18000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-18500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-18500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-18500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-19000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-19000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-19000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-19500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-19500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-19500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-20000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-20000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-20000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-20500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-20500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-20500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-21000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-21000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-21000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-21500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-21500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-21500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-22000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-22000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-22000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-22500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-22500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-22500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-23000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-23000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-23000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-23500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-23500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-23500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-24000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-24000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-24000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-24500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-24500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-24500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-25000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-25000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-25000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-25500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-25500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-25500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-26000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-26000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-26000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-26500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-26500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-26500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-27000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-27000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-27000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-27500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-27500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-27500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-28000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-28000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-28000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-28500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-28500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-28500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-29000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-29000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-29000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-29500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-29500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-29500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-30000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-30000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-30000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-30500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-30500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-30500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-31000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-31000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-31000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-31500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-31500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-31500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-32000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-32000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-32000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-32500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-32500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-32500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-33000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-33000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-33000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-33500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-33500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-33500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-34000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-34000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-34000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-34500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-34500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-34500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-35000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-35000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-35000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-35500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-35500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-35500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-36000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-36000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-36000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-36500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-36500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-36500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-37000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-37000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-37000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-37500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-37500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-37500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-38000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-38000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-38000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-38500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-38500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-38500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-39000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-39000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-39000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-39500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-39500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-39500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-40000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-40000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-40000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-40500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-40500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-40500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-41000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-41000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-41000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-41500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-41500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-41500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-42000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-42000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-42000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-42500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-42500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-42500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-43000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-43000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-43000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-43500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-43500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-43500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-44000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-44000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-44000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-44500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-44500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-44500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-45000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-45000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-45000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-45500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-45500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-45500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-46000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-46000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-46000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-46500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-46500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-46500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-47000\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-47000/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-47000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_classifier_checkpoints/checkpoint-47500\n",
      "Configuration saved in Machine_text_classifier_checkpoints/checkpoint-47500/config.json\n",
      "Model weights saved in Machine_text_classifier_checkpoints/checkpoint-47500/pytorch_model.bin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:1422\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1420\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1422\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1425\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1428\u001b[0m ):\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2029\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2027\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m   2028\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2029\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Start training\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abb518a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/config.json from cache at /home/bart/.cache/huggingface/transformers/726139048d10597682731a7a4c0b8ef0382927911bc0f6f050a4f7f0afb04c2a.149cdc07694f3925e290abc5528c57a543bcbc9af955c0202b1028584ad15cb4\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"cointegrated/rubert-tiny2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.3,\n",
      "  \"classifier_dropout\": 0.3,\n",
      "  \"emb_size\": 312,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.3,\n",
      "  \"hidden_size\": 312,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 600,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 83828\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/pytorch_model.bin from cache at /home/bart/.cache/huggingface/transformers/a04241e554166e7b53f251768f0155f1b8a3f631c8e61b632fb38b3fa4a11314.6ea1ba06b786dd5a8753d0d1da827629b627d560c1d01af937410f09ce3e3983\n",
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/vocab.txt from cache at /home/bart/.cache/huggingface/transformers/16af2afaa4ceaa8d50b689bd4c2f7ef7fe3bfac06c0aac7d82a5c1c72298b62a.cc3312d07ccf88871a3c2b7cb3442138e6785101efead94d9f77e96301cf7f4a\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/tokenizer.json from cache at /home/bart/.cache/huggingface/transformers/cfa6d82dc8ecc7fe3f06deb449f38968cdd188bb84c8da0e06f0bbfddbede1e3.550ab7157d36210bf96c7c3b30e621933d37d635c5f2e290f7e88bd5f7c9198a\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/special_tokens_map.json from cache at /home/bart/.cache/huggingface/transformers/20317640533199c6b37a557395cd5ee5fcb8777be7c89bb1314bfd43058b35e9.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/tokenizer_config.json from cache at /home/bart/.cache/huggingface/transformers/e818c3e83969c6aa46da3b5b2eafe049b197e0e787503bf7c643ec64422a51fc.1cec470309dd45bda58f63ce3bb829fe84e2a93e1fc2857ceff76e77262d7944\n",
      "Loading cached processed dataset at /home/bart/.cache/huggingface/datasets/csv/default-081ceb0bf9c7c7cd/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-845d10a59f5a378b.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6826be12b5d449dc97b137a5c1ba85ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "from transformers import TrainingArguments, Trainer, AutoTokenizer\n",
    "from transformers.utils import logging\n",
    "\n",
    "\n",
    "# Then get the dataset\n",
    "!export TRANSFORMERS_NO_ADVISORY_WARNINGS=1\n",
    "datasets = dataset\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"cointegrated/rubert-tiny2\",\n",
    "    num_labels=2,\n",
    "    attention_probs_dropout_prob=0.3,\n",
    "    hidden_dropout_prob=0.3,\n",
    "    classifier_dropout=0.3\n",
    ")\n",
    "\n",
    "f1_metric = load_metric(\"f1\")\n",
    "precision_metric = load_metric(\"precision\")\n",
    "recall_metric = load_metric(\"recall\")\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"Machine_text_1e7_classifier_checkpoints\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=18,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=1e-7,\n",
    "    # log_level=\"error\",\n",
    "    logging_first_step=True,\n",
    ")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "\n",
    "\n",
    "datasets = copy.deepcopy(dataset)\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"]\n",
    "small_eval_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27316729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/bart/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12917\n",
      "  Num Epochs = 18\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 116262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21892' max='116262' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 21892/116262 1:12:53 < 5:14:13, 5.01 it/s, Epoch 3.39/18]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.615900</td>\n",
       "      <td>0.610916</td>\n",
       "      <td>0.745037</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.028369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.567600</td>\n",
       "      <td>0.563897</td>\n",
       "      <td>0.743176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.546700</td>\n",
       "      <td>0.531497</td>\n",
       "      <td>0.743176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-1000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-1000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-1500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-1500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-2000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-2000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-2500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-2500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-3000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-3000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-3500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-3500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-4000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-4000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-4500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-4500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-5000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-5000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-5500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-5500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-6000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-6000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-6000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-6500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-6500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-7000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-7000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-7500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-7500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-8000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-8000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-8500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-8500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-9000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-9000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-9500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-9500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-10000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-10000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-10500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-10500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-11000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-11000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-11000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-11500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-11500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-11500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-12000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-12000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-12000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-12500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-12500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-12500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-13000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-13000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-13000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-13500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-13500/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-13500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-14000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-14000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-14000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-14500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-14500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-14500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-15000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-15000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-15000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-15500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-15500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-15500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-16000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-16000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-16000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-16500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-16500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-16500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-17000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-17000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-17000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-17500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-17500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-17500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-18000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-18000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-18000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-18500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-18500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-18500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-19000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-19000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-19000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-19500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-19500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-19500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-20000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-20000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-20000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-20500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-20500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-20500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-21000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-21000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-21000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-21500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-21500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-21500/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "print(\"Start training\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1a0dd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/config.json from cache at /home/bart/.cache/huggingface/transformers/726139048d10597682731a7a4c0b8ef0382927911bc0f6f050a4f7f0afb04c2a.149cdc07694f3925e290abc5528c57a543bcbc9af955c0202b1028584ad15cb4\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"cointegrated/rubert-tiny2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.3,\n",
      "  \"classifier_dropout\": 0.3,\n",
      "  \"emb_size\": 312,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.3,\n",
      "  \"hidden_size\": 312,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 600,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 83828\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/pytorch_model.bin from cache at /home/bart/.cache/huggingface/transformers/a04241e554166e7b53f251768f0155f1b8a3f631c8e61b632fb38b3fa4a11314.6ea1ba06b786dd5a8753d0d1da827629b627d560c1d01af937410f09ce3e3983\n",
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/vocab.txt from cache at /home/bart/.cache/huggingface/transformers/16af2afaa4ceaa8d50b689bd4c2f7ef7fe3bfac06c0aac7d82a5c1c72298b62a.cc3312d07ccf88871a3c2b7cb3442138e6785101efead94d9f77e96301cf7f4a\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/tokenizer.json from cache at /home/bart/.cache/huggingface/transformers/cfa6d82dc8ecc7fe3f06deb449f38968cdd188bb84c8da0e06f0bbfddbede1e3.550ab7157d36210bf96c7c3b30e621933d37d635c5f2e290f7e88bd5f7c9198a\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/special_tokens_map.json from cache at /home/bart/.cache/huggingface/transformers/20317640533199c6b37a557395cd5ee5fcb8777be7c89bb1314bfd43058b35e9.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/tokenizer_config.json from cache at /home/bart/.cache/huggingface/transformers/e818c3e83969c6aa46da3b5b2eafe049b197e0e787503bf7c643ec64422a51fc.1cec470309dd45bda58f63ce3bb829fe84e2a93e1fc2857ceff76e77262d7944\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434d5515b0a84dcabc58328b5d47e86a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf53b57fb53544628b39d9ed38073dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "from transformers import TrainingArguments, Trainer, AutoTokenizer\n",
    "from transformers.utils import logging\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "class CustomMachineClassifierTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 4.0]).to(DEVICE))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Then get the dataset\n",
    "!export TRANSFORMERS_NO_ADVISORY_WARNINGS=1\n",
    "datasets = dataset\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"cointegrated/rubert-tiny2\",\n",
    "    num_labels=2,\n",
    "    attention_probs_dropout_prob=0.3,\n",
    "    hidden_dropout_prob=0.3,\n",
    "    classifier_dropout=0.3\n",
    ")\n",
    "\n",
    "f1_metric = load_metric(\"f1\")\n",
    "precision_metric = load_metric(\"precision\")\n",
    "recall_metric = load_metric(\"recall\")\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"Machine_text_1e7_classifier_checkpoints\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=18,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=1e-6,\n",
    "    # log_level=\"error\",\n",
    "    logging_first_step=True,\n",
    ")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "\n",
    "\n",
    "datasets = copy.deepcopy(dataset)\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"]\n",
    "small_eval_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "trainer = CustomMachineClassifierTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bfdf332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 12917\n",
      "  Num Epochs = 18\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 116262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51875' max='116262' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 51875/116262 2:59:01 < 3:42:12, 4.83 it/s, Epoch 8.03/18]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.670600</td>\n",
       "      <td>0.713106</td>\n",
       "      <td>0.743176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.755100</td>\n",
       "      <td>0.779854</td>\n",
       "      <td>0.812035</td>\n",
       "      <td>0.858065</td>\n",
       "      <td>0.321256</td>\n",
       "      <td>0.467487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.769100</td>\n",
       "      <td>0.785934</td>\n",
       "      <td>0.832506</td>\n",
       "      <td>0.758993</td>\n",
       "      <td>0.509662</td>\n",
       "      <td>0.609827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.711900</td>\n",
       "      <td>0.844970</td>\n",
       "      <td>0.837469</td>\n",
       "      <td>0.773381</td>\n",
       "      <td>0.519324</td>\n",
       "      <td>0.621387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.841600</td>\n",
       "      <td>0.840483</td>\n",
       "      <td>0.839330</td>\n",
       "      <td>0.775801</td>\n",
       "      <td>0.526570</td>\n",
       "      <td>0.627338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.724700</td>\n",
       "      <td>0.828486</td>\n",
       "      <td>0.844913</td>\n",
       "      <td>0.784722</td>\n",
       "      <td>0.545894</td>\n",
       "      <td>0.643875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.629500</td>\n",
       "      <td>0.840551</td>\n",
       "      <td>0.848635</td>\n",
       "      <td>0.789116</td>\n",
       "      <td>0.560386</td>\n",
       "      <td>0.655367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.728500</td>\n",
       "      <td>0.823027</td>\n",
       "      <td>0.853598</td>\n",
       "      <td>0.796667</td>\n",
       "      <td>0.577295</td>\n",
       "      <td>0.669468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-1000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-1000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-1500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-1500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-2000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-2000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-2500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-2500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-3000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-3000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-3500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-3500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-4000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-4000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-4500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-4500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-5000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-5000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-5500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-5500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-6000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-6000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-6000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-6500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-6500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-7000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-7000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-7500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-7500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-8000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-8000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-8500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-8500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-9000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-9000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-9500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-9500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-10000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-10000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-10500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-10500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-11000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-11000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-11000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-11500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-11500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-11500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-12000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-12000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-12000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-12500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-12500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-12500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-13000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-13000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-13000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-13500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-13500/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-13500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-14000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-14000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-14000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-14500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-14500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-14500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-15000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-15000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-15000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-15500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-15500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-15500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-16000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-16000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-16000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-16500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-16500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-16500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-17000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-17000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-17000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-17500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-17500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-17500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-18000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-18000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-18000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-18500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-18500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-18500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-19000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-19000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-19000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-19500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-19500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-19500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-20000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-20000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-20000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-20500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-20500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-20500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-21000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-21000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-21000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-21500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-21500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-21500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-22000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-22000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-22000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-22500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-22500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-22500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-23000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-23000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-23000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-23500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-23500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-23500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-24000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-24000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-24000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-24500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-24500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-24500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-25000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-25000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-25000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-25500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-25500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-25500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-26000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-26000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-26000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-26500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-26500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-26500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-27000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-27000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-27000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-27500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-27500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-27500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-28000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-28000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-28000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-28500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-28500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-28500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-29000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-29000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-29000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-29500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-29500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-29500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-30000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-30000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-30000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-30500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-30500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-30500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-31000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-31000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-31000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-31500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-31500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-31500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-32000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-32000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-32000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-32500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-32500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-32500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-33000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-33000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-33000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-33500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-33500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-33500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-34000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-34000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-34000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-34500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-34500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-34500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-35000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-35000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-35000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-35500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-35500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-35500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-36000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-36000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-36000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-36500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-36500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-36500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-37000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-37000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-37000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-37500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-37500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-37500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-38000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-38000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-38000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-38500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-38500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-38500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-39000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-39000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-39000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-39500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-39500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-39500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-40000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-40000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-40000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-40500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-40500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-40500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-41000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-41000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-41000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-41500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-41500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-41500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-42000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-42000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-42000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-42500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-42500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-42500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-43000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-43000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-43000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-43500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-43500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-43500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-44000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-44000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-44000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-44500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-44500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-44500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-45000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-45000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-45000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-45500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-45500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-45500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-46000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-46000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-46000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-46500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-46500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-46500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-47000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-47000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-47000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-47500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-47500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-47500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-48000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-48000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-48000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-48500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-48500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-48500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-49000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-49000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-49000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-49500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-49500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-49500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-50000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-50000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-50000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-50500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-50500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-50500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-51000\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-51000/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-51000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_classifier_checkpoints/checkpoint-51500\n",
      "Configuration saved in Machine_text_1e7_classifier_checkpoints/checkpoint-51500/config.json\n",
      "Model weights saved in Machine_text_1e7_classifier_checkpoints/checkpoint-51500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:1422\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1420\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1422\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1425\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1428\u001b[0m ):\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2029\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2027\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m   2028\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2029\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Start training\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d5e8a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/config.json from cache at /home/bart/.cache/huggingface/transformers/726139048d10597682731a7a4c0b8ef0382927911bc0f6f050a4f7f0afb04c2a.149cdc07694f3925e290abc5528c57a543bcbc9af955c0202b1028584ad15cb4\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"cointegrated/rubert-tiny2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.3,\n",
      "  \"classifier_dropout\": 0.3,\n",
      "  \"emb_size\": 312,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.3,\n",
      "  \"hidden_size\": 312,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 600,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 83828\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/pytorch_model.bin from cache at /home/bart/.cache/huggingface/transformers/a04241e554166e7b53f251768f0155f1b8a3f631c8e61b632fb38b3fa4a11314.6ea1ba06b786dd5a8753d0d1da827629b627d560c1d01af937410f09ce3e3983\n",
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/vocab.txt from cache at /home/bart/.cache/huggingface/transformers/16af2afaa4ceaa8d50b689bd4c2f7ef7fe3bfac06c0aac7d82a5c1c72298b62a.cc3312d07ccf88871a3c2b7cb3442138e6785101efead94d9f77e96301cf7f4a\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/tokenizer.json from cache at /home/bart/.cache/huggingface/transformers/cfa6d82dc8ecc7fe3f06deb449f38968cdd188bb84c8da0e06f0bbfddbede1e3.550ab7157d36210bf96c7c3b30e621933d37d635c5f2e290f7e88bd5f7c9198a\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/special_tokens_map.json from cache at /home/bart/.cache/huggingface/transformers/20317640533199c6b37a557395cd5ee5fcb8777be7c89bb1314bfd43058b35e9.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/cointegrated/rubert-tiny2/resolve/main/tokenizer_config.json from cache at /home/bart/.cache/huggingface/transformers/e818c3e83969c6aa46da3b5b2eafe049b197e0e787503bf7c643ec64422a51fc.1cec470309dd45bda58f63ce3bb829fe84e2a93e1fc2857ceff76e77262d7944\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8eccdc2ae224a7ea9005dc5fcbcf800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57dbc8ba1c2d47a4b4a135d5ea1ceee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "from transformers import TrainingArguments, Trainer, AutoTokenizer\n",
    "from transformers.utils import logging\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "class CustomMachineClassifierTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([4.0, 1.0]).to(DEVICE))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Then get the dataset\n",
    "!export TRANSFORMERS_NO_ADVISORY_WARNINGS=1\n",
    "datasets = dataset\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"cointegrated/rubert-tiny2\",\n",
    "    num_labels=2,\n",
    "    attention_probs_dropout_prob=0.3,\n",
    "    hidden_dropout_prob=0.3,\n",
    "    classifier_dropout=0.3\n",
    ")\n",
    "\n",
    "f1_metric = load_metric(\"f1\")\n",
    "precision_metric = load_metric(\"precision\")\n",
    "recall_metric = load_metric(\"recall\")\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"Machine_text_1e7_weight2_classifier_checkpoints\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=18,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=1e-6,\n",
    "    # log_level=\"error\",\n",
    "    logging_first_step=True,\n",
    ")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "\n",
    "\n",
    "datasets = copy.deepcopy(dataset)\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"]\n",
    "small_eval_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "trainer = CustomMachineClassifierTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db17f935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 12917\n",
      "  Num Epochs = 18\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 116262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='116262' max='116262' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [116262/116262 6:24:54, Epoch 18/18]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.387500</td>\n",
       "      <td>0.370496</td>\n",
       "      <td>0.833127</td>\n",
       "      <td>0.803347</td>\n",
       "      <td>0.463768</td>\n",
       "      <td>0.588055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.346100</td>\n",
       "      <td>0.394076</td>\n",
       "      <td>0.833127</td>\n",
       "      <td>0.798354</td>\n",
       "      <td>0.468599</td>\n",
       "      <td>0.590563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.410800</td>\n",
       "      <td>0.394128</td>\n",
       "      <td>0.838089</td>\n",
       "      <td>0.797665</td>\n",
       "      <td>0.495169</td>\n",
       "      <td>0.611028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.363500</td>\n",
       "      <td>0.393770</td>\n",
       "      <td>0.843052</td>\n",
       "      <td>0.801498</td>\n",
       "      <td>0.516908</td>\n",
       "      <td>0.628488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.429800</td>\n",
       "      <td>0.396901</td>\n",
       "      <td>0.849256</td>\n",
       "      <td>0.802120</td>\n",
       "      <td>0.548309</td>\n",
       "      <td>0.651363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.433000</td>\n",
       "      <td>0.396245</td>\n",
       "      <td>0.852357</td>\n",
       "      <td>0.809859</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.659026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.356100</td>\n",
       "      <td>0.395112</td>\n",
       "      <td>0.852978</td>\n",
       "      <td>0.819495</td>\n",
       "      <td>0.548309</td>\n",
       "      <td>0.657019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.327900</td>\n",
       "      <td>0.398552</td>\n",
       "      <td>0.853598</td>\n",
       "      <td>0.829630</td>\n",
       "      <td>0.541063</td>\n",
       "      <td>0.654971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.341800</td>\n",
       "      <td>0.396027</td>\n",
       "      <td>0.854839</td>\n",
       "      <td>0.828467</td>\n",
       "      <td>0.548309</td>\n",
       "      <td>0.659884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.364200</td>\n",
       "      <td>0.395778</td>\n",
       "      <td>0.859181</td>\n",
       "      <td>0.832740</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.673381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.422200</td>\n",
       "      <td>0.398940</td>\n",
       "      <td>0.855459</td>\n",
       "      <td>0.829091</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>0.661829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.362400</td>\n",
       "      <td>0.397297</td>\n",
       "      <td>0.857320</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.418400</td>\n",
       "      <td>0.398633</td>\n",
       "      <td>0.858561</td>\n",
       "      <td>0.839416</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.668605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.369000</td>\n",
       "      <td>0.397875</td>\n",
       "      <td>0.859801</td>\n",
       "      <td>0.835714</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.674352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.442400</td>\n",
       "      <td>0.397699</td>\n",
       "      <td>0.860422</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.675325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.319900</td>\n",
       "      <td>0.397761</td>\n",
       "      <td>0.860422</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.675325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.376800</td>\n",
       "      <td>0.397761</td>\n",
       "      <td>0.860422</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.675325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.315400</td>\n",
       "      <td>0.397761</td>\n",
       "      <td>0.860422</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.675325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-1000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-1000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-1500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-1500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-2000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-2000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-2500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-2500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-3000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-3000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-3500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-3500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-4000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-4000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-4500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-4500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-5000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-5000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-5500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-5500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-6000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-6000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-6000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-6500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-6500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-7000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-7000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-7500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-7500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-8000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-8000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-8500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-8500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-9000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-9000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-9500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-9500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-10000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-10000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-10500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-10500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-11000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-11000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-11000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-11500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-11500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-11500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-12000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-12000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-12000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-12500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-12500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-12500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-13000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-13000/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-13000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-13500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-13500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-13500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-14000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-14000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-14000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-14500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-14500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-14500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-15000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-15000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-15000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-15500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-15500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-15500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-16000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-16000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-16000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-16500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-16500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-16500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-17000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-17000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-17000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-17500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-17500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-17500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-18000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-18000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-18000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-18500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-18500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-18500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-19000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-19000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-19000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-19500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-19500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-19500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-20000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-20000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-20000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-20500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-20500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-20500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-21000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-21000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-21000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-21500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-21500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-21500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-22000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-22000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-22000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-22500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-22500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-22500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-23000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-23000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-23000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-23500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-23500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-23500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-24000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-24000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-24000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-24500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-24500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-24500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-25000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-25000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-25000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-25500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-25500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-25500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-26000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-26000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-26000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-26500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-26500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-26500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-27000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-27000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-27000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-27500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-27500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-27500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-28000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-28000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-28000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-28500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-28500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-28500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-29000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-29000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-29000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-29500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-29500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-29500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-30000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-30000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-30000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-30500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-30500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-30500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-31000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-31000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-31000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-31500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-31500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-31500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-32000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-32000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-32000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-32500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-32500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-32500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-33000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-33000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-33000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-33500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-33500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-33500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-34000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-34000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-34000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-34500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-34500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-34500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-35000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-35000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-35000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-35500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-35500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-35500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-36000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-36000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-36000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-36500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-36500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-36500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-37000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-37000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-37000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-37500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-37500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-37500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-38000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-38000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-38000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-38500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-38500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-38500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-39000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-39000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-39000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-39500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-39500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-39500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-40000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-40000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-40000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-40500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-40500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-40500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-41000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-41000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-41000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-41500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-41500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-41500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-42000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-42000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-42000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-42500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-42500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-42500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-43000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-43000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-43000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-43500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-43500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-43500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-44000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-44000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-44000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-44500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-44500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-44500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-45000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-45000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-45000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-45500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-45500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-45500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-46000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-46000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-46000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-46500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-46500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-46500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-47000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-47000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-47000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-47500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-47500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-47500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-48000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-48000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-48000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-48500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-48500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-48500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-49000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-49000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-49000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-49500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-49500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-49500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-50000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-50000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-50000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-50500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-50500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-50500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-51000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-51000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-51000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-51500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-51500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-51500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-52000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-52000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-52000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-52500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-52500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-52500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-53000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-53000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-53000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-53500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-53500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-53500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-54000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-54000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-54000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-54500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-54500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-54500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-55000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-55000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-55000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-55500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-55500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-55500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-56000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-56000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-56000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-56500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-56500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-56500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-57000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-57000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-57000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-57500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-57500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-57500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-58000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-58000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-58000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-58500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-58500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-58500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-59000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-59000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-59000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-59500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-59500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-59500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-60000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-60000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-60000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-60500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-60500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-60500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-61000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-61000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-61000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-61500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-61500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-61500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-62000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-62000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-62000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-62500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-62500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-62500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-63000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-63000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-63000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-63500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-63500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-63500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-64000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-64000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-64000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-64500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-64500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-64500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-65000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-65000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-65000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-65500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-65500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-65500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-66000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-66000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-66000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-66500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-66500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-66500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-67000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-67000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-67000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-67500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-67500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-67500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-68000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-68000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-68000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-68500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-68500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-68500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-69000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-69000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-69000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-69500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-69500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-69500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-70000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-70000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-70000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-70500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-70500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-70500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-71000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-71000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-71000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-71500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-71500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-71500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-72000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-72000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-72000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-72500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-72500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-72500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-73000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-73000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-73000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-73500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-73500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-73500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-74000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-74000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-74000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-74500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-74500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-74500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-75000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-75000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-75000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-75500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-75500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-75500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-76000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-76000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-76000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-76500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-76500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-76500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-77000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-77000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-77000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-77500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-77500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-77500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-78000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-78000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-78000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-78500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-78500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-78500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-79000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-79000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-79000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-79500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-79500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-79500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-80000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-80000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-80000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-80500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-80500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-80500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-81000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-81000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-81000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-81500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-81500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-81500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-82000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-82000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-82000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-82500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-82500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-82500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-83000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-83000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-83000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-83500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-83500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-83500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-84000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-84000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-84000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-84500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-84500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-84500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-85000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-85000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-85000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-85500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-85500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-85500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-86000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-86000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-86000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-86500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-86500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-86500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-87000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-87000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-87000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-87500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-87500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-87500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-88000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-88000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-88000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-88500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-88500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-88500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-89000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-89000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-89000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-89500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-89500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-89500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-90000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-90000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-90000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-90500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-90500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-90500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-91000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-91000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-91000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-91500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-91500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-91500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-92000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-92000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-92000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-92500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-92500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-92500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-93000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-93000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-93000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-93500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-93500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-93500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-94000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-94000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-94000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-94500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-94500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-94500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-95000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-95000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-95000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-95500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-95500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-95500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-96000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-96000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-96000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-96500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-96500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-96500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-97000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-97000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-97000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-97500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-97500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-97500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-98000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-98000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-98000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-98500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-98500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-98500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-99000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-99000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-99000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-99500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-99500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-99500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-100000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-100000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-100000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-100500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-100500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-100500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-101000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-101000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-101000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-101500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-101500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-101500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-102000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-102000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-102000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-102500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-102500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-102500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-103000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-103000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-103000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-103500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-103500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-103500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-104000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-104000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-104000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-104500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-104500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-104500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-105000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-105000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-105000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-105500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-105500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-105500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-106000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-106000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-106000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-106500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-106500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-106500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-107000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-107000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-107000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-107500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-107500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-107500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-108000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-108000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-108000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-108500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-108500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-108500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-109000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-109000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-109000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-109500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-109500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-109500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-110000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-110000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-110000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-110500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-110500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-110500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-111000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-111000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-111000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-111500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-111500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-111500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-112000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-112000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-112000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-112500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-112500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-112500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-113000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-113000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-113000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-113500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-113500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-113500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-114000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-114000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-114000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-114500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-114500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-114500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-115000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-115000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-115000/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-115500\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-115500/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-115500/pytorch_model.bin\n",
      "Saving model checkpoint to Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-116000\n",
      "Configuration saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-116000/config.json\n",
      "Model weights saved in Machine_text_1e7_weight2_classifier_checkpoints/checkpoint-116000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=116262, training_loss=0.3728683192406566, metrics={'train_runtime': 23094.5515, 'train_samples_per_second': 10.068, 'train_steps_per_second': 5.034, 'total_flos': 6858189468647424.0, 'train_loss': 0.3728683192406566, 'epoch': 18.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Start training\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db6d86b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0c230bb",
   "metadata": {},
   "source": [
    "# Lexis Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73200c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-081ceb0bf9c7c7cd\n",
      "Reusing dataset csv (/home/bart/.cache/huggingface/datasets/csv/default-081ceb0bf9c7c7cd/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d37a15e2494cf39870ff8dfe3659d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('csv', data_files={'train': './rucola/train.csv', 'test': './rucola/test.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc6d75b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_type = \"Lexis\"\n",
    "lexis_train_column = []\n",
    "lexis_test_column = []\n",
    "for row in dataset[\"train\"]:\n",
    "    if error_type in row[\"error_type\"]:\n",
    "        lexis_train_column.append(1)\n",
    "    else:\n",
    "        lexis_train_column.append(0)\n",
    "        \n",
    "for row in dataset[\"test\"]:\n",
    "    if error_type in row[\"error_type\"]:\n",
    "        lexis_test_column.append(1)\n",
    "    else:\n",
    "        lexis_test_column.append(0)\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].add_column(\"label\", lexis_train_column)\n",
    "dataset[\"test\"] = dataset[\"test\"].add_column(\"label\", lexis_test_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c674e590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Мне предоставилась возможность все видеть, сам оставаясь незамеченным.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = dataset[\"train\"].column_names\n",
    "column_names.remove(\"label\")\n",
    "column_names.remove(\"text\")\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].remove_columns(column_names)\n",
    "dataset[\"test\"] = dataset[\"test\"].remove_columns(column_names)\n",
    "\n",
    "dataset[\"train\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c28ac3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Они хотят, чтобы я разговаривал с вами.', 'label': 0}\n",
      "{'text': 'Через четверть часа он уже сидел в ресторане.', 'label': 0}\n",
      "{'text': 'Как я заметил еще из окна, каждый из них что-то нес.', 'label': 0}\n",
      "{'text': 'Придет он или нет, зависит от ряда обстоятельств.', 'label': 0}\n",
      "{'text': 'Ценность сведений установилась на основе предпочтений.', 'label': 0}\n",
      "{'text': 'Они приняли во внимание слухи, что Аня предаст Катю.', 'label': 0}\n",
      "{'text': 'На повороте дороги показалась коляска.', 'label': 0}\n",
      "{'text': 'Ученые заявили об истощении людей до степени худобы при отсутствии еды и воды.', 'label': 0}\n",
      "{'text': 'Он немедленно не ушел.', 'label': 0}\n",
      "{'text': 'Пиво он любит темное.', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(dataset[\"train\"][30 + i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3e0105e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08492684059766199\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "for row in dataset[\"train\"]:\n",
    "    if row[\"label\"] == 1: \n",
    "        counter += 1\n",
    "\n",
    "print(counter/len(dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "490125bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dbc49ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sberbank-ai/ruRoberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sberbank-ai/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading cached processed dataset at /home/bart/.cache/huggingface/datasets/csv/default-081ceb0bf9c7c7cd/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-793f140ab0b204f7.arrow\n",
      "Loading cached processed dataset at /home/bart/.cache/huggingface/datasets/csv/default-081ceb0bf9c7c7cd/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-4c0cd10d0d4f135b.arrow\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "from transformers import TrainingArguments, Trainer, AutoTokenizer\n",
    "from transformers.utils import logging\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "class CustomLexisClassifierTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([12.0, 1.0]).to(DEVICE))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Then get the dataset\n",
    "!export TRANSFORMERS_NO_ADVISORY_WARNINGS=1\n",
    "datasets = dataset\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"sberbank-ai/ruRoberta-large\",\n",
    "    num_labels=2,\n",
    "    attention_probs_dropout_prob=0.2,\n",
    "    hidden_dropout_prob=0.2,\n",
    "    classifier_dropout=0.2\n",
    ")\n",
    "\n",
    "f1_metric = load_metric(\"f1\")\n",
    "precision_metric = load_metric(\"precision\")\n",
    "recall_metric = load_metric(\"recall\")\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"Lexis_1e6_classifier_checkpoints\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=18,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=5e-7,\n",
    "    # log_level=\"error\",\n",
    "    logging_first_step=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/ruRoberta-large\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=20)\n",
    "\n",
    "datasets = copy.deepcopy(dataset)\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"]\n",
    "small_eval_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78ee61b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/bart/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12917\n",
      "  Num Epochs = 18\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 116262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14984' max='116262' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 14984/116262 1:12:26 < 8:09:44, 3.45 it/s, Epoch 2.32/18]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.530100</td>\n",
       "      <td>0.520525</td>\n",
       "      <td>0.915012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.498600</td>\n",
       "      <td>0.547664</td>\n",
       "      <td>0.915012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-500\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-500/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-1000\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-1000/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-1500\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-1500/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-2000\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-2000/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-2500\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-2500/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-3000\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-3000/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-3500\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-3500/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-4000\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-4000/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-4500\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-4500/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-5000\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-5000/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-5500\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-5500/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-6000\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-6000/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-6000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-6500\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-6500/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-7000\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-7000/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-7500\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-7500/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-8000\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-8000/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-8500\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-8500/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-9000\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-9000/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-9500\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-9500/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-10000\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-10000/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-10500\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-10500/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-11000\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-11000/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-11000/pytorch_model.bin\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-11500\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-11500/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-11500/pytorch_model.bin\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-12000\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-12000/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-12000/pytorch_model.bin\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-12500\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-12500/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-12500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1612\n",
      "  Batch size = 2\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-13000\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-13000/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-13000/pytorch_model.bin\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-13500\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-13500/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-13500/pytorch_model.bin\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-14000\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-14000/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-14000/pytorch_model.bin\n",
      "Saving model checkpoint to Lexis_1e6_classifier_checkpoints/checkpoint-14500\n",
      "Configuration saved in Lexis_1e6_classifier_checkpoints/checkpoint-14500/config.json\n",
      "Model weights saved in Lexis_1e6_classifier_checkpoints/checkpoint-14500/pytorch_model.bin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:1487\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1485\u001b[0m     optimizer_was_run \u001b[38;5;241m=\u001b[39m scale_before \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m scale_after\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1487\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_was_run \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed:\n\u001b[1;32m   1490\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/optimization.py:361\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    359\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta1))\n\u001b[1;32m    360\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m--> 361\u001b[0m denom \u001b[38;5;241m=\u001b[39m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m step_size \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m]:  \u001b[38;5;66;03m# No bias correction for Bert\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Start training\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cfd857",
   "metadata": {},
   "source": [
    "# Multilabel grammar classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e1da162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 16:48:24.794032: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-07 16:48:24.794056: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\n",
    "import pickle\n",
    "from transformers import *\n",
    "from tqdm import tqdm, trange\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37b9bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "915aa06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>authored_by</th>\n",
       "      <th>error_type</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>В 1929 году Ньюкомб переехал на Мальту в качес...</td>\n",
       "      <td>1</td>\n",
       "      <td>Machine</td>\n",
       "      <td>0</td>\n",
       "      <td>WikiMatrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Вдруг решетка беззвучно поехала в сторону, и н...</td>\n",
       "      <td>1</td>\n",
       "      <td>Expert</td>\n",
       "      <td>0</td>\n",
       "      <td>Paducheva2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Этим летом не никуда ездили.</td>\n",
       "      <td>0</td>\n",
       "      <td>Expert</td>\n",
       "      <td>Syntax</td>\n",
       "      <td>Rusgram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>В 2011 году был выпущен документальный фильм «...</td>\n",
       "      <td>1</td>\n",
       "      <td>Machine</td>\n",
       "      <td>0</td>\n",
       "      <td>WikiMatrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Поговорительница Пресли поссорилась с поп-звез...</td>\n",
       "      <td>0</td>\n",
       "      <td>Machine</td>\n",
       "      <td>Morphology</td>\n",
       "      <td>WikiMatrix</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  is_correct  \\\n",
       "0           0  В 1929 году Ньюкомб переехал на Мальту в качес...           1   \n",
       "1           1  Вдруг решетка беззвучно поехала в сторону, и н...           1   \n",
       "2           2                       Этим летом не никуда ездили.           0   \n",
       "3           3  В 2011 году был выпущен документальный фильм «...           1   \n",
       "4           4  Поговорительница Пресли поссорилась с поп-звез...           0   \n",
       "\n",
       "  authored_by  error_type         source  \n",
       "0     Machine           0     WikiMatrix  \n",
       "1      Expert           0  Paducheva2004  \n",
       "2      Expert      Syntax        Rusgram  \n",
       "3     Machine           0     WikiMatrix  \n",
       "4     Machine  Morphology     WikiMatrix  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('./rucola/train.csv')\n",
    "#  './rucola/test.csv'\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb975a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Lexis', '0', 'Hallucination', 'Commonsense', 'Syntax', 'Morphology', 'Semantics'}\n"
     ]
    }
   ],
   "source": [
    "error_types = set()\n",
    "for _, row in train_df.iterrows():\n",
    "    error_types.update(row[\"error_type\"].split(\"|\"))\n",
    "print(error_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4af785db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.assign(morphology=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "train_df = train_df.assign(hallucination=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "train_df = train_df.assign(lexis=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "train_df = train_df.assign(semantics=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "train_df = train_df.assign(syntax=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "train_df = train_df.assign(commonsense=pd.Series(np.zeros(len(train_df), dtype=int)).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1f44a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>authored_by</th>\n",
       "      <th>error_type</th>\n",
       "      <th>source</th>\n",
       "      <th>morphology</th>\n",
       "      <th>hallucination</th>\n",
       "      <th>lexis</th>\n",
       "      <th>semantics</th>\n",
       "      <th>syntax</th>\n",
       "      <th>commonsense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>В 1929 году Ньюкомб переехал на Мальту в качес...</td>\n",
       "      <td>1</td>\n",
       "      <td>Machine</td>\n",
       "      <td>0</td>\n",
       "      <td>WikiMatrix</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Вдруг решетка беззвучно поехала в сторону, и н...</td>\n",
       "      <td>1</td>\n",
       "      <td>Expert</td>\n",
       "      <td>0</td>\n",
       "      <td>Paducheva2004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Этим летом не никуда ездили.</td>\n",
       "      <td>0</td>\n",
       "      <td>Expert</td>\n",
       "      <td>Syntax</td>\n",
       "      <td>Rusgram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>В 2011 году был выпущен документальный фильм «...</td>\n",
       "      <td>1</td>\n",
       "      <td>Machine</td>\n",
       "      <td>0</td>\n",
       "      <td>WikiMatrix</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Поговорительница Пресли поссорилась с поп-звез...</td>\n",
       "      <td>0</td>\n",
       "      <td>Machine</td>\n",
       "      <td>Morphology</td>\n",
       "      <td>WikiMatrix</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  is_correct  \\\n",
       "0           0  В 1929 году Ньюкомб переехал на Мальту в качес...           1   \n",
       "1           1  Вдруг решетка беззвучно поехала в сторону, и н...           1   \n",
       "2           2                       Этим летом не никуда ездили.           0   \n",
       "3           3  В 2011 году был выпущен документальный фильм «...           1   \n",
       "4           4  Поговорительница Пресли поссорилась с поп-звез...           0   \n",
       "\n",
       "  authored_by  error_type         source  morphology  hallucination  lexis  \\\n",
       "0     Machine           0     WikiMatrix           0              0      0   \n",
       "1      Expert           0  Paducheva2004           0              0      0   \n",
       "2      Expert      Syntax        Rusgram           0              0      0   \n",
       "3     Machine           0     WikiMatrix           0              0      0   \n",
       "4     Machine  Morphology     WikiMatrix           0              0      0   \n",
       "\n",
       "   semantics  syntax  commonsense  \n",
       "0          0       0            0  \n",
       "1          0       0            0  \n",
       "2          0       0            0  \n",
       "3          0       0            0  \n",
       "4          0       0            0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd17cef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Lexis', 'Hallucination', 'Commonsense', 'Syntax', 'Morphology', 'Semantics'}\n"
     ]
    }
   ],
   "source": [
    "error_types = error_types - set(['0'])\n",
    "print(error_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60cab20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label columns:  ['morphology', 'hallucination', 'lexis', 'semantics', 'syntax', 'commonsense']\n"
     ]
    }
   ],
   "source": [
    "cols = train_df.columns\n",
    "label_cols = list(cols[6:])\n",
    "num_labels = len(label_cols)\n",
    "print('Label columns: ', label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ad23923",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in train_df.iterrows():\n",
    "    for error_type in error_types:\n",
    "        if error_type in row[\"error_type\"]:\n",
    "            train_df.at[i, error_type.lower()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbcc1a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>authored_by</th>\n",
       "      <th>error_type</th>\n",
       "      <th>source</th>\n",
       "      <th>morphology</th>\n",
       "      <th>hallucination</th>\n",
       "      <th>lexis</th>\n",
       "      <th>semantics</th>\n",
       "      <th>syntax</th>\n",
       "      <th>commonsense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>В 1929 году Ньюкомб переехал на Мальту в качес...</td>\n",
       "      <td>1</td>\n",
       "      <td>Machine</td>\n",
       "      <td>0</td>\n",
       "      <td>WikiMatrix</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Вдруг решетка беззвучно поехала в сторону, и н...</td>\n",
       "      <td>1</td>\n",
       "      <td>Expert</td>\n",
       "      <td>0</td>\n",
       "      <td>Paducheva2004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Этим летом не никуда ездили.</td>\n",
       "      <td>0</td>\n",
       "      <td>Expert</td>\n",
       "      <td>Syntax</td>\n",
       "      <td>Rusgram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>В 2011 году был выпущен документальный фильм «...</td>\n",
       "      <td>1</td>\n",
       "      <td>Machine</td>\n",
       "      <td>0</td>\n",
       "      <td>WikiMatrix</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Поговорительница Пресли поссорилась с поп-звез...</td>\n",
       "      <td>0</td>\n",
       "      <td>Machine</td>\n",
       "      <td>Morphology</td>\n",
       "      <td>WikiMatrix</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  is_correct  \\\n",
       "0           0  В 1929 году Ньюкомб переехал на Мальту в качес...           1   \n",
       "1           1  Вдруг решетка беззвучно поехала в сторону, и н...           1   \n",
       "2           2                       Этим летом не никуда ездили.           0   \n",
       "3           3  В 2011 году был выпущен документальный фильм «...           1   \n",
       "4           4  Поговорительница Пресли поссорилась с поп-звез...           0   \n",
       "\n",
       "  authored_by  error_type         source  morphology  hallucination  lexis  \\\n",
       "0     Machine           0     WikiMatrix           0              0      0   \n",
       "1      Expert           0  Paducheva2004           0              0      0   \n",
       "2      Expert      Syntax        Rusgram           0              0      0   \n",
       "3     Machine           0     WikiMatrix           0              0      0   \n",
       "4     Machine  Morphology     WikiMatrix           1              0      0   \n",
       "\n",
       "   semantics  syntax  commonsense  \n",
       "0          0       0            0  \n",
       "1          0       0            0  \n",
       "2          0       1            0  \n",
       "3          0       0            0  \n",
       "4          0       0            0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb202c2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>authored_by</th>\n",
       "      <th>error_type</th>\n",
       "      <th>source</th>\n",
       "      <th>morphology</th>\n",
       "      <th>hallucination</th>\n",
       "      <th>lexis</th>\n",
       "      <th>semantics</th>\n",
       "      <th>syntax</th>\n",
       "      <th>commonsense</th>\n",
       "      <th>one_hot_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>В 1929 году Ньюкомб переехал на Мальту в качес...</td>\n",
       "      <td>1</td>\n",
       "      <td>Machine</td>\n",
       "      <td>0</td>\n",
       "      <td>WikiMatrix</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Вдруг решетка беззвучно поехала в сторону, и н...</td>\n",
       "      <td>1</td>\n",
       "      <td>Expert</td>\n",
       "      <td>0</td>\n",
       "      <td>Paducheva2004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Этим летом не никуда ездили.</td>\n",
       "      <td>0</td>\n",
       "      <td>Expert</td>\n",
       "      <td>Syntax</td>\n",
       "      <td>Rusgram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>В 2011 году был выпущен документальный фильм «...</td>\n",
       "      <td>1</td>\n",
       "      <td>Machine</td>\n",
       "      <td>0</td>\n",
       "      <td>WikiMatrix</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Поговорительница Пресли поссорилась с поп-звез...</td>\n",
       "      <td>0</td>\n",
       "      <td>Machine</td>\n",
       "      <td>Morphology</td>\n",
       "      <td>WikiMatrix</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  is_correct  \\\n",
       "0           0  В 1929 году Ньюкомб переехал на Мальту в качес...           1   \n",
       "1           1  Вдруг решетка беззвучно поехала в сторону, и н...           1   \n",
       "2           2                       Этим летом не никуда ездили.           0   \n",
       "3           3  В 2011 году был выпущен документальный фильм «...           1   \n",
       "4           4  Поговорительница Пресли поссорилась с поп-звез...           0   \n",
       "\n",
       "  authored_by  error_type         source  morphology  hallucination  lexis  \\\n",
       "0     Machine           0     WikiMatrix           0              0      0   \n",
       "1      Expert           0  Paducheva2004           0              0      0   \n",
       "2      Expert      Syntax        Rusgram           0              0      0   \n",
       "3     Machine           0     WikiMatrix           0              0      0   \n",
       "4     Machine  Morphology     WikiMatrix           1              0      0   \n",
       "\n",
       "   semantics  syntax  commonsense      one_hot_labels  \n",
       "0          0       0            0  [0, 0, 0, 0, 0, 0]  \n",
       "1          0       0            0  [0, 0, 0, 0, 0, 0]  \n",
       "2          0       1            0  [0, 0, 0, 0, 1, 0]  \n",
       "3          0       0            0  [0, 0, 0, 0, 0, 0]  \n",
       "4          0       0            0  [1, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['one_hot_labels'] = list(train_df[label_cols].values)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f4bb564",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(train_df.one_hot_labels.values)\n",
    "texts = list(train_df.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58862c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(labels[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb81efa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В 1929 году Ньюкомб переехал на Мальту в качестве главного инженера, а в 1932 году ушел в отставку.\n",
      "Вдруг решетка беззвучно поехала в сторону, и на балконе возникла таинственная фигура, прячущаяся от лунного света, и погрозила Ивану пальцем.\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe76f415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/vocab.json from cache at /home/bart/.cache/huggingface/transformers/01c769031f99defa27c759f85c0f21715d1506ed5c4ab01f7d7ef8c8b3b5674a.10df436f410c7709f54ad2bf7e973429f03c613012be0df709996d5aef3706af\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/merges.txt from cache at /home/bart/.cache/huggingface/transformers/6d742b9584f250f0088e6a6674f80ae706d8a390d6bc30f7cfaccb4292556f55.e4807b2d97082a2dfbf6623d7aa973a5057f87e44b329ec0b63daafd5d2ef593\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/config.json from cache at /home/bart/.cache/huggingface/transformers/191cf2edd2e8d2007f9a39479b45315ac1b79fe23fa4786511de1c8f9b9def46.ebd71ffa9cb74c7d1d39cf67343e2ae8465e2091f9fb70dbd5dec0cf58b1b60b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"sberbank-ai/ruRoberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer outputs:  dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoTokenizer, RobertaTokenizer\n",
    "\n",
    "\n",
    "# max_length = 100\n",
    "tokenizer = RobertaTokenizer.from_pretrained('sberbank-ai/ruRoberta-large', do_lower_case=False) # tokenizer\n",
    "encodings = tokenizer.batch_encode_plus(texts, padding='longest') # tokenizer's encoding method\n",
    "print('tokenizer outputs: ', encodings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f66c040",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encodings['input_ids'] # tokenized and encoded sentences\n",
    "# token_type_ids = encodings['token_type_ids'] # token type ids\n",
    "attention_masks = encodings['attention_mask'] # attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6454a7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df label indices with only one instance:  [12907, 12895]\n"
     ]
    }
   ],
   "source": [
    "# Identifying indices of 'one_hot_labels' entries that only occur once - this will allow us to stratify split our training data later\n",
    "label_counts = train_df.one_hot_labels.astype(str).value_counts()\n",
    "one_freq = label_counts[label_counts==1].keys()\n",
    "one_freq_idxs = sorted(list(train_df[train_df.one_hot_labels.astype(str).isin(one_freq)].index), reverse=True)\n",
    "print('df label indices with only one instance: ', one_freq_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29c53860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering single instance inputs to force into the training set after stratified split\n",
    "one_freq_input_ids = [input_ids.pop(i) for i in one_freq_idxs]\n",
    "# one_freq_token_types = [token_type_ids.pop(i) for i in one_freq_idxs]\n",
    "one_freq_attention_masks = [attention_masks.pop(i) for i in one_freq_idxs]\n",
    "one_freq_labels = [labels.pop(i) for i in one_freq_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62a644aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 596, 22981, 1169, 5802, 818, 315, 24360, 324, 8558, 1714, 281, 3396, 7882, 29560, 16, 376, 281, 22174, 1169, 5611, 281, 14878, 18, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfb3d9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8422/1120849839.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  train_labels = torch.tensor(train_labels)\n"
     ]
    }
   ],
   "source": [
    "# Use train_test_split to split our data into train and validation sets\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels, attention_masks,\n",
    "        random_state=2020, test_size=0.10, stratify = labels)\n",
    "\n",
    "# Add one frequency data to train data\n",
    "train_inputs.extend(one_freq_input_ids)\n",
    "train_labels.extend(one_freq_labels)\n",
    "train_masks.extend(one_freq_attention_masks)\n",
    "# train_token_types.extend(one_freq_token_types)\n",
    "\n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "# train_token_types = torch.tensor(train_token_types)\n",
    "\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "# validation_token_types = torch.tensor(validation_token_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93368ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0baf60f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning with XLNet, the authors recommend a batch size of 32, 48, or 128. We will use 32 here to avoid memory issues.\n",
    "batch_size = 13\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbc66853",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# i = 0\n",
    "# for i, batch in enumerate(validation_dataloader):\n",
    "#     print(batch)\n",
    "#     print(================================\")b_logit_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6292a835",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(validation_dataloader, 'validation_data_loader')\n",
    "torch.save(train_dataloader, 'train_data_loader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca782c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/config.json from cache at /home/bart/.cache/huggingface/transformers/191cf2edd2e8d2007f9a39479b45315ac1b79fe23fa4786511de1c8f9b9def46.ebd71ffa9cb74c7d1d39cf67343e2ae8465e2091f9fb70dbd5dec0cf58b1b60b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/home/jovyan/models/roberta/roberta_l\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/pytorch_model.bin from cache at /home/bart/.cache/huggingface/transformers/57935eccbb92bb4e5465411da844aeb075cac5b8c255c770a81463fa909d8ccc.f03af7b16e88082afbdd696d0647fe6bf624c0776d71dcece2b0e9973d081b26\n",
      "Some weights of the model checkpoint at sberbank-ai/ruRoberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sberbank-ai/ruRoberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"sberbank-ai/ruRoberta-large\", num_labels=num_labels)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49f717b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39497a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bart/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-8, correct_bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699032f0",
   "metadata": {},
   "source": [
    "Commonsense: 0.009\n",
    "Lexis: 0.085\n",
    "Morphology: 0.055\n",
    "Hallucination: 0.034\n",
    "Syntax: 0.192\n",
    "Semantics: 0.074\n",
    "\n",
    "morphology\thallucination\tlexis\tsemantics\tsyntax\tcommonsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62231787",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trange' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# trange is a tqdm wrapper around the normal python range\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_num \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrange\u001b[49m(epochs, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Set our model to training mode (as opposed to evaluation mode)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Tracking variables\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trange' is not defined"
     ]
    }
   ],
   "source": [
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 10\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for epoch_num in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "    # Training\n",
    "\n",
    "    # Set our model to training mode (as opposed to evaluation mode)\n",
    "    model.train()\n",
    "\n",
    "    # Tracking variables\n",
    "    tr_loss = 0 #running loss\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "  # Train the data for one epoch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # # Forward pass for multiclass classification\n",
    "        # outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        # loss = outputs[0]\n",
    "        # logits = outputs[1]\n",
    "\n",
    "        # Forward pass for multilabel classification\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = outputs[0]\n",
    "        loss_func = BCEWithLogitsLoss(weight=torch.Tensor([20/5, 20/10, 20/8, 20/7, 20/9, 20/15]).to(device)) \n",
    "        loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "        # loss_func = BCELoss() \n",
    "        # loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "        train_loss_set.append(loss.item())    \n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "    # Validation\n",
    "\n",
    "    # Put model in evaluation mode to evaluate loss on the validation set\n",
    "    model.eval()\n",
    "\n",
    "    # Variables to gather full output\n",
    "    logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n",
    "\n",
    "  # Predict\n",
    "    for i, batch in enumerate(validation_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "      # Forward pass\n",
    "            outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            b_logit_pred = outs[0]\n",
    "            pred_label = torch.sigmoid(b_logit_pred)\n",
    "\n",
    "            b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "            pred_label = pred_label.to('cpu').numpy()\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "        tokenized_texts.append(b_input_ids)\n",
    "        logit_preds.append(b_logit_pred)\n",
    "        true_labels.append(b_labels)\n",
    "        pred_labels.append(pred_label)\n",
    "        \n",
    "#         print(\"TRUE LABELS\", true_labels)\n",
    "#         print(\"PRED LABELS\", pred_labels)\n",
    "\n",
    "  # Flatten outputs\n",
    "    pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    threshold = 0.50\n",
    "    pred_bools = [pl>threshold for pl in pred_labels]\n",
    "    true_bools = [tl==1 for tl in true_labels]\n",
    "    \n",
    "    macro_thresholds = np.array(range(1,10))/10\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in macro_thresholds:\n",
    "        pred_bools = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_macro_th = macro_thresholds[np.argmax(f1_results)] #best macro threshold value\n",
    "\n",
    "    micro_thresholds = (np.array(range(10))/100)+best_macro_th #calculating micro threshold values\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in micro_thresholds:\n",
    "        pred_bools = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_f1_idx = np.argmax(f1_results) #best threshold value\n",
    "\n",
    "    # Printing and saving classification report\n",
    "    print('Best Threshold: ', micro_thresholds[best_f1_idx])\n",
    "    print('Test F1 Accuracy: ', f1_results[best_f1_idx])\n",
    "    print('Test Flat Accuracy: ', flat_acc_results[best_f1_idx], '\\n')\n",
    "\n",
    "    best_pred_bools = [pl>micro_thresholds[best_f1_idx] for pl in pred_labels]\n",
    "    clf_report_optimized = classification_report(true_bools,best_pred_bools, target_names=label_cols)\n",
    "    pickle.dump(clf_report_optimized, open('classification_report_optimized.txt','wb'))\n",
    "    print(clf_report_optimized)\n",
    "    \n",
    "    val_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')*100\n",
    "    val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
    "\n",
    "    print('F1 Validation Accuracy: ', val_f1_accuracy)\n",
    "    print('Flat Validation Accuracy: ', val_flat_accuracy)\n",
    "    torch.save(model.state_dict(), f'new_roberta_grammatic_{epoch_num}_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf16b205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                                                                                                                                                                                          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.8031140979229481\n",
      "Best Threshold:  0.17\n",
      "Test F1 Accuracy:  0.2925018559762435\n",
      "Test Flat Accuracy:  0.44040247678018574 \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   morphology       0.00      0.00      0.00        72\n",
      "hallucination       0.00      0.00      0.00        46\n",
      "        lexis       0.00      0.00      0.00       110\n",
      "    semantics       0.00      0.00      0.00        98\n",
      "       syntax       0.26      0.80      0.39       245\n",
      "  commonsense       0.00      0.00      0.00        13\n",
      "\n",
      "    micro avg       0.26      0.34      0.29       584\n",
      "    macro avg       0.04      0.13      0.07       584\n",
      " weighted avg       0.11      0.34      0.16       584\n",
      "  samples avg       0.15      0.14      0.14       584\n",
      "\n",
      "F1 Validation Accuracy:  24.92522432701894\n",
      "Flat Validation Accuracy:  54.41176470588235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch:  10%|████████████████████▉                                                                                                                                                                                            | 1/10 [05:41<51:09, 341.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.7980487536412152\n",
      "Best Threshold:  0.17\n",
      "Test F1 Accuracy:  0.29680365296803657\n",
      "Test Flat Accuracy:  0.45897832817337464 \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   morphology       0.00      0.00      0.00        72\n",
      "hallucination       0.00      0.00      0.00        46\n",
      "        lexis       0.00      0.00      0.00       110\n",
      "    semantics       0.00      0.00      0.00        98\n",
      "       syntax       0.27      0.80      0.40       245\n",
      "  commonsense       0.00      0.00      0.00        13\n",
      "\n",
      "    micro avg       0.27      0.33      0.30       584\n",
      "    macro avg       0.04      0.13      0.07       584\n",
      " weighted avg       0.11      0.33      0.17       584\n",
      "  samples avg       0.15      0.14      0.14       584\n",
      "\n",
      "F1 Validation Accuracy:  24.52642073778664\n",
      "Flat Validation Accuracy:  54.179566563467496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch:  20%|█████████████████████████████████████████▊                                                                                                                                                                       | 2/10 [11:18<45:09, 338.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.7935602168857252\n",
      "Best Threshold:  0.17\n",
      "Test F1 Accuracy:  0.299298519095869\n",
      "Test Flat Accuracy:  0.47368421052631576 \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   morphology       0.00      0.00      0.00        72\n",
      "hallucination       0.00      0.00      0.00        46\n",
      "        lexis       0.00      0.00      0.00       110\n",
      "    semantics       0.00      0.00      0.00        98\n",
      "       syntax       0.27      0.78      0.41       245\n",
      "  commonsense       0.00      0.00      0.00        13\n",
      "\n",
      "    micro avg       0.27      0.33      0.30       584\n",
      "    macro avg       0.05      0.13      0.07       584\n",
      " weighted avg       0.12      0.33      0.17       584\n",
      "  samples avg       0.15      0.14      0.14       584\n",
      "\n",
      "F1 Validation Accuracy:  24.901185770750985\n",
      "Flat Validation Accuracy:  54.10216718266254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch:  30%|██████████████████████████████████████████████████████████████▋                                                                                                                                                  | 3/10 [16:53<39:19, 337.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.7894908929756921\n",
      "Best Threshold:  0.16\n",
      "Test F1 Accuracy:  0.2992125984251968\n",
      "Test Flat Accuracy:  0.4264705882352941 \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   morphology       0.00      0.00      0.00        72\n",
      "hallucination       0.00      0.00      0.00        46\n",
      "        lexis       0.00      0.00      0.00       110\n",
      "    semantics       0.00      0.00      0.00        98\n",
      "       syntax       0.26      0.85      0.40       245\n",
      "  commonsense       0.00      0.00      0.00        13\n",
      "\n",
      "    micro avg       0.26      0.36      0.30       584\n",
      "    macro avg       0.04      0.14      0.07       584\n",
      " weighted avg       0.11      0.36      0.17       584\n",
      "  samples avg       0.16      0.15      0.15       584\n",
      "\n",
      "F1 Validation Accuracy:  25.750242013552754\n",
      "Flat Validation Accuracy:  53.94736842105263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch:  40%|███████████████████████████████████████████████████████████████████████████████████▌                                                                                                                             | 4/10 [22:33<33:50, 338.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.7875714842749085\n",
      "Best Threshold:  0.16\n",
      "Test F1 Accuracy:  0.29753265602322204\n",
      "Test Flat Accuracy:  0.4311145510835913 \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   morphology       0.00      0.00      0.00        72\n",
      "hallucination       0.00      0.00      0.00        46\n",
      "        lexis       0.00      0.00      0.00       110\n",
      "    semantics       0.00      0.00      0.00        98\n",
      "       syntax       0.26      0.84      0.39       245\n",
      "  commonsense       0.00      0.00      0.00        13\n",
      "\n",
      "    micro avg       0.26      0.35      0.30       584\n",
      "    macro avg       0.04      0.14      0.07       584\n",
      " weighted avg       0.11      0.35      0.17       584\n",
      "  samples avg       0.16      0.14      0.15       584\n",
      "\n",
      "F1 Validation Accuracy:  26.440037771482526\n",
      "Flat Validation Accuracy:  53.40557275541795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch:  50%|████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                        | 5/10 [28:16<28:20, 340.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.7841048996330415\n",
      "Best Threshold:  0.16\n",
      "Test F1 Accuracy:  0.2991893883566691\n",
      "Test Flat Accuracy:  0.44272445820433437 \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   morphology       0.00      0.00      0.00        72\n",
      "hallucination       0.00      0.00      0.00        46\n",
      "        lexis       0.00      0.00      0.00       110\n",
      "    semantics       0.00      0.00      0.00        98\n",
      "       syntax       0.26      0.83      0.40       245\n",
      "  commonsense       0.00      0.00      0.00        13\n",
      "\n",
      "    micro avg       0.26      0.35      0.30       584\n",
      "    macro avg       0.04      0.14      0.07       584\n",
      " weighted avg       0.11      0.35      0.17       584\n",
      "  samples avg       0.16      0.14      0.15       584\n",
      "\n",
      "F1 Validation Accuracy:  26.75397567820393\n",
      "Flat Validation Accuracy:  53.1733746130031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch:  60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                   | 6/10 [33:51<22:32, 338.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.7824711601246991\n",
      "Best Threshold:  0.16\n",
      "Test F1 Accuracy:  0.30208333333333337\n",
      "Test Flat Accuracy:  0.45123839009287925 \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   morphology       0.00      0.00      0.00        72\n",
      "hallucination       0.00      0.00      0.00        46\n",
      "        lexis       0.00      0.00      0.00       110\n",
      "    semantics       0.00      0.00      0.00        98\n",
      "       syntax       0.27      0.83      0.40       245\n",
      "  commonsense       0.00      0.00      0.00        13\n",
      "\n",
      "    micro avg       0.27      0.35      0.30       584\n",
      "    macro avg       0.04      0.14      0.07       584\n",
      " weighted avg       0.11      0.35      0.17       584\n",
      "  samples avg       0.16      0.14      0.15       584\n",
      "\n",
      "F1 Validation Accuracy:  27.272727272727266\n",
      "Flat Validation Accuracy:  53.1733746130031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch:  70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                              | 7/10 [39:25<16:50, 336.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.7798087680241395\n",
      "Best Threshold:  0.15000000000000002\n",
      "Test F1 Accuracy:  0.3007730147575545\n",
      "Test Flat Accuracy:  0.4171826625386997 \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   morphology       0.00      0.00      0.00        72\n",
      "hallucination       0.00      0.00      0.00        46\n",
      "        lexis       0.00      0.00      0.00       110\n",
      "    semantics       0.00      0.00      0.00        98\n",
      "       syntax       0.26      0.87      0.39       245\n",
      "  commonsense       0.00      0.00      0.00        13\n",
      "\n",
      "    micro avg       0.26      0.37      0.30       584\n",
      "    macro avg       0.04      0.15      0.07       584\n",
      " weighted avg       0.11      0.37      0.17       584\n",
      "  samples avg       0.17      0.15      0.16       584\n",
      "\n",
      "F1 Validation Accuracy:  27.490774907749078\n",
      "Flat Validation Accuracy:  53.1733746130031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch:  80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                         | 8/10 [44:59<11:11, 335.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.7772336910691178\n",
      "Best Threshold:  0.15000000000000002\n",
      "Test F1 Accuracy:  0.3033788641265277\n",
      "Test Flat Accuracy:  0.4342105263157895 \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   morphology       0.00      0.00      0.00        72\n",
      "hallucination       0.00      0.00      0.00        46\n",
      "        lexis       0.00      0.00      0.00       110\n",
      "    semantics       0.00      0.00      0.00        98\n",
      "       syntax       0.26      0.86      0.40       245\n",
      "  commonsense       0.00      0.00      0.00        13\n",
      "\n",
      "    micro avg       0.26      0.36      0.30       584\n",
      "    macro avg       0.04      0.14      0.07       584\n",
      " weighted avg       0.11      0.36      0.17       584\n",
      "  samples avg       0.16      0.15      0.15       584\n",
      "\n",
      "F1 Validation Accuracy:  27.598896044158234\n",
      "Flat Validation Accuracy:  53.09597523219814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch:  90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                     | 9/10 [50:31<05:34, 334.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.7753338163912726\n",
      "Best Threshold:  0.15000000000000002\n",
      "Test F1 Accuracy:  0.3029861616897305\n",
      "Test Flat Accuracy:  0.4411764705882353 \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   morphology       0.00      0.00      0.00        72\n",
      "hallucination       0.00      0.00      0.00        46\n",
      "        lexis       0.00      0.00      0.00       110\n",
      "    semantics       0.00      0.00      0.00        98\n",
      "       syntax       0.26      0.85      0.40       245\n",
      "  commonsense       0.00      0.00      0.00        13\n",
      "\n",
      "    micro avg       0.26      0.36      0.30       584\n",
      "    macro avg       0.04      0.14      0.07       584\n",
      " weighted avg       0.11      0.36      0.17       584\n",
      "  samples avg       0.16      0.15      0.15       584\n",
      "\n",
      "F1 Validation Accuracy:  27.472527472527474\n",
      "Flat Validation Accuracy:  52.78637770897833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bart/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [56:01<00:00, 336.17s/it]\n"
     ]
    }
   ],
   "source": [
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 10\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for epoch_num in trange(10, 10 + epochs, desc=\"Epoch\"):\n",
    "\n",
    "    # Training\n",
    "\n",
    "    # Set our model to training mode (as opposed to evaluation mode)\n",
    "    model.train()\n",
    "\n",
    "    # Tracking variables\n",
    "    tr_loss = 0 #running loss\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "  # Train the data for one epoch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # # Forward pass for multiclass classification\n",
    "        # outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        # loss = outputs[0]\n",
    "        # logits = outputs[1]\n",
    "\n",
    "        # Forward pass for multilabel classification\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = outputs[0]\n",
    "        loss_func = BCEWithLogitsLoss(weight=torch.Tensor([20/5, 20/10, 20/8, 20/7, 20/5, 20/10]).to(device)) \n",
    "        loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "        # loss_func = BCELoss() \n",
    "        # loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "        train_loss_set.append(loss.item())    \n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "    # Validation\n",
    "\n",
    "    # Put model in evaluation mode to evaluate loss on the validation set\n",
    "    model.eval()\n",
    "\n",
    "    # Variables to gather full output\n",
    "    logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n",
    "\n",
    "  # Predict\n",
    "    for i, batch in enumerate(validation_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "      # Forward pass\n",
    "            outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            b_logit_pred = outs[0]\n",
    "            pred_label = torch.sigmoid(b_logit_pred)\n",
    "\n",
    "            b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "            pred_label = pred_label.to('cpu').numpy()\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "        tokenized_texts.append(b_input_ids)\n",
    "        logit_preds.append(b_logit_pred)\n",
    "        true_labels.append(b_labels)\n",
    "        pred_labels.append(pred_label)\n",
    "        \n",
    "#         print(\"TRUE LABELS\", true_labels)\n",
    "#         print(\"PRED LABELS\", pred_labels)\n",
    "\n",
    "  # Flatten outputs\n",
    "    pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    threshold = 0.50\n",
    "    pred_bools = [pl>threshold for pl in pred_labels]\n",
    "    true_bools = [tl==1 for tl in true_labels]\n",
    "    \n",
    "    macro_thresholds = np.array(range(1,10))/10\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in macro_thresholds:\n",
    "        pred_bools = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_macro_th = macro_thresholds[np.argmax(f1_results)] #best macro threshold value\n",
    "\n",
    "    micro_thresholds = (np.array(range(10))/100)+best_macro_th #calculating micro threshold values\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in micro_thresholds:\n",
    "        pred_bools = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_f1_idx = np.argmax(f1_results) #best threshold value\n",
    "\n",
    "    # Printing and saving classification report\n",
    "    print('Best Threshold: ', micro_thresholds[best_f1_idx])\n",
    "    print('Test F1 Accuracy: ', f1_results[best_f1_idx])\n",
    "    print('Test Flat Accuracy: ', flat_acc_results[best_f1_idx], '\\n')\n",
    "\n",
    "    best_pred_bools = [pl>micro_thresholds[best_f1_idx] for pl in pred_labels]\n",
    "    clf_report_optimized = classification_report(true_bools,best_pred_bools, target_names=label_cols)\n",
    "    pickle.dump(clf_report_optimized, open('classification_report_optimized.txt','wb'))\n",
    "    print(clf_report_optimized)\n",
    "    \n",
    "    val_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')*100\n",
    "    val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
    "\n",
    "    print('F1 Validation Accuracy: ', val_f1_accuracy)\n",
    "    print('Flat Validation Accuracy: ', val_flat_accuracy)\n",
    "    torch.save(model.state_dict(), f'roberta_grammatic_{epoch_num}_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b75f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'roberta_grammatic_3epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b61e196",
   "metadata": {},
   "source": [
    "# Narrow grammar classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "870e1dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 21:31:35.300345: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-07 21:31:35.300364: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\n",
    "import pickle\n",
    "from transformers import *\n",
    "from tqdm import tqdm, trange\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54d0f837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>authored_by</th>\n",
       "      <th>error_type</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>В 1929 году Ньюкомб переехал на Мальту в качес...</td>\n",
       "      <td>1</td>\n",
       "      <td>Machine</td>\n",
       "      <td>0</td>\n",
       "      <td>WikiMatrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Вдруг решетка беззвучно поехала в сторону, и н...</td>\n",
       "      <td>1</td>\n",
       "      <td>Expert</td>\n",
       "      <td>0</td>\n",
       "      <td>Paducheva2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Этим летом не никуда ездили.</td>\n",
       "      <td>0</td>\n",
       "      <td>Expert</td>\n",
       "      <td>Syntax</td>\n",
       "      <td>Rusgram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>В 2011 году был выпущен документальный фильм «...</td>\n",
       "      <td>1</td>\n",
       "      <td>Machine</td>\n",
       "      <td>0</td>\n",
       "      <td>WikiMatrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Поговорительница Пресли поссорилась с поп-звез...</td>\n",
       "      <td>0</td>\n",
       "      <td>Machine</td>\n",
       "      <td>Morphology</td>\n",
       "      <td>WikiMatrix</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  is_correct  \\\n",
       "0           0  В 1929 году Ньюкомб переехал на Мальту в качес...           1   \n",
       "1           1  Вдруг решетка беззвучно поехала в сторону, и н...           1   \n",
       "2           2                       Этим летом не никуда ездили.           0   \n",
       "3           3  В 2011 году был выпущен документальный фильм «...           1   \n",
       "4           4  Поговорительница Пресли поссорилась с поп-звез...           0   \n",
       "\n",
       "  authored_by  error_type         source  \n",
       "0     Machine           0     WikiMatrix  \n",
       "1      Expert           0  Paducheva2004  \n",
       "2      Expert      Syntax        Rusgram  \n",
       "3     Machine           0     WikiMatrix  \n",
       "4     Machine  Morphology     WikiMatrix  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('./rucola/train.csv')\n",
    "#  './rucola/test.csv'\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e11a3197",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b0650c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Morphology', 'Semantics', 'Syntax', '0', 'Hallucination', 'Lexis', 'Commonsense'}\n"
     ]
    }
   ],
   "source": [
    "error_types = set()\n",
    "for _, row in train_df.iterrows():\n",
    "    error_types.update(row[\"error_type\"].split(\"|\"))\n",
    "print(error_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9ac5fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df.assign(morphology=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(hallucination=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(lexis=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(semantics=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "train_df = train_df.assign(syntax=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(commonsense=pd.Series(np.zeros(len(train_df), dtype=int)).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1304fd",
   "metadata": {},
   "source": [
    "Commonsense: 0.009\n",
    "Lexis: 0.085\n",
    "Morphology: 0.055\n",
    "Hallucination: 0.034\n",
    "Syntax: 0.192\n",
    "Semantics: 0.074\n",
    "\n",
    "morphology\thallucination\tlexis\tsemantics\tsyntax\tcommonsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e2db407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Syntax'}\n"
     ]
    }
   ],
   "source": [
    "error_types = set([\"Syntax\"])\n",
    "print(error_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5e0888b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label columns:  ['syntax']\n"
     ]
    }
   ],
   "source": [
    "cols = train_df.columns\n",
    "label_cols = list(cols[6:])\n",
    "num_labels = len(label_cols)\n",
    "print('Label columns: ', label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d238948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in train_df.iterrows():\n",
    "    for error_type in error_types:\n",
    "        if error_type in row[\"error_type\"]:\n",
    "            train_df.at[i, error_type.lower()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12c7c4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['one_hot_labels'] = list(train_df[label_cols].values)\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "586435f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(train_df.syntax.values)\n",
    "texts = list(train_df.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c24922ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/vocab.json from cache at /home/bart/.cache/huggingface/transformers/01c769031f99defa27c759f85c0f21715d1506ed5c4ab01f7d7ef8c8b3b5674a.10df436f410c7709f54ad2bf7e973429f03c613012be0df709996d5aef3706af\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/merges.txt from cache at /home/bart/.cache/huggingface/transformers/6d742b9584f250f0088e6a6674f80ae706d8a390d6bc30f7cfaccb4292556f55.e4807b2d97082a2dfbf6623d7aa973a5057f87e44b329ec0b63daafd5d2ef593\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/config.json from cache at /home/bart/.cache/huggingface/transformers/191cf2edd2e8d2007f9a39479b45315ac1b79fe23fa4786511de1c8f9b9def46.ebd71ffa9cb74c7d1d39cf67343e2ae8465e2091f9fb70dbd5dec0cf58b1b60b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"sberbank-ai/ruRoberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer outputs:  dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoTokenizer, RobertaTokenizer\n",
    "\n",
    "\n",
    "# max_length = 100\n",
    "tokenizer = RobertaTokenizer.from_pretrained('sberbank-ai/ruRoberta-large', do_lower_case=False) # tokenizer\n",
    "encodings = tokenizer.batch_encode_plus(texts, padding='longest') # tokenizer's encoding method\n",
    "print('tokenizer outputs: ', encodings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb605b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encodings['input_ids'] # tokenized and encoded sentences\n",
    "attention_masks = encodings['attention_mask'] # attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "910d73b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df label indices with only one instance:  []\n"
     ]
    }
   ],
   "source": [
    "# Identifying indices of 'one_hot_labels' entries that only occur once - this will allow us to stratify split our training data later\n",
    "label_counts = train_df.syntax.astype(str).value_counts()\n",
    "one_freq = label_counts[label_counts==1].keys()\n",
    "one_freq_idxs = sorted(list(train_df[train_df.syntax.astype(str).isin(one_freq)].index), reverse=True)\n",
    "print('df label indices with only one instance: ', one_freq_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "145a17bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering single instance inputs to force into the training set after stratified split\n",
    "one_freq_input_ids = [input_ids.pop(i) for i in one_freq_idxs]\n",
    "one_freq_attention_masks = [attention_masks.pop(i) for i in one_freq_idxs]\n",
    "one_freq_labels = [labels.pop(i) for i in one_freq_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da7d9ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels, attention_masks,\n",
    "        random_state=2020, test_size=0.10, stratify = labels)\n",
    "\n",
    "# Add one frequency data to train data\n",
    "train_inputs.extend(one_freq_input_ids)\n",
    "train_labels.extend(one_freq_labels)\n",
    "train_masks.extend(one_freq_attention_masks)\n",
    "\n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea61f7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning with XLNet, the authors recommend a batch size of 32, 48, or 128. We will use 32 here to avoid memory issues.\n",
    "batch_size = 14\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "916a7696",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(validation_dataloader, 'validation_data_loader')\n",
    "torch.save(train_dataloader, 'train_data_loader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78e85052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/config.json from cache at /home/bart/.cache/huggingface/transformers/191cf2edd2e8d2007f9a39479b45315ac1b79fe23fa4786511de1c8f9b9def46.ebd71ffa9cb74c7d1d39cf67343e2ae8465e2091f9fb70dbd5dec0cf58b1b60b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/home/jovyan/models/roberta/roberta_l\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/pytorch_model.bin from cache at /home/bart/.cache/huggingface/transformers/57935eccbb92bb4e5465411da844aeb075cac5b8c255c770a81463fa909d8ccc.f03af7b16e88082afbdd696d0647fe6bf624c0776d71dcece2b0e9973d081b26\n",
      "Some weights of the model checkpoint at sberbank-ai/ruRoberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sberbank-ai/ruRoberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"sberbank-ai/ruRoberta-large\", num_labels=num_labels)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "264ee2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1cff85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bart/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=5e-7, correct_bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3874c90e",
   "metadata": {},
   "source": [
    "Commonsense: 0.009 Lexis: 0.085 Morphology: 0.055 Hallucination: 0.034 Syntax: 0.192 Semantics: 0.074\n",
    "\n",
    "morphology hallucination lexis semantics syntax commonsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7aabeac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                                                                                                                                                                                          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.34279328671191\n",
      "Eval loss: 0.3712323009967804\n",
      "Best Threshold:  0.2\n",
      "Test F1 Accuracy:  0.6265060240963854\n",
      "Test Flat Accuracy:  0.8560371517027864 \n",
      "\n",
      "F1 Validation Accuracy:  51.32275132275132\n",
      "Flat Validation Accuracy:  85.75851393188854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|████████████████████▉                                                                                                                                                                                            | 1/10 [05:48<52:19, 348.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.32449622822994934\n",
      "Eval loss: 0.3502197265625\n",
      "Best Threshold:  0.3\n",
      "Test F1 Accuracy:  0.6363636363636365\n",
      "Test Flat Accuracy:  0.8637770897832817 \n",
      "\n",
      "F1 Validation Accuracy:  57.701711491442545\n",
      "Flat Validation Accuracy:  86.60990712074303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|█████████████████████████████████████████▊                                                                                                                                                                       | 2/10 [11:09<44:18, 332.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3099139377824451\n",
      "Eval loss: 0.34784457087516785\n",
      "Best Threshold:  0.27\n",
      "Test F1 Accuracy:  0.6486486486486487\n",
      "Test Flat Accuracy:  0.8691950464396285 \n",
      "\n",
      "F1 Validation Accuracy:  57.50636132315522\n",
      "Flat Validation Accuracy:  87.07430340557275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|██████████████████████████████████████████████████████████████▋                                                                                                                                                  | 3/10 [16:48<39:07, 335.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.29684649094060583\n",
      "Eval loss: 0.3542931377887726\n",
      "Best Threshold:  0.21000000000000002\n",
      "Test F1 Accuracy:  0.6516393442622951\n",
      "Test Flat Accuracy:  0.868421052631579 \n",
      "\n",
      "F1 Validation Accuracy:  55.58441558441558\n",
      "Flat Validation Accuracy:  86.76470588235294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|███████████████████████████████████████████████████████████████████████████████████▌                                                                                                                             | 4/10 [23:18<34:57, 349.63s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Update parameters and take a step using the computed gradient\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# scheduler.step()\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Update tracking variables\u001b[39;00m\n\u001b[1;32m     51\u001b[0m tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/optimization.py:361\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    359\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta1))\n\u001b[1;32m    360\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m--> 361\u001b[0m denom \u001b[38;5;241m=\u001b[39m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m step_size \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m]:  \u001b[38;5;66;03m# No bias correction for Bert\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "# num_labels = 2\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 10\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for epoch_num in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "    # Training\n",
    "\n",
    "    # Set our model to training mode (as opposed to evaluation mode)\n",
    "    model.train()\n",
    "\n",
    "    # Tracking variables\n",
    "    tr_loss = 0 #running loss\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "  # Train the data for one epoch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # # Forward pass for multiclass classification\n",
    "        # outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        # loss = outputs[0]\n",
    "        # logits = outputs[1]\n",
    "\n",
    "        # Forward pass for multilabel classification\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "        \n",
    "        loss_func = BCEWithLogitsLoss() \n",
    "        loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "        # loss_func = BCELoss() \n",
    "        # loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "        train_loss_set.append(loss.item())    \n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "    # Validation\n",
    "\n",
    "    # Put model in evaluation mode to evaluate loss on the validation set\n",
    "    model.eval()\n",
    "\n",
    "    # Variables to gather full output\n",
    "    logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n",
    "\n",
    "  # Predict\n",
    "    nb_eval_steps = 0\n",
    "    eval_loss = 0.0\n",
    "    for i, batch in enumerate(validation_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "      # Forward pass\n",
    "            outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            b_logit_pred = outs[0]\n",
    "            \n",
    "            loss_func = BCEWithLogitsLoss() \n",
    "            loss = loss_func(b_logit_pred.view(-1, num_labels), b_labels.type_as(b_logit_pred).view(-1, num_labels)) #convert labels to float for calculation\n",
    "        \n",
    "\n",
    "            pred_label = torch.sigmoid(b_logit_pred)\n",
    "\n",
    "            b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "            pred_label = pred_label.to('cpu').numpy()\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        nb_eval_steps += 1\n",
    "        eval_loss += loss\n",
    "        tokenized_texts.append(b_input_ids)\n",
    "        logit_preds.append(b_logit_pred)\n",
    "        true_labels.append(b_labels)\n",
    "        pred_labels.append(pred_label)\n",
    "    print(\"Eval loss: {}\".format(eval_loss/nb_eval_steps))\n",
    "        \n",
    "#         print(\"TRUE LABELS\", true_labels)\n",
    "#         print(\"PRED LABELS\", pred_labels)\n",
    "\n",
    "  # Flatten outputs\n",
    "    pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    threshold = 0.50\n",
    "    pred_bools = [pl>threshold for pl in pred_labels]\n",
    "    true_bools = [tl==1 for tl in true_labels]\n",
    "    \n",
    "    macro_thresholds = np.array(range(1,10))/10\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in macro_thresholds:\n",
    "        pred_bools_minmax = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools_minmax)\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools_minmax)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_macro_th = macro_thresholds[np.argmax(f1_results)] #best macro threshold value\n",
    "\n",
    "    micro_thresholds = (np.array(range(10))/100)+best_macro_th #calculating micro threshold values\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in micro_thresholds:\n",
    "        pred_bools_minmax = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools_minmax)\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools_minmax)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_f1_idx = np.argmax(f1_results) #best threshold value\n",
    "\n",
    "    # Printing and saving classification report\n",
    "    print('Best Threshold: ', micro_thresholds[best_f1_idx])\n",
    "    print('Test F1 Accuracy: ', f1_results[best_f1_idx])\n",
    "    print('Test Flat Accuracy: ', flat_acc_results[best_f1_idx], '\\n')\n",
    "\n",
    "    best_pred_bools = [pl>micro_thresholds[best_f1_idx] for pl in pred_labels]\n",
    "\n",
    "    \n",
    "    val_f1_accuracy = f1_score(true_bools,pred_bools)*100\n",
    "    val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
    "\n",
    "    print('F1 Validation Accuracy: ', val_f1_accuracy)\n",
    "    print('Flat Validation Accuracy: ', val_flat_accuracy)\n",
    "    torch.save(model.state_dict(), f'roberta_syntax_{epoch_num}_epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b12075",
   "metadata": {},
   "source": [
    "# General RuRoberta classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6090bca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 00:38:53.390469: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-08 00:38:53.390507: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\n",
    "import pickle\n",
    "from transformers import *\n",
    "from tqdm import tqdm, trange\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "159af0cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>authored_by</th>\n",
       "      <th>error_type</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>В 1929 году Ньюкомб переехал на Мальту в качес...</td>\n",
       "      <td>1</td>\n",
       "      <td>Machine</td>\n",
       "      <td>0</td>\n",
       "      <td>WikiMatrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Вдруг решетка беззвучно поехала в сторону, и н...</td>\n",
       "      <td>1</td>\n",
       "      <td>Expert</td>\n",
       "      <td>0</td>\n",
       "      <td>Paducheva2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Этим летом не никуда ездили.</td>\n",
       "      <td>0</td>\n",
       "      <td>Expert</td>\n",
       "      <td>Syntax</td>\n",
       "      <td>Rusgram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>В 2011 году был выпущен документальный фильм «...</td>\n",
       "      <td>1</td>\n",
       "      <td>Machine</td>\n",
       "      <td>0</td>\n",
       "      <td>WikiMatrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Поговорительница Пресли поссорилась с поп-звез...</td>\n",
       "      <td>0</td>\n",
       "      <td>Machine</td>\n",
       "      <td>Morphology</td>\n",
       "      <td>WikiMatrix</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  is_correct  \\\n",
       "0           0  В 1929 году Ньюкомб переехал на Мальту в качес...           1   \n",
       "1           1  Вдруг решетка беззвучно поехала в сторону, и н...           1   \n",
       "2           2                       Этим летом не никуда ездили.           0   \n",
       "3           3  В 2011 году был выпущен документальный фильм «...           1   \n",
       "4           4  Поговорительница Пресли поссорилась с поп-звез...           0   \n",
       "\n",
       "  authored_by  error_type         source  \n",
       "0     Machine           0     WikiMatrix  \n",
       "1      Expert           0  Paducheva2004  \n",
       "2      Expert      Syntax        Rusgram  \n",
       "3     Machine           0     WikiMatrix  \n",
       "4     Machine  Morphology     WikiMatrix  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('./rucola/train.csv')\n",
    "#  './rucola/test.csv'\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11824b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79272129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label columns:  ['is_correct']\n"
     ]
    }
   ],
   "source": [
    "cols = train_df.columns\n",
    "label_cols = list(cols[2:3])\n",
    "num_labels = len(label_cols)\n",
    "print('Label columns: ', label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84d048d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(train_df.is_correct.values)\n",
    "texts = list(train_df.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5f5e8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/vocab.json from cache at /home/bart/.cache/huggingface/transformers/01c769031f99defa27c759f85c0f21715d1506ed5c4ab01f7d7ef8c8b3b5674a.10df436f410c7709f54ad2bf7e973429f03c613012be0df709996d5aef3706af\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/merges.txt from cache at /home/bart/.cache/huggingface/transformers/6d742b9584f250f0088e6a6674f80ae706d8a390d6bc30f7cfaccb4292556f55.e4807b2d97082a2dfbf6623d7aa973a5057f87e44b329ec0b63daafd5d2ef593\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/config.json from cache at /home/bart/.cache/huggingface/transformers/191cf2edd2e8d2007f9a39479b45315ac1b79fe23fa4786511de1c8f9b9def46.ebd71ffa9cb74c7d1d39cf67343e2ae8465e2091f9fb70dbd5dec0cf58b1b60b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"sberbank-ai/ruRoberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer outputs:  dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoTokenizer, RobertaTokenizer\n",
    "\n",
    "\n",
    "# max_length = 100\n",
    "tokenizer = RobertaTokenizer.from_pretrained('sberbank-ai/ruRoberta-large', do_lower_case=False) # tokenizer\n",
    "encodings = tokenizer.batch_encode_plus(texts, padding='longest') # tokenizer's encoding method\n",
    "print('tokenizer outputs: ', encodings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07c44f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encodings['input_ids'] # tokenized and encoded sentences\n",
    "attention_masks = encodings['attention_mask'] # attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "588126cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df label indices with only one instance:  []\n"
     ]
    }
   ],
   "source": [
    "# Identifying indices of 'one_hot_labels' entries that only occur once - this will allow us to stratify split our training data later\n",
    "label_counts = train_df.is_correct.astype(str).value_counts()\n",
    "one_freq = label_counts[label_counts==1].keys()\n",
    "one_freq_idxs = sorted(list(train_df[train_df.is_correct.astype(str).isin(one_freq)].index), reverse=True)\n",
    "print('df label indices with only one instance: ', one_freq_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98e40138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering single instance inputs to force into the training set after stratified split\n",
    "one_freq_input_ids = [input_ids.pop(i) for i in one_freq_idxs]\n",
    "one_freq_attention_masks = [attention_masks.pop(i) for i in one_freq_idxs]\n",
    "one_freq_labels = [labels.pop(i) for i in one_freq_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1652875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels, attention_masks,\n",
    "        random_state=2020, test_size=0.10, stratify = labels)\n",
    "\n",
    "# Add one frequency data to train data\n",
    "train_inputs.extend(one_freq_input_ids)\n",
    "train_labels.extend(one_freq_labels)\n",
    "train_masks.extend(one_freq_attention_masks)\n",
    "\n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "370aa15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning with XLNet, the authors recommend a batch size of 32, 48, or 128. We will use 32 here to avoid memory issues.\n",
    "batch_size = 14\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f40cc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(validation_dataloader, 'validation_data_loader')\n",
    "torch.save(train_dataloader, 'train_data_loader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44d694a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/config.json from cache at /home/bart/.cache/huggingface/transformers/191cf2edd2e8d2007f9a39479b45315ac1b79fe23fa4786511de1c8f9b9def46.ebd71ffa9cb74c7d1d39cf67343e2ae8465e2091f9fb70dbd5dec0cf58b1b60b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/home/jovyan/models/roberta/roberta_l\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/pytorch_model.bin from cache at /home/bart/.cache/huggingface/transformers/57935eccbb92bb4e5465411da844aeb075cac5b8c255c770a81463fa909d8ccc.f03af7b16e88082afbdd696d0647fe6bf624c0776d71dcece2b0e9973d081b26\n",
      "Some weights of the model checkpoint at sberbank-ai/ruRoberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sberbank-ai/ruRoberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"sberbank-ai/ruRoberta-large\", num_labels=num_labels)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdf3ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d4d586f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bart/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=5e-7, correct_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb4104ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                                                                                                                                                                                          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6154052181579576\n",
      "Eval loss: 0.5653561353683472\n",
      "Best Threshold:  0.4\n",
      "Test F1 Accuracy:  0.7929373996789726\n",
      "Test Flat Accuracy:  0.7004643962848297 \n",
      "\n",
      "F1 Validation Accuracy:  78.80562060889929\n",
      "Flat Validation Accuracy:  71.9814241486068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|████████████████████▉                                                                                                                                                                                            | 1/10 [05:21<48:11, 321.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5473098854044571\n",
      "Eval loss: 0.5757425427436829\n",
      "Best Threshold:  0.51\n",
      "Test F1 Accuracy:  0.81\n",
      "Test Flat Accuracy:  0.7352941176470589 \n",
      "\n",
      "F1 Validation Accuracy:  80.94975151849806\n",
      "Flat Validation Accuracy:  73.29721362229103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|█████████████████████████████████████████▊                                                                                                                                                                       | 2/10 [10:36<42:20, 317.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5087471951861674\n",
      "Eval loss: 0.5278174877166748\n",
      "Best Threshold:  0.44\n",
      "Test F1 Accuracy:  0.8166948674562888\n",
      "Test Flat Accuracy:  0.7484520123839009 \n",
      "\n",
      "F1 Validation Accuracy:  81.36970400464307\n",
      "Flat Validation Accuracy:  75.15479876160991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|██████████████████████████████████████████████████████████████▋                                                                                                                                                  | 3/10 [16:01<37:27, 321.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4840308181430459\n",
      "Eval loss: 0.6047687530517578\n",
      "Best Threshold:  0.64\n",
      "Test F1 Accuracy:  0.8187001140250856\n",
      "Test Flat Accuracy:  0.7538699690402477 \n",
      "\n",
      "F1 Validation Accuracy:  81.19241192411923\n",
      "Flat Validation Accuracy:  73.14241486068111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|███████████████████████████████████████████████████████████████████████████████████▌                                                                                                                             | 4/10 [21:45<33:00, 330.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4631256307648981\n",
      "Eval loss: 0.5864363312721252\n",
      "Best Threshold:  0.61\n",
      "Test F1 Accuracy:  0.8241320432555493\n",
      "Test Flat Accuracy:  0.7608359133126935 \n",
      "\n",
      "F1 Validation Accuracy:  81.68398031711318\n",
      "Flat Validation Accuracy:  74.07120743034056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                        | 5/10 [27:21<27:40, 332.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4517196253318649\n",
      "Eval loss: 0.5300514101982117\n",
      "Best Threshold:  0.62\n",
      "Test F1 Accuracy:  0.8293556085918854\n",
      "Test Flat Accuracy:  0.7786377708978328 \n",
      "\n",
      "F1 Validation Accuracy:  82.61363636363637\n",
      "Flat Validation Accuracy:  76.31578947368422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                   | 6/10 [32:38<21:48, 327.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4281902333046842\n",
      "Eval loss: 0.5694604516029358\n",
      "Best Threshold:  0.7\n",
      "Test F1 Accuracy:  0.830842797369994\n",
      "Test Flat Accuracy:  0.7809597523219814 \n",
      "\n",
      "F1 Validation Accuracy:  82.55555555555554\n",
      "Flat Validation Accuracy:  75.69659442724458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                              | 7/10 [37:54<16:10, 323.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4111516928091807\n",
      "Eval loss: 0.5562500953674316\n",
      "Best Threshold:  0.64\n",
      "Test F1 Accuracy:  0.8345153664302601\n",
      "Test Flat Accuracy:  0.7832817337461301 \n",
      "\n",
      "F1 Validation Accuracy:  82.68471517202482\n",
      "Flat Validation Accuracy:  76.23839009287926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                         | 8/10 [43:09<10:41, 320.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3972213422491806\n",
      "Eval loss: 0.5258675217628479\n",
      "Best Threshold:  0.65\n",
      "Test F1 Accuracy:  0.8361650485436893\n",
      "Test Flat Accuracy:  0.7910216718266254 \n",
      "\n",
      "F1 Validation Accuracy:  83.38158656629993\n",
      "Flat Validation Accuracy:  77.78637770897832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                     | 9/10 [48:25<05:19, 319.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.38536903436971487\n",
      "Eval loss: 0.5635668039321899\n",
      "Best Threshold:  0.7\n",
      "Test F1 Accuracy:  0.835138387484958\n",
      "Test Flat Accuracy:  0.7879256965944272 \n",
      "\n",
      "F1 Validation Accuracy:  83.29545454545453\n",
      "Flat Validation Accuracy:  77.24458204334366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [53:46<00:00, 322.63s/it]\n"
     ]
    }
   ],
   "source": [
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "# num_labels = 2\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 10\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for epoch_num in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "    # Training\n",
    "\n",
    "    # Set our model to training mode (as opposed to evaluation mode)\n",
    "    model.train()\n",
    "\n",
    "    # Tracking variables\n",
    "    tr_loss = 0 #running loss\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "  # Train the data for one epoch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # # Forward pass for multiclass classification\n",
    "        # outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        # loss = outputs[0]\n",
    "        # logits = outputs[1]\n",
    "\n",
    "        # Forward pass for multilabel classification\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "        \n",
    "        loss_func = BCEWithLogitsLoss() \n",
    "        loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "        # loss_func = BCELoss() \n",
    "        # loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "        train_loss_set.append(loss.item())    \n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "    # Validation\n",
    "\n",
    "    # Put model in evaluation mode to evaluate loss on the validation set\n",
    "    model.eval()\n",
    "\n",
    "    # Variables to gather full output\n",
    "    logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n",
    "\n",
    "  # Predict\n",
    "    nb_eval_steps = 0\n",
    "    eval_loss = 0.0\n",
    "    for i, batch in enumerate(validation_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "      # Forward pass\n",
    "            outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            b_logit_pred = outs[0]\n",
    "            \n",
    "            loss_func = BCEWithLogitsLoss() \n",
    "            loss = loss_func(b_logit_pred.view(-1, num_labels), b_labels.type_as(b_logit_pred).view(-1, num_labels)) #convert labels to float for calculation\n",
    "        \n",
    "\n",
    "            pred_label = torch.sigmoid(b_logit_pred)\n",
    "\n",
    "            b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "            pred_label = pred_label.to('cpu').numpy()\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        nb_eval_steps += 1\n",
    "        eval_loss += loss\n",
    "        tokenized_texts.append(b_input_ids)\n",
    "        logit_preds.append(b_logit_pred)\n",
    "        true_labels.append(b_labels)\n",
    "        pred_labels.append(pred_label)\n",
    "    print(\"Eval loss: {}\".format(eval_loss/nb_eval_steps))\n",
    "        \n",
    "#         print(\"TRUE LABELS\", true_labels)\n",
    "#         print(\"PRED LABELS\", pred_labels)\n",
    "\n",
    "  # Flatten outputs\n",
    "    pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    threshold = 0.50\n",
    "    pred_bools = [pl>threshold for pl in pred_labels]\n",
    "    true_bools = [tl==1 for tl in true_labels]\n",
    "    \n",
    "    macro_thresholds = np.array(range(1,10))/10\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in macro_thresholds:\n",
    "        pred_bools_minmax = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools_minmax)\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools_minmax)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_macro_th = macro_thresholds[np.argmax(f1_results)] #best macro threshold value\n",
    "\n",
    "    micro_thresholds = (np.array(range(10))/100)+best_macro_th #calculating micro threshold values\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in micro_thresholds:\n",
    "        pred_bools_minmax = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools_minmax)\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools_minmax)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_f1_idx = np.argmax(f1_results) #best threshold value\n",
    "\n",
    "    # Printing and saving classification report\n",
    "    print('Best Threshold: ', micro_thresholds[best_f1_idx])\n",
    "    print('Test F1 Accuracy: ', f1_results[best_f1_idx])\n",
    "    print('Test Flat Accuracy: ', flat_acc_results[best_f1_idx], '\\n')\n",
    "\n",
    "    best_pred_bools = [pl>micro_thresholds[best_f1_idx] for pl in pred_labels]\n",
    "\n",
    "    \n",
    "    val_f1_accuracy = f1_score(true_bools,pred_bools)*100\n",
    "    val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
    "\n",
    "    print('F1 Validation Accuracy: ', val_f1_accuracy)\n",
    "    print('Flat Validation Accuracy: ', val_flat_accuracy)\n",
    "    torch.save(model.state_dict(), f'roberta_general_{epoch_num}_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4562c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load('roberta_general_8_epochs')\n",
    "from transformers import RobertaForSequenceClassification, RobertaConfig\n",
    "\n",
    "# model = RobertaForSequenceClassification.from_pretrained(\"./roberta_general_8_epochs\", num_labels=2, ignore_mismatched_sizes=True)\n",
    "state_dict = torch.load(\"./roberta_general_8_epochs\")\n",
    "\n",
    "#model = RobertaForSequenceClassification(RobertaConfig(num_labels=2, ignore_mismatched_sizes=True))\n",
    "#model.load_state_dict(torch.load(\"./roberta_general_8_epochs\"))\n",
    "\n",
    "# model = RobertaForSequenceClassification.from_pretrained(\n",
    "#     \"./roberta_general_8_epochs\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "#     num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "#                     # You can increase this for multi-class tasks.   \n",
    "#     output_attentions = False, # Whether the model returns attentions weights.\n",
    "#     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e71018f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/config.json from cache at /home/bart/.cache/huggingface/transformers/191cf2edd2e8d2007f9a39479b45315ac1b79fe23fa4786511de1c8f9b9def46.ebd71ffa9cb74c7d1d39cf67343e2ae8465e2091f9fb70dbd5dec0cf58b1b60b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/home/jovyan/models/roberta/roberta_l\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/pytorch_model.bin from cache at /home/bart/.cache/huggingface/transformers/57935eccbb92bb4e5465411da844aeb075cac5b8c255c770a81463fa909d8ccc.f03af7b16e88082afbdd696d0647fe6bf624c0776d71dcece2b0e9973d081b26\n",
      "Some weights of the model checkpoint at sberbank-ai/ruRoberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sberbank-ai/ruRoberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaConfig\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"sberbank-ai/ruRoberta-large\", state_dict=state_dict, num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46d93134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bae69a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"RuCoLA-main/data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1091590f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Организация партии в начале парламентских выбо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Чувствуется большая симпатия автора к Андрею Б...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Основатель Amazon Джефф Безос посмертно награж...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Указать название требуемого сертификационного ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Они живут в очень тяжелых материальных условия...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           sentence\n",
       "0   0  Организация партии в начале парламентских выбо...\n",
       "1   1  Чувствуется большая симпатия автора к Андрею Б...\n",
       "2   2  Основатель Amazon Джефф Безос посмертно награж...\n",
       "3   3  Указать название требуемого сертификационного ...\n",
       "4   4  Они живут в очень тяжелых материальных условия..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "215fbb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "texts = list(test_df.sentence.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "995c459f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/vocab.json from cache at /home/bart/.cache/huggingface/transformers/01c769031f99defa27c759f85c0f21715d1506ed5c4ab01f7d7ef8c8b3b5674a.10df436f410c7709f54ad2bf7e973429f03c613012be0df709996d5aef3706af\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/merges.txt from cache at /home/bart/.cache/huggingface/transformers/6d742b9584f250f0088e6a6674f80ae706d8a390d6bc30f7cfaccb4292556f55.e4807b2d97082a2dfbf6623d7aa973a5057f87e44b329ec0b63daafd5d2ef593\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/config.json from cache at /home/bart/.cache/huggingface/transformers/191cf2edd2e8d2007f9a39479b45315ac1b79fe23fa4786511de1c8f9b9def46.ebd71ffa9cb74c7d1d39cf67343e2ae8465e2091f9fb70dbd5dec0cf58b1b60b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"sberbank-ai/ruRoberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer outputs:  dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoTokenizer, RobertaTokenizer\n",
    "\n",
    "\n",
    "# max_length = 100\n",
    "tokenizer = RobertaTokenizer.from_pretrained('sberbank-ai/ruRoberta-large', do_lower_case=False) # tokenizer\n",
    "encodings = tokenizer.batch_encode_plus(texts, padding='longest') # tokenizer's encoding method\n",
    "print('tokenizer outputs: ', encodings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ce3d674",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encodings['input_ids'] # tokenized and encoded sentences\n",
    "attention_masks = encodings['attention_mask'] # attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d3ec673",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_inputs = torch.tensor(input_ids)\n",
    "validation_masks = torch.tensor(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59e07a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = TensorDataset(validation_inputs, validation_masks)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "138aa018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c14662b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 1    \n",
    "pred_labels = []\n",
    "for i, batch in enumerate(validation_dataloader):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask = batch\n",
    "    with torch.no_grad():\n",
    "  # Forward pass\n",
    "        outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        b_logit_pred = outs[0]\n",
    "\n",
    "        pred_label = torch.sigmoid(b_logit_pred)\n",
    "\n",
    "        b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "        pred_label = pred_label.to(device).cpu().numpy()\n",
    "\n",
    "    pred_labels.append(pred_label)\n",
    "\n",
    "#         print(\"TRUE LABELS\", true_labels)\n",
    "#         print(\"PRED LABELS\", pred_labels)\n",
    "\n",
    "# Flatten outputs\n",
    "pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "\n",
    "# Calculate Accuracy\n",
    "threshold = 0.65\n",
    "pred_bools = [pl>threshold for pl in pred_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "32df5dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.assign(acceptable=pd.Series(np.zeros(len(test_df), dtype=int)).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a304985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, label in enumerate(pred_bools):\n",
    "    test_df.at[i, 'acceptable'] = 1 if label else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c46470eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>acceptable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Организация партии в начале парламентских выбо...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Чувствуется большая симпатия автора к Андрею Б...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Основатель Amazon Джефф Безос посмертно награж...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Указать название требуемого сертификационного ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Они живут в очень тяжелых материальных условия...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           sentence  acceptable\n",
       "0   0  Организация партии в начале парламентских выбо...           0\n",
       "1   1  Чувствуется большая симпатия автора к Андрею Б...           0\n",
       "2   2  Основатель Amazon Джефф Безос посмертно награж...           1\n",
       "3   3  Указать название требуемого сертификационного ...           1\n",
       "4   4  Они живут в очень тяжелых материальных условия...           1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "913de1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.loc[:, [\"id\", \"acceptable\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc08eb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>acceptable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  acceptable\n",
       "0   0           0\n",
       "1   1           0\n",
       "2   2           1\n",
       "3   3           1\n",
       "4   4           1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "478f6bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"first_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed153070",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralClassifierRoberta:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.tokenizer()\n",
    "    \n",
    "    def predict(self, text):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb57fa25",
   "metadata": {},
   "source": [
    "# Machine text ruroberta classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04cd4a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 04:49:52.326823: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-08 04:49:52.326843: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\n",
    "import pickle\n",
    "from transformers import *\n",
    "from tqdm import tqdm, trange\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6466caf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>authored_by</th>\n",
       "      <th>error_type</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>В 1929 году Ньюкомб переехал на Мальту в качес...</td>\n",
       "      <td>1</td>\n",
       "      <td>Machine</td>\n",
       "      <td>0</td>\n",
       "      <td>WikiMatrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Вдруг решетка беззвучно поехала в сторону, и н...</td>\n",
       "      <td>1</td>\n",
       "      <td>Expert</td>\n",
       "      <td>0</td>\n",
       "      <td>Paducheva2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Этим летом не никуда ездили.</td>\n",
       "      <td>0</td>\n",
       "      <td>Expert</td>\n",
       "      <td>Syntax</td>\n",
       "      <td>Rusgram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>В 2011 году был выпущен документальный фильм «...</td>\n",
       "      <td>1</td>\n",
       "      <td>Machine</td>\n",
       "      <td>0</td>\n",
       "      <td>WikiMatrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Поговорительница Пресли поссорилась с поп-звез...</td>\n",
       "      <td>0</td>\n",
       "      <td>Machine</td>\n",
       "      <td>Morphology</td>\n",
       "      <td>WikiMatrix</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  is_correct  \\\n",
       "0           0  В 1929 году Ньюкомб переехал на Мальту в качес...           1   \n",
       "1           1  Вдруг решетка беззвучно поехала в сторону, и н...           1   \n",
       "2           2                       Этим летом не никуда ездили.           0   \n",
       "3           3  В 2011 году был выпущен документальный фильм «...           1   \n",
       "4           4  Поговорительница Пресли поссорилась с поп-звез...           0   \n",
       "\n",
       "  authored_by  error_type         source  \n",
       "0     Machine           0     WikiMatrix  \n",
       "1      Expert           0  Paducheva2004  \n",
       "2      Expert      Syntax        Rusgram  \n",
       "3     Machine           0     WikiMatrix  \n",
       "4     Machine  Morphology     WikiMatrix  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('./rucola/train.csv')\n",
    "#  './rucola/test.csv'\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4e6f4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77c108a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.assign(machine=pd.Series(np.zeros(len(train_df), dtype=int)).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "613b0b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in train_df.iterrows():\n",
    "    if row[\"authored_by\"] == \"Machine\":\n",
    "        train_df.at[i, \"machine\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5a2597f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>authored_by</th>\n",
       "      <th>error_type</th>\n",
       "      <th>source</th>\n",
       "      <th>machine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>В 1929 году Ньюкомб переехал на Мальту в качес...</td>\n",
       "      <td>1</td>\n",
       "      <td>Machine</td>\n",
       "      <td>0</td>\n",
       "      <td>WikiMatrix</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Вдруг решетка беззвучно поехала в сторону, и н...</td>\n",
       "      <td>1</td>\n",
       "      <td>Expert</td>\n",
       "      <td>0</td>\n",
       "      <td>Paducheva2004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Этим летом не никуда ездили.</td>\n",
       "      <td>0</td>\n",
       "      <td>Expert</td>\n",
       "      <td>Syntax</td>\n",
       "      <td>Rusgram</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>В 2011 году был выпущен документальный фильм «...</td>\n",
       "      <td>1</td>\n",
       "      <td>Machine</td>\n",
       "      <td>0</td>\n",
       "      <td>WikiMatrix</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Поговорительница Пресли поссорилась с поп-звез...</td>\n",
       "      <td>0</td>\n",
       "      <td>Machine</td>\n",
       "      <td>Morphology</td>\n",
       "      <td>WikiMatrix</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  is_correct  \\\n",
       "0           0  В 1929 году Ньюкомб переехал на Мальту в качес...           1   \n",
       "1           1  Вдруг решетка беззвучно поехала в сторону, и н...           1   \n",
       "2           2                       Этим летом не никуда ездили.           0   \n",
       "3           3  В 2011 году был выпущен документальный фильм «...           1   \n",
       "4           4  Поговорительница Пресли поссорилась с поп-звез...           0   \n",
       "\n",
       "  authored_by  error_type         source  machine  \n",
       "0     Machine           0     WikiMatrix        1  \n",
       "1      Expert           0  Paducheva2004        0  \n",
       "2      Expert      Syntax        Rusgram        0  \n",
       "3     Machine           0     WikiMatrix        1  \n",
       "4     Machine  Morphology     WikiMatrix        1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1ce78a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label columns:  ['machine']\n"
     ]
    }
   ],
   "source": [
    "cols = train_df.columns\n",
    "label_cols = list(cols[6:7])\n",
    "num_labels = len(label_cols)\n",
    "print('Label columns: ', label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91d0a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(train_df.machine.values)\n",
    "texts = list(train_df.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "130fe960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/vocab.json from cache at /home/bart/.cache/huggingface/transformers/01c769031f99defa27c759f85c0f21715d1506ed5c4ab01f7d7ef8c8b3b5674a.10df436f410c7709f54ad2bf7e973429f03c613012be0df709996d5aef3706af\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/merges.txt from cache at /home/bart/.cache/huggingface/transformers/6d742b9584f250f0088e6a6674f80ae706d8a390d6bc30f7cfaccb4292556f55.e4807b2d97082a2dfbf6623d7aa973a5057f87e44b329ec0b63daafd5d2ef593\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/config.json from cache at /home/bart/.cache/huggingface/transformers/191cf2edd2e8d2007f9a39479b45315ac1b79fe23fa4786511de1c8f9b9def46.ebd71ffa9cb74c7d1d39cf67343e2ae8465e2091f9fb70dbd5dec0cf58b1b60b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"sberbank-ai/ruRoberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer outputs:  dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoTokenizer, RobertaTokenizer\n",
    "\n",
    "\n",
    "# max_length = 100\n",
    "tokenizer = RobertaTokenizer.from_pretrained('sberbank-ai/ruRoberta-large', do_lower_case=False) # tokenizer\n",
    "encodings = tokenizer.batch_encode_plus(texts, padding='longest') # tokenizer's encoding method\n",
    "print('tokenizer outputs: ', encodings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46e246a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encodings['input_ids'] # tokenized and encoded sentences\n",
    "attention_masks = encodings['attention_mask'] # attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2813c03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df label indices with only one instance:  []\n"
     ]
    }
   ],
   "source": [
    "# Identifying indices of 'one_hot_labels' entries that only occur once - this will allow us to stratify split our training data later\n",
    "label_counts = train_df.machine.astype(str).value_counts()\n",
    "one_freq = label_counts[label_counts==1].keys()\n",
    "one_freq_idxs = sorted(list(train_df[train_df.machine.astype(str).isin(one_freq)].index), reverse=True)\n",
    "print('df label indices with only one instance: ', one_freq_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93f7e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering single instance inputs to force into the training set after stratified split\n",
    "one_freq_input_ids = [input_ids.pop(i) for i in one_freq_idxs]\n",
    "one_freq_attention_masks = [attention_masks.pop(i) for i in one_freq_idxs]\n",
    "one_freq_labels = [labels.pop(i) for i in one_freq_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dd97be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels, attention_masks,\n",
    "        random_state=2020, test_size=0.10, stratify = labels)\n",
    "\n",
    "# Add one frequency data to train data\n",
    "train_inputs.extend(one_freq_input_ids)\n",
    "train_labels.extend(one_freq_labels)\n",
    "train_masks.extend(one_freq_attention_masks)\n",
    "\n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65590c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning with XLNet, the authors recommend a batch size of 32, 48, or 128. We will use 32 here to avoid memory issues.\n",
    "batch_size = 15\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fb8f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(validation_dataloader, 'validation_data_loader')\n",
    "torch.save(train_dataloader, 'train_data_loader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e03d85ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/config.json from cache at /home/bart/.cache/huggingface/transformers/191cf2edd2e8d2007f9a39479b45315ac1b79fe23fa4786511de1c8f9b9def46.ebd71ffa9cb74c7d1d39cf67343e2ae8465e2091f9fb70dbd5dec0cf58b1b60b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/home/jovyan/models/roberta/roberta_l\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/pytorch_model.bin from cache at /home/bart/.cache/huggingface/transformers/57935eccbb92bb4e5465411da844aeb075cac5b8c255c770a81463fa909d8ccc.f03af7b16e88082afbdd696d0647fe6bf624c0776d71dcece2b0e9973d081b26\n",
      "Some weights of the model checkpoint at sberbank-ai/ruRoberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sberbank-ai/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"sberbank-ai/ruRoberta-large\", num_labels=num_labels)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "558a1d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8606c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bart/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=5e-7, correct_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7acc25d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                                                                                                                                                                                          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.39499194179811786\n",
      "Eval loss: 0.2807169556617737\n",
      "Best Threshold:  0.5\n",
      "Test F1 Accuracy:  0.7876923076923077\n",
      "Test Flat Accuracy:  0.8931888544891641 \n",
      "\n",
      "F1 Validation Accuracy:  78.76923076923077\n",
      "Flat Validation Accuracy:  89.3188854489164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|████████████████████▉                                                                                                                                                                                            | 1/10 [05:13<47:02, 313.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.27040439009666445\n",
      "Eval loss: 0.23714572191238403\n",
      "Best Threshold:  0.42000000000000004\n",
      "Test F1 Accuracy:  0.8224852071005917\n",
      "Test Flat Accuracy:  0.9071207430340558 \n",
      "\n",
      "F1 Validation Accuracy:  81.36645962732919\n",
      "Flat Validation Accuracy:  90.71207430340557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|█████████████████████████████████████████▊                                                                                                                                                                       | 2/10 [10:26<41:46, 313.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.23190960294537005\n",
      "Eval loss: 0.2224176824092865\n",
      "Best Threshold:  0.51\n",
      "Test F1 Accuracy:  0.8353658536585366\n",
      "Test Flat Accuracy:  0.9164086687306502 \n",
      "\n",
      "F1 Validation Accuracy:  83.40943683409438\n",
      "Flat Validation Accuracy:  91.56346749226006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|██████████████████████████████████████████████████████████████▋                                                                                                                                                  | 3/10 [15:40<36:33, 313.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.20469732190572446\n",
      "Eval loss: 0.21466340124607086\n",
      "Best Threshold:  0.51\n",
      "Test F1 Accuracy:  0.843558282208589\n",
      "Test Flat Accuracy:  0.9210526315789473 \n",
      "\n",
      "F1 Validation Accuracy:  84.22664624808576\n",
      "Flat Validation Accuracy:  92.02786377708978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|███████████████████████████████████████████████████████████████████████████████████▌                                                                                                                             | 4/10 [20:53<31:20, 313.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1827444372205965\n",
      "Eval loss: 0.21876636147499084\n",
      "Best Threshold:  0.43000000000000005\n",
      "Test F1 Accuracy:  0.847457627118644\n",
      "Test Flat Accuracy:  0.923374613003096 \n",
      "\n",
      "F1 Validation Accuracy:  83.94276629570747\n",
      "Flat Validation Accuracy:  92.18266253869969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                        | 5/10 [26:07<26:07, 313.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.16400979872672788\n",
      "Eval loss: 0.21744725108146667\n",
      "Best Threshold:  0.75\n",
      "Test F1 Accuracy:  0.8377952755905512\n",
      "Test Flat Accuracy:  0.9202786377708978 \n",
      "\n",
      "F1 Validation Accuracy:  83.38192419825073\n",
      "Flat Validation Accuracy:  91.17647058823529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                   | 6/10 [31:20<20:53, 313.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1480827987350283\n",
      "Eval loss: 0.21220357716083527\n",
      "Best Threshold:  0.5\n",
      "Test F1 Accuracy:  0.8398791540785498\n",
      "Test Flat Accuracy:  0.9179566563467493 \n",
      "\n",
      "F1 Validation Accuracy:  83.98791540785498\n",
      "Flat Validation Accuracy:  91.79566563467493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                              | 7/10 [36:34<15:40, 313.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.13250965426886274\n",
      "Eval loss: 0.21896778047084808\n",
      "Best Threshold:  0.8\n",
      "Test F1 Accuracy:  0.8465266558966074\n",
      "Test Flat Accuracy:  0.9264705882352942 \n",
      "\n",
      "F1 Validation Accuracy:  83.52941176470588\n",
      "Flat Validation Accuracy:  91.3312693498452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                         | 8/10 [41:52<10:30, 315.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.11876946758719222\n",
      "Eval loss: 0.21579889953136444\n",
      "Best Threshold:  0.61\n",
      "Test F1 Accuracy:  0.8562596599690881\n",
      "Test Flat Accuracy:  0.9280185758513931 \n",
      "\n",
      "F1 Validation Accuracy:  84.84848484848484\n",
      "Flat Validation Accuracy:  92.26006191950464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                     | 9/10 [47:18<05:18, 318.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10982647804843802\n",
      "Eval loss: 0.22453783452510834\n",
      "Best Threshold:  0.74\n",
      "Test F1 Accuracy:  0.849772382397572\n",
      "Test Flat Accuracy:  0.923374613003096 \n",
      "\n",
      "F1 Validation Accuracy:  84.22575976845151\n",
      "Flat Validation Accuracy:  91.56346749226006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [52:32<00:00, 315.22s/it]\n"
     ]
    }
   ],
   "source": [
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "# num_labels = 2\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 10\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for epoch_num in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "    # Training\n",
    "\n",
    "    # Set our model to training mode (as opposed to evaluation mode)\n",
    "    model.train()\n",
    "\n",
    "    # Tracking variables\n",
    "    tr_loss = 0 #running loss\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "  # Train the data for one epoch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # # Forward pass for multiclass classification\n",
    "        # outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        # loss = outputs[0]\n",
    "        # logits = outputs[1]\n",
    "\n",
    "        # Forward pass for multilabel classification\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "        \n",
    "        loss_func = BCEWithLogitsLoss() \n",
    "        loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "        # loss_func = BCELoss() \n",
    "        # loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "        train_loss_set.append(loss.item())    \n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "    # Validation\n",
    "\n",
    "    # Put model in evaluation mode to evaluate loss on the validation set\n",
    "    model.eval()\n",
    "\n",
    "    # Variables to gather full output\n",
    "    logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n",
    "\n",
    "  # Predict\n",
    "    nb_eval_steps = 0\n",
    "    eval_loss = 0.0\n",
    "    for i, batch in enumerate(validation_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "      # Forward pass\n",
    "            outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            b_logit_pred = outs[0]\n",
    "            \n",
    "            loss_func = BCEWithLogitsLoss() \n",
    "            loss = loss_func(b_logit_pred.view(-1, num_labels), b_labels.type_as(b_logit_pred).view(-1, num_labels)) #convert labels to float for calculation\n",
    "        \n",
    "\n",
    "            pred_label = torch.sigmoid(b_logit_pred)\n",
    "\n",
    "            b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "            pred_label = pred_label.to('cpu').numpy()\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        nb_eval_steps += 1\n",
    "        eval_loss += loss\n",
    "        tokenized_texts.append(b_input_ids)\n",
    "        logit_preds.append(b_logit_pred)\n",
    "        true_labels.append(b_labels)\n",
    "        pred_labels.append(pred_label)\n",
    "    print(\"Eval loss: {}\".format(eval_loss/nb_eval_steps))\n",
    "        \n",
    "#         print(\"TRUE LABELS\", true_labels)\n",
    "#         print(\"PRED LABELS\", pred_labels)\n",
    "\n",
    "  # Flatten outputs\n",
    "    pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    threshold = 0.50\n",
    "    pred_bools = [pl>threshold for pl in pred_labels]\n",
    "    true_bools = [tl==1 for tl in true_labels]\n",
    "    \n",
    "    macro_thresholds = np.array(range(1,10))/10\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in macro_thresholds:\n",
    "        pred_bools_minmax = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools_minmax)\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools_minmax)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_macro_th = macro_thresholds[np.argmax(f1_results)] #best macro threshold value\n",
    "\n",
    "    micro_thresholds = (np.array(range(10))/100)+best_macro_th #calculating micro threshold values\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in micro_thresholds:\n",
    "        pred_bools_minmax = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools_minmax)\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools_minmax)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_f1_idx = np.argmax(f1_results) #best threshold value\n",
    "\n",
    "    # Printing and saving classification report\n",
    "    print('Best Threshold: ', micro_thresholds[best_f1_idx])\n",
    "    print('Test F1 Accuracy: ', f1_results[best_f1_idx])\n",
    "    print('Test Flat Accuracy: ', flat_acc_results[best_f1_idx], '\\n')\n",
    "\n",
    "    best_pred_bools = [pl>micro_thresholds[best_f1_idx] for pl in pred_labels]\n",
    "\n",
    "    \n",
    "    val_f1_accuracy = f1_score(true_bools,pred_bools)*100\n",
    "    val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
    "\n",
    "    print('F1 Validation Accuracy: ', val_f1_accuracy)\n",
    "    print('Flat Validation Accuracy: ', val_flat_accuracy)\n",
    "    torch.save(model.state_dict(), f'roberta_machine_{epoch_num}_epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df35195",
   "metadata": {},
   "source": [
    "# Syntax ruRoberta classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b039d0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\n",
    "import pickle\n",
    "from transformers import *\n",
    "from tqdm import tqdm, trange\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "342243ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>authored_by</th>\n",
       "      <th>error_type</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>В 1929 году Ньюкомб переехал на Мальту в качес...</td>\n",
       "      <td>1</td>\n",
       "      <td>Machine</td>\n",
       "      <td>0</td>\n",
       "      <td>WikiMatrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Вдруг решетка беззвучно поехала в сторону, и н...</td>\n",
       "      <td>1</td>\n",
       "      <td>Expert</td>\n",
       "      <td>0</td>\n",
       "      <td>Paducheva2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Этим летом не никуда ездили.</td>\n",
       "      <td>0</td>\n",
       "      <td>Expert</td>\n",
       "      <td>Syntax</td>\n",
       "      <td>Rusgram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>В 2011 году был выпущен документальный фильм «...</td>\n",
       "      <td>1</td>\n",
       "      <td>Machine</td>\n",
       "      <td>0</td>\n",
       "      <td>WikiMatrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Поговорительница Пресли поссорилась с поп-звез...</td>\n",
       "      <td>0</td>\n",
       "      <td>Machine</td>\n",
       "      <td>Morphology</td>\n",
       "      <td>WikiMatrix</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  is_correct  \\\n",
       "0           0  В 1929 году Ньюкомб переехал на Мальту в качес...           1   \n",
       "1           1  Вдруг решетка беззвучно поехала в сторону, и н...           1   \n",
       "2           2                       Этим летом не никуда ездили.           0   \n",
       "3           3  В 2011 году был выпущен документальный фильм «...           1   \n",
       "4           4  Поговорительница Пресли поссорилась с поп-звез...           0   \n",
       "\n",
       "  authored_by  error_type         source  \n",
       "0     Machine           0     WikiMatrix  \n",
       "1      Expert           0  Paducheva2004  \n",
       "2      Expert      Syntax        Rusgram  \n",
       "3     Machine           0     WikiMatrix  \n",
       "4     Machine  Morphology     WikiMatrix  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('./rucola/train.csv')\n",
    "#  './rucola/test.csv'\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a7f6e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c55f6bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Syntax', 'Semantics', 'Morphology', 'Hallucination', 'Commonsense', '0', 'Lexis'}\n"
     ]
    }
   ],
   "source": [
    "error_types = set()\n",
    "for _, row in train_df.iterrows():\n",
    "    error_types.update(row[\"error_type\"].split(\"|\"))\n",
    "print(error_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63bdc5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df.assign(morphology=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(hallucination=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(lexis=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(semantics=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "train_df = train_df.assign(syntax=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(commonsense=pd.Series(np.zeros(len(train_df), dtype=int)).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eccd536a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Syntax'}\n"
     ]
    }
   ],
   "source": [
    "error_types = set([\"Syntax\"])\n",
    "print(error_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ccc6af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label columns:  ['syntax']\n"
     ]
    }
   ],
   "source": [
    "cols = train_df.columns\n",
    "label_cols = list(cols[6:])\n",
    "num_labels = len(label_cols)\n",
    "print('Label columns: ', label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a0de01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in train_df.iterrows():\n",
    "    for error_type in error_types:\n",
    "        if error_type in row[\"error_type\"]:\n",
    "            train_df.at[i, error_type.lower()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e839f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(train_df.syntax.values)\n",
    "texts = list(train_df.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46aa6d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/vocab.json from cache at /home/bart/.cache/huggingface/transformers/01c769031f99defa27c759f85c0f21715d1506ed5c4ab01f7d7ef8c8b3b5674a.10df436f410c7709f54ad2bf7e973429f03c613012be0df709996d5aef3706af\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/merges.txt from cache at /home/bart/.cache/huggingface/transformers/6d742b9584f250f0088e6a6674f80ae706d8a390d6bc30f7cfaccb4292556f55.e4807b2d97082a2dfbf6623d7aa973a5057f87e44b329ec0b63daafd5d2ef593\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/config.json from cache at /home/bart/.cache/huggingface/transformers/191cf2edd2e8d2007f9a39479b45315ac1b79fe23fa4786511de1c8f9b9def46.ebd71ffa9cb74c7d1d39cf67343e2ae8465e2091f9fb70dbd5dec0cf58b1b60b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"sberbank-ai/ruRoberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer outputs:  dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoTokenizer, RobertaTokenizer\n",
    "\n",
    "\n",
    "# max_length = 100\n",
    "tokenizer = RobertaTokenizer.from_pretrained('sberbank-ai/ruRoberta-large', do_lower_case=False) # tokenizer\n",
    "encodings = tokenizer.batch_encode_plus(texts, padding='longest') # tokenizer's encoding method\n",
    "print('tokenizer outputs: ', encodings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "280805d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encodings['input_ids'] # tokenized and encoded sentences\n",
    "attention_masks = encodings['attention_mask'] # attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5448034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df label indices with only one instance:  []\n"
     ]
    }
   ],
   "source": [
    "# Identifying indices of 'one_hot_labels' entries that only occur once - this will allow us to stratify split our training data later\n",
    "label_counts = train_df.syntax.astype(str).value_counts()\n",
    "one_freq = label_counts[label_counts==1].keys()\n",
    "one_freq_idxs = sorted(list(train_df[train_df.syntax.astype(str).isin(one_freq)].index), reverse=True)\n",
    "print('df label indices with only one instance: ', one_freq_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6cdb0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering single instance inputs to force into the training set after stratified split\n",
    "one_freq_input_ids = [input_ids.pop(i) for i in one_freq_idxs]\n",
    "one_freq_attention_masks = [attention_masks.pop(i) for i in one_freq_idxs]\n",
    "one_freq_labels = [labels.pop(i) for i in one_freq_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fef58985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels, attention_masks,\n",
    "        random_state=2020, test_size=0.10, stratify = labels)\n",
    "\n",
    "# Add one frequency data to train data\n",
    "train_inputs.extend(one_freq_input_ids)\n",
    "train_labels.extend(one_freq_labels)\n",
    "train_masks.extend(one_freq_attention_masks)\n",
    "\n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fcd8aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning with XLNet, the authors recommend a batch size of 32, 48, or 128. We will use 32 here to avoid memory issues.\n",
    "batch_size = 14\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0bbbc7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(validation_dataloader, 'validation_data_loader')\n",
    "torch.save(train_dataloader, 'train_data_loader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51f11633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/config.json from cache at /home/bart/.cache/huggingface/transformers/191cf2edd2e8d2007f9a39479b45315ac1b79fe23fa4786511de1c8f9b9def46.ebd71ffa9cb74c7d1d39cf67343e2ae8465e2091f9fb70dbd5dec0cf58b1b60b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/home/jovyan/models/roberta/roberta_l\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/pytorch_model.bin from cache at /home/bart/.cache/huggingface/transformers/57935eccbb92bb4e5465411da844aeb075cac5b8c255c770a81463fa909d8ccc.f03af7b16e88082afbdd696d0647fe6bf624c0776d71dcece2b0e9973d081b26\n",
      "Some weights of the model checkpoint at sberbank-ai/ruRoberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sberbank-ai/ruRoberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"sberbank-ai/ruRoberta-large\", num_labels=num_labels)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30f428e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83709991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bart/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=5e-7, correct_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "792372f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                                                                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.43491881665304105\n",
      "Eval loss: 0.36454904079437256\n",
      "Best Threshold:  0.35\n",
      "Test F1 Accuracy:  0.5751633986928104\n",
      "Test Flat Accuracy:  0.8490712074303406 \n",
      "\n",
      "F1 Validation Accuracy:  37.19512195121951\n",
      "Flat Validation Accuracy:  84.05572755417957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|████████████▎                                                                                                              | 1/10 [05:38<50:49, 338.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.35012167857155185\n",
      "Eval loss: 0.3465913236141205\n",
      "Best Threshold:  0.31\n",
      "Test F1 Accuracy:  0.6300211416490487\n",
      "Test Flat Accuracy:  0.8645510835913313 \n",
      "\n",
      "F1 Validation Accuracy:  56.330749354005185\n",
      "Flat Validation Accuracy:  86.91950464396285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|████████████████████████▌                                                                                                  | 2/10 [11:13<44:48, 336.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3136877304864274\n",
      "Eval loss: 0.3497809171676636\n",
      "Best Threshold:  0.25\n",
      "Test F1 Accuracy:  0.6680497925311203\n",
      "Test Flat Accuracy:  0.8761609907120743 \n",
      "\n",
      "F1 Validation Accuracy:  57.14285714285714\n",
      "Flat Validation Accuracy:  86.9969040247678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|████████████████████████████████████▉                                                                                      | 3/10 [17:02<39:46, 340.99s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Update parameters and take a step using the computed gradient\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# scheduler.step()\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Update tracking variables\u001b[39;00m\n\u001b[1;32m     51\u001b[0m tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/optimization.py:360\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# In-place operations to update the averages at the same time\u001b[39;00m\n\u001b[1;32m    359\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta1))\n\u001b[0;32m--> 360\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m denom \u001b[38;5;241m=\u001b[39m exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt()\u001b[38;5;241m.\u001b[39madd_(group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    363\u001b[0m step_size \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "# num_labels = 2\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 10\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for epoch_num in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "    # Training\n",
    "\n",
    "    # Set our model to training mode (as opposed to evaluation mode)\n",
    "    model.train()\n",
    "\n",
    "    # Tracking variables\n",
    "    tr_loss = 0 #running loss\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "  # Train the data for one epoch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # # Forward pass for multiclass classification\n",
    "        # outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        # loss = outputs[0]\n",
    "        # logits = outputs[1]\n",
    "\n",
    "        # Forward pass for multilabel classification\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "        \n",
    "        loss_func = BCEWithLogitsLoss() \n",
    "        loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "        # loss_func = BCELoss() \n",
    "        # loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "        train_loss_set.append(loss.item())    \n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "    # Validation\n",
    "\n",
    "    # Put model in evaluation mode to evaluate loss on the validation set\n",
    "    model.eval()\n",
    "\n",
    "    # Variables to gather full output\n",
    "    logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n",
    "\n",
    "  # Predict\n",
    "    nb_eval_steps = 0\n",
    "    eval_loss = 0.0\n",
    "    for i, batch in enumerate(validation_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "      # Forward pass\n",
    "            outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            b_logit_pred = outs[0]\n",
    "            \n",
    "            loss_func = BCEWithLogitsLoss() \n",
    "            loss = loss_func(b_logit_pred.view(-1, num_labels), b_labels.type_as(b_logit_pred).view(-1, num_labels)) #convert labels to float for calculation\n",
    "        \n",
    "\n",
    "            pred_label = torch.sigmoid(b_logit_pred)\n",
    "\n",
    "            b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "            pred_label = pred_label.to('cpu').numpy()\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        nb_eval_steps += 1\n",
    "        eval_loss += loss\n",
    "        tokenized_texts.append(b_input_ids)\n",
    "        logit_preds.append(b_logit_pred)\n",
    "        true_labels.append(b_labels)\n",
    "        pred_labels.append(pred_label)\n",
    "    print(\"Eval loss: {}\".format(eval_loss/nb_eval_steps))\n",
    "        \n",
    "#         print(\"TRUE LABELS\", true_labels)\n",
    "#         print(\"PRED LABELS\", pred_labels)\n",
    "\n",
    "  # Flatten outputs\n",
    "    pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    threshold = 0.50\n",
    "    pred_bools = [pl>threshold for pl in pred_labels]\n",
    "    true_bools = [tl==1 for tl in true_labels]\n",
    "    \n",
    "    macro_thresholds = np.array(range(1,10))/10\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in macro_thresholds:\n",
    "        pred_bools_minmax = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools_minmax)\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools_minmax)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_macro_th = macro_thresholds[np.argmax(f1_results)] #best macro threshold value\n",
    "\n",
    "    micro_thresholds = (np.array(range(10))/100)+best_macro_th #calculating micro threshold values\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in micro_thresholds:\n",
    "        pred_bools_minmax = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools_minmax)\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools_minmax)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_f1_idx = np.argmax(f1_results) #best threshold value\n",
    "\n",
    "    # Printing and saving classification report\n",
    "    print('Best Threshold: ', micro_thresholds[best_f1_idx])\n",
    "    print('Test F1 Accuracy: ', f1_results[best_f1_idx])\n",
    "    print('Test Flat Accuracy: ', flat_acc_results[best_f1_idx], '\\n')\n",
    "\n",
    "    best_pred_bools = [pl>micro_thresholds[best_f1_idx] for pl in pred_labels]\n",
    "\n",
    "    \n",
    "    val_f1_accuracy = f1_score(true_bools,pred_bools)*100\n",
    "    val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
    "\n",
    "    print('F1 Validation Accuracy: ', val_f1_accuracy)\n",
    "    print('Flat Validation Accuracy: ', val_flat_accuracy)\n",
    "    torch.save(model.state_dict(), f'roberta_syntax_{epoch_num}_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fb8347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3285f716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a95aa21",
   "metadata": {},
   "source": [
    "# Semantics Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f65dfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-04 22:18:23.106063: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-04 22:18:23.106083: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                               text  is_correct  \\\n",
      "0           0  В 1929 году Ньюкомб переехал на Мальту в качес...           1   \n",
      "1           1  Вдруг решетка беззвучно поехала в сторону, и н...           1   \n",
      "2           2                       Этим летом не никуда ездили.           0   \n",
      "3           3  В 2011 году был выпущен документальный фильм «...           1   \n",
      "4           4  Поговорительница Пресли поссорилась с поп-звез...           0   \n",
      "\n",
      "  authored_by  error_type         source  \n",
      "0     Machine           0     WikiMatrix  \n",
      "1      Expert           0  Paducheva2004  \n",
      "2      Expert      Syntax        Rusgram  \n",
      "3     Machine           0     WikiMatrix  \n",
      "4     Machine  Morphology     WikiMatrix  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\n",
    "import pickle\n",
    "from transformers import *\n",
    "from tqdm import tqdm, trange\n",
    "from ast import literal_eval\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('./rucola/train.csv')\n",
    "#  './rucola/test.csv'\n",
    "print(train_df.head())\n",
    "device = 'cuda'\n",
    "\n",
    "# train_df = train_df.assign(morphology=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(hallucination=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(lexis=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "train_df = train_df.assign(semantics=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(syntax=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(commonsense=pd.Series(np.zeros(len(train_df), dtype=int)).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5dcfe2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Syntax', 'Commonsense', 'Hallucination', 'Morphology', 'Semantics', 'Lexis', '0'}\n",
      "{'Semantics'}\n",
      "Label columns:  ['semantics']\n"
     ]
    }
   ],
   "source": [
    "error_types = set()\n",
    "for _, row in train_df.iterrows():\n",
    "    error_types.update(row[\"error_type\"].split(\"|\"))\n",
    "print(error_types)\n",
    "\n",
    "# ВОТ ЗДЕСЬ ПОМЕНЯТЬ\n",
    "error_types = set([\"Semantics\"])\n",
    "print(error_types)\n",
    "\n",
    "cols = train_df.columns\n",
    "# И ВОТ ЗДЕСЬ\n",
    "label_cols = list(cols[6:])\n",
    "num_labels = len(label_cols)\n",
    "print('Label columns: ', label_cols)\n",
    "\n",
    "for i, row in train_df.iterrows():\n",
    "    for error_type in error_types:\n",
    "        if error_type in row[\"error_type\"]:\n",
    "            train_df.at[i, error_type.lower()] = 1\n",
    "\n",
    "# И ВОТ ЗДЕСЬ ЕЩЕ\n",
    "labels = list(train_df.semantics.values)\n",
    "texts = list(train_df.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb526d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/vocab.json from cache at /home/bart/.cache/huggingface/transformers/01c769031f99defa27c759f85c0f21715d1506ed5c4ab01f7d7ef8c8b3b5674a.10df436f410c7709f54ad2bf7e973429f03c613012be0df709996d5aef3706af\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/merges.txt from cache at /home/bart/.cache/huggingface/transformers/6d742b9584f250f0088e6a6674f80ae706d8a390d6bc30f7cfaccb4292556f55.e4807b2d97082a2dfbf6623d7aa973a5057f87e44b329ec0b63daafd5d2ef593\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/config.json from cache at /home/bart/.cache/huggingface/transformers/191cf2edd2e8d2007f9a39479b45315ac1b79fe23fa4786511de1c8f9b9def46.ebd71ffa9cb74c7d1d39cf67343e2ae8465e2091f9fb70dbd5dec0cf58b1b60b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"sberbank-ai/ruRoberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer outputs:  dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoTokenizer, RobertaTokenizer\n",
    "\n",
    "\n",
    "# max_length = 100\n",
    "tokenizer = RobertaTokenizer.from_pretrained('sberbank-ai/ruRoberta-large', do_lower_case=False) # tokenizer\n",
    "encodings = tokenizer.batch_encode_plus(texts, padding='longest') # tokenizer's encoding method\n",
    "print('tokenizer outputs: ', encodings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "603cc981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df label indices with only one instance:  []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/config.json from cache at /home/bart/.cache/huggingface/transformers/191cf2edd2e8d2007f9a39479b45315ac1b79fe23fa4786511de1c8f9b9def46.ebd71ffa9cb74c7d1d39cf67343e2ae8465e2091f9fb70dbd5dec0cf58b1b60b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/home/jovyan/models/roberta/roberta_l\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/pytorch_model.bin from cache at /home/bart/.cache/huggingface/transformers/57935eccbb92bb4e5465411da844aeb075cac5b8c255c770a81463fa909d8ccc.f03af7b16e88082afbdd696d0647fe6bf624c0776d71dcece2b0e9973d081b26\n",
      "Some weights of the model checkpoint at sberbank-ai/ruRoberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sberbank-ai/ruRoberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/bart/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_ids = encodings['input_ids'] # tokenized and encoded sentences\n",
    "attention_masks = encodings['attention_mask'] # attention masks\n",
    "\n",
    "#ВОТ ЗДЕСЬ ПОМЕНЯТЬ\n",
    "label_counts = train_df.semantics.astype(str).value_counts()\n",
    "one_freq = label_counts[label_counts==1].keys()\n",
    "one_freq_idxs = sorted(list(train_df[train_df.semantics.astype(str).isin(one_freq)].index), reverse=True)\n",
    "print('df label indices with only one instance: ', one_freq_idxs)\n",
    "\n",
    "\n",
    "one_freq_input_ids = [input_ids.pop(i) for i in one_freq_idxs]\n",
    "one_freq_attention_masks = [attention_masks.pop(i) for i in one_freq_idxs]\n",
    "one_freq_labels = [labels.pop(i) for i in one_freq_idxs]\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels, attention_masks,\n",
    "        random_state=2020, test_size=0.10, stratify = labels)\n",
    "\n",
    "train_inputs.extend(one_freq_input_ids)\n",
    "train_labels.extend(one_freq_labels)\n",
    "train_masks.extend(one_freq_attention_masks)\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "\n",
    "batch_size = 14\n",
    "\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "torch.save(validation_dataloader, 'validation_data_loader')\n",
    "torch.save(train_dataloader, 'train_data_loader')\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"sberbank-ai/ruRoberta-large\", num_labels=num_labels)\n",
    "model.cuda()\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=5e-7, correct_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bf2499c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                                                                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.5520970946184565\n",
      "Eval loss: 1.4418853521347046\n",
      "Best Threshold:  0.73\n",
      "Test F1 Accuracy:  0.27472527472527475\n",
      "Test Flat Accuracy:  0.7956656346749226 \n",
      "\n",
      "F1 Validation Accuracy:  16.590701914311758\n",
      "Flat Validation Accuracy:  29.17956656346749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|████████████▎                                                                                                              | 1/10 [05:35<50:19, 335.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.4576851190499307\n",
      "Eval loss: 1.3658764362335205\n",
      "Best Threshold:  0.73\n",
      "Test F1 Accuracy:  0.3260188087774295\n",
      "Test Flat Accuracy:  0.8335913312693498 \n",
      "\n",
      "F1 Validation Accuracy:  20.0\n",
      "Flat Validation Accuracy:  46.749226006191954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|████████████████████████▌                                                                                                  | 2/10 [11:07<44:29, 333.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.3895097233615052\n",
      "Eval loss: 1.3053600788116455\n",
      "Best Threshold:  0.74\n",
      "Test F1 Accuracy:  0.358974358974359\n",
      "Test Flat Accuracy:  0.8645510835913313 \n",
      "\n",
      "F1 Validation Accuracy:  21.693121693121693\n",
      "Flat Validation Accuracy:  54.179566563467496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|████████████████████████████████████▉                                                                                      | 3/10 [16:40<38:52, 333.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.296689233409799\n",
      "Eval loss: 1.2690678834915161\n",
      "Best Threshold:  0.73\n",
      "Test F1 Accuracy:  0.37344398340248963\n",
      "Test Flat Accuracy:  0.8831269349845201 \n",
      "\n",
      "F1 Validation Accuracy:  28.102189781021895\n",
      "Flat Validation Accuracy:  69.5046439628483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|█████████████████████████████████████████████████▏                                                                         | 4/10 [22:11<33:14, 332.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1905144835350459\n",
      "Eval loss: 1.282047986984253\n",
      "Best Threshold:  0.76\n",
      "Test F1 Accuracy:  0.3879310344827586\n",
      "Test Flat Accuracy:  0.8900928792569659 \n",
      "\n",
      "F1 Validation Accuracy:  31.674208144796385\n",
      "Flat Validation Accuracy:  76.62538699690403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████████████████████████████████████████████████████████████▌                                                             | 5/10 [27:42<27:39, 331.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1217741076840104\n",
      "Eval loss: 1.3322372436523438\n",
      "Best Threshold:  0.76\n",
      "Test F1 Accuracy:  0.3876651982378855\n",
      "Test Flat Accuracy:  0.8924148606811145 \n",
      "\n",
      "F1 Validation Accuracy:  33.59173126614987\n",
      "Flat Validation Accuracy:  80.10835913312694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|█████████████████████████████████████████████████████████████████████████▊                                                 | 6/10 [33:13<22:05, 331.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.0048768793453522\n",
      "Eval loss: 1.4530199766159058\n",
      "Best Threshold:  0.7799999999999999\n",
      "Test F1 Accuracy:  0.4093023255813953\n",
      "Test Flat Accuracy:  0.901702786377709 \n",
      "\n",
      "F1 Validation Accuracy:  35.92814371257485\n",
      "Flat Validation Accuracy:  83.43653250773994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|██████████████████████████████████████████████████████████████████████████████████████                                     | 7/10 [38:44<16:33, 331.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.925469541879624\n",
      "Eval loss: 1.566460371017456\n",
      "Best Threshold:  0.6\n",
      "Test F1 Accuracy:  0.40145985401459855\n",
      "Test Flat Accuracy:  0.8730650154798761 \n",
      "\n",
      "F1 Validation Accuracy:  37.540453074433664\n",
      "Flat Validation Accuracy:  85.06191950464397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|██████████████████████████████████████████████████████████████████████████████████████████████████▍                        | 8/10 [44:14<11:02, 331.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.8132642108754274\n",
      "Eval loss: 1.5846630334854126\n",
      "Best Threshold:  0.75\n",
      "Test F1 Accuracy:  0.43137254901960786\n",
      "Test Flat Accuracy:  0.8877708978328174 \n",
      "\n",
      "F1 Validation Accuracy:  38.69047619047619\n",
      "Flat Validation Accuracy:  84.05572755417957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋            | 9/10 [49:45<05:31, 331.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.7225380661739339\n",
      "Eval loss: 1.7614424228668213\n",
      "Best Threshold:  0.75\n",
      "Test F1 Accuracy:  0.41463414634146345\n",
      "Test Flat Accuracy:  0.8885448916408669 \n",
      "\n",
      "F1 Validation Accuracy:  40.0\n",
      "Flat Validation Accuracy:  85.60371517027863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [55:17<00:00, 331.76s/it]\n"
     ]
    }
   ],
   "source": [
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "# num_labels = 2\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 10\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for epoch_num in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "    # Training\n",
    "\n",
    "    # Set our model to training mode (as opposed to evaluation mode)\n",
    "    model.train()\n",
    "\n",
    "    # Tracking variables\n",
    "    tr_loss = 0 #running loss\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "  # Train the data for one epoch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # # Forward pass for multiclass classification\n",
    "        # outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        # loss = outputs[0]\n",
    "        # logits = outputs[1]\n",
    "\n",
    "        # Forward pass for multilabel classification\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "        \n",
    "        loss_func = BCEWithLogitsLoss(pos_weight=torch.tensor([20]).to(device)) \n",
    "        loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "        # loss_func = BCELoss() \n",
    "        # loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "        train_loss_set.append(loss.item())    \n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "    # Validation\n",
    "\n",
    "    # Put model in evaluation mode to evaluate loss on the validation set\n",
    "    model.eval()\n",
    "\n",
    "    # Variables to gather full output\n",
    "    logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n",
    "\n",
    "  # Predict\n",
    "    nb_eval_steps = 0\n",
    "    eval_loss = 0.0\n",
    "    for i, batch in enumerate(validation_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "      # Forward pass\n",
    "            outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            b_logit_pred = outs[0]\n",
    "            \n",
    "            loss_func = BCEWithLogitsLoss(pos_weight=torch.tensor([20]).to(device)) \n",
    "            loss = loss_func(b_logit_pred.view(-1, num_labels), b_labels.type_as(b_logit_pred).view(-1, num_labels)) #convert labels to float for calculation\n",
    "        \n",
    "\n",
    "            pred_label = torch.sigmoid(b_logit_pred)\n",
    "\n",
    "            b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "            pred_label = pred_label.to('cpu').numpy()\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        nb_eval_steps += 1\n",
    "        eval_loss += loss\n",
    "        tokenized_texts.append(b_input_ids)\n",
    "        logit_preds.append(b_logit_pred)\n",
    "        true_labels.append(b_labels)\n",
    "        pred_labels.append(pred_label)\n",
    "    print(\"Eval loss: {}\".format(eval_loss/nb_eval_steps))\n",
    "        \n",
    "#         print(\"TRUE LABELS\", true_labels)\n",
    "#         print(\"PRED LABELS\", pred_labels)\n",
    "\n",
    "  # Flatten outputs\n",
    "    pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    threshold = 0.50\n",
    "    pred_bools = [pl>threshold for pl in pred_labels]\n",
    "    true_bools = [tl==1 for tl in true_labels]\n",
    "    \n",
    "    macro_thresholds = np.array(range(1,10))/10\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in macro_thresholds:\n",
    "        pred_bools_minmax = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools_minmax)\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools_minmax)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_macro_th = macro_thresholds[np.argmax(f1_results)] #best macro threshold value\n",
    "\n",
    "    micro_thresholds = (np.array(range(10))/100)+best_macro_th #calculating micro threshold values\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in micro_thresholds:\n",
    "        pred_bools_minmax = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools_minmax)\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools_minmax)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_f1_idx = np.argmax(f1_results) #best threshold value\n",
    "\n",
    "    # Printing and saving classification report\n",
    "    print('Best Threshold: ', micro_thresholds[best_f1_idx])\n",
    "    print('Test F1 Accuracy: ', f1_results[best_f1_idx])\n",
    "    print('Test Flat Accuracy: ', flat_acc_results[best_f1_idx], '\\n')\n",
    "\n",
    "    best_pred_bools = [pl>micro_thresholds[best_f1_idx] for pl in pred_labels]\n",
    "\n",
    "    \n",
    "    val_f1_accuracy = f1_score(true_bools,pred_bools)*100\n",
    "    val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
    "\n",
    "    print('F1 Validation Accuracy: ', val_f1_accuracy)\n",
    "    print('Flat Validation Accuracy: ', val_flat_accuracy)\n",
    "    torch.save(model.state_dict(), f'roberta_semantics_{epoch_num}_epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fabdcb",
   "metadata": {},
   "source": [
    "Epoch:  30%|████████████████████████████████████▉                                                                                      | 3/10 [16:26<38:23, 329.12s/it]\n",
    "Train loss: 0.7307959680235917\n",
    "Eval loss: 0.9185694456100464\n",
    "Best Threshold:  0.63\n",
    "Test F1 Accuracy:  0.4369747899159664\n",
    "Test Flat Accuracy:  0.8962848297213623 \n",
    "\n",
    "F1 Validation Accuracy:  38.989169675090245\n",
    "Flat Validation Accuracy:  86.91950464396285"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b749db0",
   "metadata": {},
   "source": [
    "# Lexis RuRoberta classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdb5f64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-04 11:30:56.113849: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-04 11:30:56.113868: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                               text  is_correct  \\\n",
      "0           0  В 1929 году Ньюкомб переехал на Мальту в качес...           1   \n",
      "1           1  Вдруг решетка беззвучно поехала в сторону, и н...           1   \n",
      "2           2                       Этим летом не никуда ездили.           0   \n",
      "3           3  В 2011 году был выпущен документальный фильм «...           1   \n",
      "4           4  Поговорительница Пресли поссорилась с поп-звез...           0   \n",
      "\n",
      "  authored_by  error_type         source  \n",
      "0     Machine           0     WikiMatrix  \n",
      "1      Expert           0  Paducheva2004  \n",
      "2      Expert      Syntax        Rusgram  \n",
      "3     Machine           0     WikiMatrix  \n",
      "4     Machine  Morphology     WikiMatrix  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\n",
    "import pickle\n",
    "from transformers import *\n",
    "from tqdm import tqdm, trange\n",
    "from ast import literal_eval\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('./rucola/train.csv')\n",
    "#  './rucola/test.csv'\n",
    "print(train_df.head())\n",
    "device = 'cuda'\n",
    "\n",
    "# train_df = train_df.assign(morphology=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(hallucination=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "train_df = train_df.assign(lexis=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(semantics=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(syntax=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(commonsense=pd.Series(np.zeros(len(train_df), dtype=int)).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f46aeb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Commonsense', 'Syntax', 'Morphology', 'Lexis', '0', 'Semantics', 'Hallucination'}\n",
      "{'Lexis'}\n",
      "Label columns:  ['lexis']\n"
     ]
    }
   ],
   "source": [
    "error_types = set()\n",
    "for _, row in train_df.iterrows():\n",
    "    error_types.update(row[\"error_type\"].split(\"|\"))\n",
    "print(error_types)\n",
    "\n",
    "# ВОТ ЗДЕСЬ ПОМЕНЯТЬ\n",
    "error_types = set([\"Lexis\"])\n",
    "print(error_types)\n",
    "\n",
    "cols = train_df.columns\n",
    "# И ВОТ ЗДЕСЬ\n",
    "label_cols = list(cols[6:])\n",
    "num_labels = len(label_cols)\n",
    "print('Label columns: ', label_cols)\n",
    "\n",
    "for i, row in train_df.iterrows():\n",
    "    for error_type in error_types:\n",
    "        if error_type in row[\"error_type\"]:\n",
    "            train_df.at[i, error_type.lower()] = 1\n",
    "\n",
    "# И ВОТ ЗДЕСЬ ЕЩЕ\n",
    "labels = list(train_df.lexis.values)\n",
    "texts = list(train_df.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e044876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/vocab.json from cache at /home/bart/.cache/huggingface/transformers/01c769031f99defa27c759f85c0f21715d1506ed5c4ab01f7d7ef8c8b3b5674a.10df436f410c7709f54ad2bf7e973429f03c613012be0df709996d5aef3706af\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/merges.txt from cache at /home/bart/.cache/huggingface/transformers/6d742b9584f250f0088e6a6674f80ae706d8a390d6bc30f7cfaccb4292556f55.e4807b2d97082a2dfbf6623d7aa973a5057f87e44b329ec0b63daafd5d2ef593\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/config.json from cache at /home/bart/.cache/huggingface/transformers/191cf2edd2e8d2007f9a39479b45315ac1b79fe23fa4786511de1c8f9b9def46.ebd71ffa9cb74c7d1d39cf67343e2ae8465e2091f9fb70dbd5dec0cf58b1b60b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"sberbank-ai/ruRoberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer outputs:  dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoTokenizer, RobertaTokenizer\n",
    "\n",
    "\n",
    "# max_length = 100\n",
    "tokenizer = RobertaTokenizer.from_pretrained('sberbank-ai/ruRoberta-large', do_lower_case=False) # tokenizer\n",
    "encodings = tokenizer.batch_encode_plus(texts, padding='longest') # tokenizer's encoding method\n",
    "print('tokenizer outputs: ', encodings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfa2e266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df label indices with only one instance:  []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/config.json from cache at /home/bart/.cache/huggingface/transformers/191cf2edd2e8d2007f9a39479b45315ac1b79fe23fa4786511de1c8f9b9def46.ebd71ffa9cb74c7d1d39cf67343e2ae8465e2091f9fb70dbd5dec0cf58b1b60b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/home/jovyan/models/roberta/roberta_l\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/pytorch_model.bin from cache at /home/bart/.cache/huggingface/transformers/57935eccbb92bb4e5465411da844aeb075cac5b8c255c770a81463fa909d8ccc.f03af7b16e88082afbdd696d0647fe6bf624c0776d71dcece2b0e9973d081b26\n",
      "Some weights of the model checkpoint at sberbank-ai/ruRoberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sberbank-ai/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/bart/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_ids = encodings['input_ids'] # tokenized and encoded sentences\n",
    "attention_masks = encodings['attention_mask'] # attention masks\n",
    "\n",
    "#ВОТ ЗДЕСЬ ПОМЕНЯТЬ\n",
    "label_counts = train_df.lexis.astype(str).value_counts()\n",
    "one_freq = label_counts[label_counts==1].keys()\n",
    "one_freq_idxs = sorted(list(train_df[train_df.lexis.astype(str).isin(one_freq)].index), reverse=True)\n",
    "print('df label indices with only one instance: ', one_freq_idxs)\n",
    "\n",
    "\n",
    "one_freq_input_ids = [input_ids.pop(i) for i in one_freq_idxs]\n",
    "one_freq_attention_masks = [attention_masks.pop(i) for i in one_freq_idxs]\n",
    "one_freq_labels = [labels.pop(i) for i in one_freq_idxs]\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels, attention_masks,\n",
    "        random_state=2020, test_size=0.10, stratify = labels)\n",
    "\n",
    "train_inputs.extend(one_freq_input_ids)\n",
    "train_labels.extend(one_freq_labels)\n",
    "train_masks.extend(one_freq_attention_masks)\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "\n",
    "batch_size = 14\n",
    "\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "torch.save(validation_dataloader, 'validation_data_loader')\n",
    "torch.save(train_dataloader, 'train_data_loader')\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"sberbank-ai/ruRoberta-large\", num_labels=num_labels)\n",
    "model.cuda()\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=5e-7, correct_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "511a2373",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                                                                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1551466820902774\n",
      "Eval loss: 1.0872077941894531\n",
      "Best Threshold:  0.6\n",
      "Test F1 Accuracy:  0.2917933130699088\n",
      "Test Flat Accuracy:  0.8196594427244582 \n",
      "\n",
      "F1 Validation Accuracy:  26.26970227670753\n",
      "Flat Validation Accuracy:  67.41486068111455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|████████████▎                                                                                                              | 1/10 [05:20<48:05, 320.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.0894129726358843\n",
      "Eval loss: 1.0790956020355225\n",
      "Best Threshold:  0.6599999999999999\n",
      "Test F1 Accuracy:  0.3050847457627119\n",
      "Test Flat Accuracy:  0.8413312693498453 \n",
      "\n",
      "F1 Validation Accuracy:  26.102292768959433\n",
      "Flat Validation Accuracy:  67.56965944272446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|████████████████████████▌                                                                                                  | 2/10 [10:41<42:45, 320.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.0816408683390106\n",
      "Eval loss: 1.0745985507965088\n",
      "Best Threshold:  0.7\n",
      "Test F1 Accuracy:  0.2899628252788104\n",
      "Test Flat Accuracy:  0.8521671826625387 \n",
      "\n",
      "F1 Validation Accuracy:  26.689774696707104\n",
      "Flat Validation Accuracy:  67.26006191950464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|████████████████████████████████████▉                                                                                      | 3/10 [16:02<37:25, 320.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.037941712262972\n",
      "Eval loss: 1.074633240699768\n",
      "Best Threshold:  0.64\n",
      "Test F1 Accuracy:  0.30582524271844663\n",
      "Test Flat Accuracy:  0.7786377708978328 \n",
      "\n",
      "F1 Validation Accuracy:  25.908372827804104\n",
      "Flat Validation Accuracy:  63.69969040247678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|█████████████████████████████████████████████████▏                                                                         | 4/10 [21:23<32:05, 320.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.0221181084030946\n",
      "Eval loss: 1.057361125946045\n",
      "Best Threshold:  0.61\n",
      "Test F1 Accuracy:  0.31216931216931215\n",
      "Test Flat Accuracy:  0.7987616099071208 \n",
      "\n",
      "F1 Validation Accuracy:  28.044280442804432\n",
      "Flat Validation Accuracy:  69.81424148606811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████████████████████████████████████████████████████████████▌                                                             | 5/10 [26:44<26:44, 320.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.0116351993864336\n",
      "Eval loss: 1.0504114627838135\n",
      "Best Threshold:  0.7\n",
      "Test F1 Accuracy:  0.3049853372434018\n",
      "Test Flat Accuracy:  0.81656346749226 \n",
      "\n",
      "F1 Validation Accuracy:  26.34146341463414\n",
      "Flat Validation Accuracy:  64.93808049535603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|█████████████████████████████████████████████████████████████████████████▊                                                 | 6/10 [32:05<21:23, 320.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.9921138568475358\n",
      "Eval loss: 1.0562021732330322\n",
      "Best Threshold:  0.61\n",
      "Test F1 Accuracy:  0.30500000000000005\n",
      "Test Flat Accuracy:  0.7848297213622291 \n",
      "\n",
      "F1 Validation Accuracy:  28.83895131086142\n",
      "Flat Validation Accuracy:  70.58823529411765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|██████████████████████████████████████████████████████████████████████████████████████                                     | 7/10 [37:26<16:02, 320.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.9713360816060994\n",
      "Eval loss: 1.0583086013793945\n",
      "Best Threshold:  0.6\n",
      "Test F1 Accuracy:  0.3030303030303031\n",
      "Test Flat Accuracy:  0.7685758513931888 \n",
      "\n",
      "F1 Validation Accuracy:  28.51985559566787\n",
      "Flat Validation Accuracy:  69.3498452012384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|██████████████████████████████████████████████████████████████████████████████████████████████████▍                        | 8/10 [42:47<10:41, 320.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.004841740434781\n",
      "Eval loss: 1.06016206741333\n",
      "Best Threshold:  0.6\n",
      "Test F1 Accuracy:  0.3102625298329356\n",
      "Test Flat Accuracy:  0.7763157894736842 \n",
      "\n",
      "F1 Validation Accuracy:  28.937728937728934\n",
      "Flat Validation Accuracy:  69.96904024767801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋            | 9/10 [48:08<05:21, 321.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.9601786064721474\n",
      "Eval loss: 1.0532913208007812\n",
      "Best Threshold:  0.6\n",
      "Test F1 Accuracy:  0.3151515151515151\n",
      "Test Flat Accuracy:  0.8250773993808049 \n",
      "\n",
      "F1 Validation Accuracy:  29.885057471264364\n",
      "Flat Validation Accuracy:  76.39318885448917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [53:31<00:00, 321.12s/it]\n"
     ]
    }
   ],
   "source": [
    "train_loss_set = []\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch_num in trange(epochs, desc=\"Epoch\"):\n",
    "    model.train()\n",
    "\n",
    "    tr_loss = 0 \n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "        \n",
    "        loss_func = BCEWithLogitsLoss(pos_weight=torch.tensor([10]).to(device)) \n",
    "        loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels))\n",
    "\n",
    "        train_loss_set.append(loss.item())    \n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "    # Validation\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n",
    "\n",
    "    nb_eval_steps = 0\n",
    "    eval_loss = 0.0\n",
    "    for i, batch in enumerate(validation_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            b_logit_pred = outs[0]\n",
    "            \n",
    "            loss_func = BCEWithLogitsLoss(pos_weight=torch.tensor([10]).to(device)) \n",
    "            loss = loss_func(b_logit_pred.view(-1, num_labels), b_labels.type_as(b_logit_pred).view(-1, num_labels)) #convert labels to float for calculation\n",
    "        \n",
    "\n",
    "            pred_label = torch.sigmoid(b_logit_pred)\n",
    "\n",
    "            b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "            pred_label = pred_label.to('cpu').numpy()\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        nb_eval_steps += 1\n",
    "        eval_loss += loss\n",
    "        tokenized_texts.append(b_input_ids)\n",
    "        logit_preds.append(b_logit_pred)\n",
    "        true_labels.append(b_labels)\n",
    "        pred_labels.append(pred_label)\n",
    "    print(\"Eval loss: {}\".format(eval_loss/nb_eval_steps))\n",
    "        \n",
    "#         print(\"TRUE LABELS\", true_labels)\n",
    "#         print(\"PRED LABELS\", pred_labels)\n",
    "\n",
    "  # Flatten outputs\n",
    "    pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    threshold = 0.50\n",
    "    pred_bools = [pl>threshold for pl in pred_labels]\n",
    "    true_bools = [tl==1 for tl in true_labels]\n",
    "    \n",
    "    macro_thresholds = np.array(range(1,10))/10\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in macro_thresholds:\n",
    "        pred_bools_minmax = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools_minmax)\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools_minmax)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_macro_th = macro_thresholds[np.argmax(f1_results)] #best macro threshold value\n",
    "\n",
    "    micro_thresholds = (np.array(range(10))/100)+best_macro_th #calculating micro threshold values\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in micro_thresholds:\n",
    "        pred_bools_minmax = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools_minmax)\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools_minmax)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_f1_idx = np.argmax(f1_results) #best threshold value\n",
    "\n",
    "    # Printing and saving classification report\n",
    "    print('Best Threshold: ', micro_thresholds[best_f1_idx])\n",
    "    print('Test F1 Accuracy: ', f1_results[best_f1_idx])\n",
    "    print('Test Flat Accuracy: ', flat_acc_results[best_f1_idx], '\\n')\n",
    "\n",
    "    best_pred_bools = [pl>micro_thresholds[best_f1_idx] for pl in pred_labels]\n",
    "\n",
    "    \n",
    "    val_f1_accuracy = f1_score(true_bools,pred_bools)*100\n",
    "    val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
    "\n",
    "    print('F1 Validation Accuracy: ', val_f1_accuracy)\n",
    "    print('Flat Validation Accuracy: ', val_flat_accuracy)\n",
    "    torch.save(model.state_dict(), f'roberta_lexis_{epoch_num}_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c91f67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                                                                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.941356969564687\n",
      "Eval loss: 1.058608055114746\n",
      "Best Threshold:  0.62\n",
      "Test F1 Accuracy:  0.3161094224924012\n",
      "Test Flat Accuracy:  0.8258513931888545 \n",
      "\n",
      "F1 Validation Accuracy:  30.73496659242762\n",
      "Flat Validation Accuracy:  75.92879256965944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|████████████▎                                                                                                              | 1/10 [05:19<47:56, 319.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.9160150175849167\n",
      "Eval loss: 1.0610003471374512\n",
      "Best Threshold:  0.62\n",
      "Test F1 Accuracy:  0.31843575418994413\n",
      "Test Flat Accuracy:  0.8111455108359134 \n",
      "\n",
      "F1 Validation Accuracy:  29.85685071574642\n",
      "Flat Validation Accuracy:  73.45201238390094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|████████████████████████▌                                                                                                  | 2/10 [10:30<41:57, 314.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.8928165054565159\n",
      "Eval loss: 1.063208818435669\n",
      "Best Threshold:  0.61\n",
      "Test F1 Accuracy:  0.32044198895027626\n",
      "Test Flat Accuracy:  0.8095975232198143 \n",
      "\n",
      "F1 Validation Accuracy:  30.641821946169774\n",
      "Flat Validation Accuracy:  74.07120743034056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|████████████████████████████████████▉                                                                                      | 3/10 [15:42<36:32, 313.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.8808809498372061\n",
      "Eval loss: 1.0761843919754028\n",
      "Best Threshold:  0.6\n",
      "Test F1 Accuracy:  0.33519553072625696\n",
      "Test Flat Accuracy:  0.8157894736842105 \n",
      "\n",
      "F1 Validation Accuracy:  30.065359477124183\n",
      "Flat Validation Accuracy:  75.15479876160991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|█████████████████████████████████████████████████▏                                                                         | 4/10 [20:54<31:15, 312.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.8628612617759212\n",
      "Eval loss: 1.0539538860321045\n",
      "Best Threshold:  0.73\n",
      "Test F1 Accuracy:  0.32542372881355935\n",
      "Test Flat Accuracy:  0.8459752321981424 \n",
      "\n",
      "F1 Validation Accuracy:  28.35538752362949\n",
      "Flat Validation Accuracy:  70.6656346749226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████████████████████████████████████████████████████████████▌                                                             | 5/10 [26:04<25:59, 311.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.8449674035667942\n",
      "Eval loss: 1.0758552551269531\n",
      "Best Threshold:  0.61\n",
      "Test F1 Accuracy:  0.33078880407124683\n",
      "Test Flat Accuracy:  0.7964396284829721 \n",
      "\n",
      "F1 Validation Accuracy:  29.482071713147405\n",
      "Flat Validation Accuracy:  72.60061919504643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|█████████████████████████████████████████████████████████████████████████▊                                                 | 6/10 [31:15<20:46, 311.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.8179626847504134\n",
      "Eval loss: 1.0846061706542969\n",
      "Best Threshold:  0.71\n",
      "Test F1 Accuracy:  0.3245033112582781\n",
      "Test Flat Accuracy:  0.8421052631578947 \n",
      "\n",
      "F1 Validation Accuracy:  30.58350100603622\n",
      "Flat Validation Accuracy:  73.29721362229103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|██████████████████████████████████████████████████████████████████████████████████████                                     | 7/10 [36:26<15:34, 311.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.776985435763421\n",
      "Eval loss: 1.1638553142547607\n",
      "Best Threshold:  0.7\n",
      "Test F1 Accuracy:  0.3161764705882353\n",
      "Test Flat Accuracy:  0.8560371517027864 \n",
      "\n",
      "F1 Validation Accuracy:  31.503579952267298\n",
      "Flat Validation Accuracy:  77.78637770897832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|██████████████████████████████████████████████████████████████████████████████████████████████████▍                        | 8/10 [42:19<10:34, 317.46s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m b_input_ids, b_input_mask, b_labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_input_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     21\u001b[0m loss_func \u001b[38;5;241m=\u001b[39m BCEWithLogitsLoss(pos_weight\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m10\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)) \n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:1209\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1207\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1209\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1220\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1221\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:851\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    842\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    844\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    845\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    846\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    849\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    850\u001b[0m )\n\u001b[0;32m--> 851\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    864\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:527\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    518\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    519\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    520\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:454\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    451\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    452\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 454\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    459\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/modeling_utils.py:2928\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   2925\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m   2926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m-> 2928\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:466\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 466\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:365\u001b[0m, in \u001b[0;36mRobertaIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 365\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1172\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_is_full_backward_hook\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_full_backward_hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m   1173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1174\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss_set = []\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch_num in trange(epochs, desc=\"Epoch\"):\n",
    "    model.train()\n",
    "\n",
    "    tr_loss = 0 \n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "        \n",
    "        loss_func = BCEWithLogitsLoss(pos_weight=torch.tensor([10]).to(device)) \n",
    "        loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels))\n",
    "\n",
    "        train_loss_set.append(loss.item())    \n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "    # Validation\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n",
    "\n",
    "    nb_eval_steps = 0\n",
    "    eval_loss = 0.0\n",
    "    for i, batch in enumerate(validation_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            b_logit_pred = outs[0]\n",
    "            \n",
    "            loss_func = BCEWithLogitsLoss(pos_weight=torch.tensor([10]).to(device)) \n",
    "            loss = loss_func(b_logit_pred.view(-1, num_labels), b_labels.type_as(b_logit_pred).view(-1, num_labels)) #convert labels to float for calculation\n",
    "        \n",
    "\n",
    "            pred_label = torch.sigmoid(b_logit_pred)\n",
    "\n",
    "            b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "            pred_label = pred_label.to('cpu').numpy()\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        nb_eval_steps += 1\n",
    "        eval_loss += loss\n",
    "        tokenized_texts.append(b_input_ids)\n",
    "        logit_preds.append(b_logit_pred)\n",
    "        true_labels.append(b_labels)\n",
    "        pred_labels.append(pred_label)\n",
    "    print(\"Eval loss: {}\".format(eval_loss/nb_eval_steps))\n",
    "        \n",
    "#         print(\"TRUE LABELS\", true_labels)\n",
    "#         print(\"PRED LABELS\", pred_labels)\n",
    "\n",
    "  # Flatten outputs\n",
    "    pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    threshold = 0.50\n",
    "    pred_bools = [pl>threshold for pl in pred_labels]\n",
    "    true_bools = [tl==1 for tl in true_labels]\n",
    "    \n",
    "    macro_thresholds = np.array(range(1,10))/10\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in macro_thresholds:\n",
    "        pred_bools_minmax = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools_minmax)\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools_minmax)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_macro_th = macro_thresholds[np.argmax(f1_results)] #best macro threshold value\n",
    "\n",
    "    micro_thresholds = (np.array(range(10))/100)+best_macro_th #calculating micro threshold values\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in micro_thresholds:\n",
    "        pred_bools_minmax = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools_minmax)\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools_minmax)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_f1_idx = np.argmax(f1_results) #best threshold value\n",
    "\n",
    "    # Printing and saving classification report\n",
    "    print('Best Threshold: ', micro_thresholds[best_f1_idx])\n",
    "    print('Test F1 Accuracy: ', f1_results[best_f1_idx])\n",
    "    print('Test Flat Accuracy: ', flat_acc_results[best_f1_idx], '\\n')\n",
    "\n",
    "    best_pred_bools = [pl>micro_thresholds[best_f1_idx] for pl in pred_labels]\n",
    "\n",
    "    \n",
    "    val_f1_accuracy = f1_score(true_bools,pred_bools)*100\n",
    "    val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
    "\n",
    "    print('F1 Validation Accuracy: ', val_f1_accuracy)\n",
    "    print('Flat Validation Accuracy: ', val_flat_accuracy)\n",
    "    torch.save(model.state_dict(), f'roberta_lexis_1{epoch_num}_epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c608c2c",
   "metadata": {},
   "source": [
    "# Hallucination Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac77e49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-04 13:57:28.096184: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-04 13:57:28.096208: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                               text  is_correct  \\\n",
      "0           0  В 1929 году Ньюкомб переехал на Мальту в качес...           1   \n",
      "1           1  Вдруг решетка беззвучно поехала в сторону, и н...           1   \n",
      "2           2                       Этим летом не никуда ездили.           0   \n",
      "3           3  В 2011 году был выпущен документальный фильм «...           1   \n",
      "4           4  Поговорительница Пресли поссорилась с поп-звез...           0   \n",
      "\n",
      "  authored_by  error_type         source  \n",
      "0     Machine           0     WikiMatrix  \n",
      "1      Expert           0  Paducheva2004  \n",
      "2      Expert      Syntax        Rusgram  \n",
      "3     Machine           0     WikiMatrix  \n",
      "4     Machine  Morphology     WikiMatrix  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\n",
    "import pickle\n",
    "from transformers import *\n",
    "from tqdm import tqdm, trange\n",
    "from ast import literal_eval\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('./rucola/train.csv')\n",
    "#  './rucola/test.csv'\n",
    "print(train_df.head())\n",
    "device = 'cuda'\n",
    "\n",
    "# train_df = train_df.assign(morphology=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "train_df = train_df.assign(hallucination=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(lexis=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(semantics=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(syntax=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(commonsense=pd.Series(np.zeros(len(train_df), dtype=int)).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41215aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Commonsense', 'Hallucination', 'Morphology', 'Lexis', 'Semantics', 'Syntax', '0'}\n",
      "{'Hallucination'}\n",
      "Label columns:  ['hallucination']\n"
     ]
    }
   ],
   "source": [
    "error_types = set()\n",
    "for _, row in train_df.iterrows():\n",
    "    error_types.update(row[\"error_type\"].split(\"|\"))\n",
    "print(error_types)\n",
    "\n",
    "# ВОТ ЗДЕСЬ ПОМЕНЯТЬ\n",
    "error_types = set([\"Hallucination\"])\n",
    "print(error_types)\n",
    "\n",
    "cols = train_df.columns\n",
    "label_cols = list(cols[6:])\n",
    "num_labels = len(label_cols)\n",
    "print('Label columns: ', label_cols)\n",
    "\n",
    "for i, row in train_df.iterrows():\n",
    "    for error_type in error_types:\n",
    "        if error_type in row[\"error_type\"]:\n",
    "            train_df.at[i, error_type.lower()] = 1\n",
    "\n",
    "# И ВОТ ЗДЕСЬ ЕЩЕ\n",
    "labels = list(train_df.hallucination.values)\n",
    "texts = list(train_df.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c077a5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/vocab.json from cache at /home/bart/.cache/huggingface/transformers/01c769031f99defa27c759f85c0f21715d1506ed5c4ab01f7d7ef8c8b3b5674a.10df436f410c7709f54ad2bf7e973429f03c613012be0df709996d5aef3706af\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/merges.txt from cache at /home/bart/.cache/huggingface/transformers/6d742b9584f250f0088e6a6674f80ae706d8a390d6bc30f7cfaccb4292556f55.e4807b2d97082a2dfbf6623d7aa973a5057f87e44b329ec0b63daafd5d2ef593\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/config.json from cache at /home/bart/.cache/huggingface/transformers/191cf2edd2e8d2007f9a39479b45315ac1b79fe23fa4786511de1c8f9b9def46.ebd71ffa9cb74c7d1d39cf67343e2ae8465e2091f9fb70dbd5dec0cf58b1b60b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"sberbank-ai/ruRoberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer outputs:  dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoTokenizer, RobertaTokenizer\n",
    "\n",
    "\n",
    "# max_length = 100\n",
    "tokenizer = RobertaTokenizer.from_pretrained('sberbank-ai/ruRoberta-large', do_lower_case=False) # tokenizer\n",
    "encodings = tokenizer.batch_encode_plus(texts, padding='longest') # tokenizer's encoding method\n",
    "print('tokenizer outputs: ', encodings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e44cdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df label indices with only one instance:  []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/config.json from cache at /home/bart/.cache/huggingface/transformers/191cf2edd2e8d2007f9a39479b45315ac1b79fe23fa4786511de1c8f9b9def46.ebd71ffa9cb74c7d1d39cf67343e2ae8465e2091f9fb70dbd5dec0cf58b1b60b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/home/jovyan/models/roberta/roberta_l\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/pytorch_model.bin from cache at /home/bart/.cache/huggingface/transformers/57935eccbb92bb4e5465411da844aeb075cac5b8c255c770a81463fa909d8ccc.f03af7b16e88082afbdd696d0647fe6bf624c0776d71dcece2b0e9973d081b26\n",
      "Some weights of the model checkpoint at sberbank-ai/ruRoberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sberbank-ai/ruRoberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/bart/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_ids = encodings['input_ids'] # tokenized and encoded sentences\n",
    "attention_masks = encodings['attention_mask'] # attention masks\n",
    "\n",
    "#ВОТ ЗДЕСЬ ПОМЕНЯТЬ\n",
    "label_counts = train_df.hallucination.astype(str).value_counts()\n",
    "one_freq = label_counts[label_counts==1].keys()\n",
    "one_freq_idxs = sorted(list(train_df[train_df.hallucination.astype(str).isin(one_freq)].index), reverse=True)\n",
    "print('df label indices with only one instance: ', one_freq_idxs)\n",
    "\n",
    "\n",
    "one_freq_input_ids = [input_ids.pop(i) for i in one_freq_idxs]\n",
    "one_freq_attention_masks = [attention_masks.pop(i) for i in one_freq_idxs]\n",
    "one_freq_labels = [labels.pop(i) for i in one_freq_idxs]\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels, attention_masks,\n",
    "        random_state=2020, test_size=0.10, stratify = labels)\n",
    "\n",
    "train_inputs.extend(one_freq_input_ids)\n",
    "train_labels.extend(one_freq_labels)\n",
    "train_masks.extend(one_freq_attention_masks)\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "\n",
    "batch_size = 14\n",
    "\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "torch.save(validation_dataloader, 'validation_data_loader')\n",
    "torch.save(train_dataloader, 'train_data_loader')\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"sberbank-ai/ruRoberta-large\", num_labels=num_labels)\n",
    "model.cuda()\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=5e-7, correct_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85ba0817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                                                                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5556655222399858\n",
      "Eval loss: 0.44424229860305786\n",
      "Best Threshold:  0.6\n",
      "Test F1 Accuracy:  0.4220183486238532\n",
      "Test Flat Accuracy:  0.9512383900928792 \n",
      "\n",
      "F1 Validation Accuracy:  39.726027397260275\n",
      "Flat Validation Accuracy:  93.18885448916409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|████████████▎                                                                                                              | 1/10 [05:19<47:57, 319.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3224711475835577\n",
      "Eval loss: 0.460440069437027\n",
      "Best Threshold:  0.54\n",
      "Test F1 Accuracy:  0.4603174603174603\n",
      "Test Flat Accuracy:  0.9473684210526315 \n",
      "\n",
      "F1 Validation Accuracy:  43.79562043795621\n",
      "Flat Validation Accuracy:  94.04024767801857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|████████████████████████▌                                                                                                  | 2/10 [10:39<42:39, 319.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2530061169555489\n",
      "Eval loss: 0.6055667400360107\n",
      "Best Threshold:  0.4\n",
      "Test F1 Accuracy:  0.46153846153846156\n",
      "Test Flat Accuracy:  0.9512383900928792 \n",
      "\n",
      "F1 Validation Accuracy:  41.584158415841586\n",
      "Flat Validation Accuracy:  95.43343653250774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|████████████████████████████████████▉                                                                                      | 3/10 [15:59<37:20, 320.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.22076432282445788\n",
      "Eval loss: 0.6230612397193909\n",
      "Best Threshold:  0.62\n",
      "Test F1 Accuracy:  0.46153846153846156\n",
      "Test Flat Accuracy:  0.9620743034055728 \n",
      "\n",
      "F1 Validation Accuracy:  45.54455445544554\n",
      "Flat Validation Accuracy:  95.74303405572755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|█████████████████████████████████████████████████▏                                                                         | 4/10 [21:19<32:00, 320.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.19223839076567625\n",
      "Eval loss: 0.5531802773475647\n",
      "Best Threshold:  0.46\n",
      "Test F1 Accuracy:  0.48780487804878053\n",
      "Test Flat Accuracy:  0.9512383900928792 \n",
      "\n",
      "F1 Validation Accuracy:  46.666666666666664\n",
      "Flat Validation Accuracy:  95.04643962848297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████████████████████████████████████████████████████████████▌                                                             | 5/10 [26:40<26:40, 320.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1595919810845735\n",
      "Eval loss: 0.6746562719345093\n",
      "Best Threshold:  0.63\n",
      "Test F1 Accuracy:  0.46808510638297873\n",
      "Test Flat Accuracy:  0.9613003095975232 \n",
      "\n",
      "F1 Validation Accuracy:  45.09803921568628\n",
      "Flat Validation Accuracy:  95.6656346749226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|█████████████████████████████████████████████████████████████████████████▊                                                 | 6/10 [32:00<21:20, 320.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.14764395729687949\n",
      "Eval loss: 0.6718494296073914\n",
      "Best Threshold:  0.22\n",
      "Test F1 Accuracy:  0.48437500000000006\n",
      "Test Flat Accuracy:  0.9489164086687306 \n",
      "\n",
      "F1 Validation Accuracy:  46.0\n",
      "Flat Validation Accuracy:  95.82043343653251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|██████████████████████████████████████████████████████████████████████████████████████                                     | 7/10 [37:20<16:00, 320.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.12502219659785116\n",
      "Eval loss: 0.8787233829498291\n",
      "Best Threshold:  0.26\n",
      "Test F1 Accuracy:  0.47916666666666663\n",
      "Test Flat Accuracy:  0.9613003095975232 \n",
      "\n",
      "F1 Validation Accuracy:  42.35294117647058\n",
      "Flat Validation Accuracy:  96.20743034055728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|██████████████████████████████████████████████████████████████████████████████████████████████████▍                        | 8/10 [43:47<10:56, 328.45s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m train_loss_set\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())    \n\u001b[1;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 28\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     31\u001b[0m nb_tr_examples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m b_input_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/optimization.py:360\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# In-place operations to update the averages at the same time\u001b[39;00m\n\u001b[1;32m    359\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta1))\n\u001b[0;32m--> 360\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    361\u001b[0m denom \u001b[38;5;241m=\u001b[39m exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt()\u001b[38;5;241m.\u001b[39madd_(group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    363\u001b[0m step_size \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss_set = []\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch_num in trange(epochs, desc=\"Epoch\"):\n",
    "    model.train()\n",
    "\n",
    "    tr_loss = 0 \n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "        \n",
    "        loss_func = BCEWithLogitsLoss(pos_weight=torch.tensor([10]).to(device)) \n",
    "        loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels))\n",
    "\n",
    "        train_loss_set.append(loss.item())    \n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "    # Validation\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n",
    "\n",
    "    nb_eval_steps = 0\n",
    "    eval_loss = 0.0\n",
    "    for i, batch in enumerate(validation_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            b_logit_pred = outs[0]\n",
    "            \n",
    "            loss_func = BCEWithLogitsLoss(pos_weight=torch.tensor([10]).to(device)) \n",
    "            loss = loss_func(b_logit_pred.view(-1, num_labels), b_labels.type_as(b_logit_pred).view(-1, num_labels)) #convert labels to float for calculation\n",
    "        \n",
    "\n",
    "            pred_label = torch.sigmoid(b_logit_pred)\n",
    "\n",
    "            b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "            pred_label = pred_label.to('cpu').numpy()\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        nb_eval_steps += 1\n",
    "        eval_loss += loss\n",
    "        tokenized_texts.append(b_input_ids)\n",
    "        logit_preds.append(b_logit_pred)\n",
    "        true_labels.append(b_labels)\n",
    "        pred_labels.append(pred_label)\n",
    "    print(\"Eval loss: {}\".format(eval_loss/nb_eval_steps))\n",
    "        \n",
    "#         print(\"TRUE LABELS\", true_labels)\n",
    "#         print(\"PRED LABELS\", pred_labels)\n",
    "\n",
    "  # Flatten outputs\n",
    "    pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    threshold = 0.50\n",
    "    pred_bools = [pl>threshold for pl in pred_labels]\n",
    "    true_bools = [tl==1 for tl in true_labels]\n",
    "    \n",
    "    macro_thresholds = np.array(range(1,10))/10\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in macro_thresholds:\n",
    "        pred_bools_minmax = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools_minmax)\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools_minmax)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_macro_th = macro_thresholds[np.argmax(f1_results)] #best macro threshold value\n",
    "\n",
    "    micro_thresholds = (np.array(range(10))/100)+best_macro_th #calculating micro threshold values\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in micro_thresholds:\n",
    "        pred_bools_minmax = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools_minmax)\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools_minmax)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_f1_idx = np.argmax(f1_results) #best threshold value\n",
    "\n",
    "    # Printing and saving classification report\n",
    "    print('Best Threshold: ', micro_thresholds[best_f1_idx])\n",
    "    print('Test F1 Accuracy: ', f1_results[best_f1_idx])\n",
    "    print('Test Flat Accuracy: ', flat_acc_results[best_f1_idx], '\\n')\n",
    "\n",
    "    best_pred_bools = [pl>micro_thresholds[best_f1_idx] for pl in pred_labels]\n",
    "\n",
    "    \n",
    "    val_f1_accuracy = f1_score(true_bools,pred_bools)*100\n",
    "    val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
    "\n",
    "    print('F1 Validation Accuracy: ', val_f1_accuracy)\n",
    "    print('Flat Validation Accuracy: ', val_flat_accuracy)\n",
    "    torch.save(model.state_dict(), f'roberta_hallucination_{epoch_num}_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c59eb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                                                                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.9842282147757556\n",
      "Eval loss: 0.749178409576416\n",
      "Best Threshold:  0.71\n",
      "Test F1 Accuracy:  0.3764705882352941\n",
      "Test Flat Accuracy:  0.9589783281733746 \n",
      "\n",
      "F1 Validation Accuracy:  26.373626373626376\n",
      "Flat Validation Accuracy:  84.44272445820434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|████████████▎                                                                                                              | 1/10 [05:19<47:56, 319.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.626151826039907\n",
      "Eval loss: 0.6737252473831177\n",
      "Best Threshold:  0.8200000000000001\n",
      "Test F1 Accuracy:  0.43589743589743596\n",
      "Test Flat Accuracy:  0.9659442724458205 \n",
      "\n",
      "F1 Validation Accuracy:  37.42690058479532\n",
      "Flat Validation Accuracy:  91.71826625386997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|████████████████████████▌                                                                                                  | 2/10 [10:40<42:40, 320.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4413819106880723\n",
      "Eval loss: 0.7963885068893433\n",
      "Best Threshold:  0.8200000000000001\n",
      "Test F1 Accuracy:  0.4418604651162791\n",
      "Test Flat Accuracy:  0.9628482972136223 \n",
      "\n",
      "F1 Validation Accuracy:  38.62068965517241\n",
      "Flat Validation Accuracy:  93.11145510835914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|████████████████████████████████████▉                                                                                      | 3/10 [16:00<37:22, 320.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3498892215686872\n",
      "Eval loss: 0.6847821474075317\n",
      "Best Threshold:  0.81\n",
      "Test F1 Accuracy:  0.46017699115044247\n",
      "Test Flat Accuracy:  0.9527863777089783 \n",
      "\n",
      "F1 Validation Accuracy:  38.20224719101124\n",
      "Flat Validation Accuracy:  91.48606811145511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|█████████████████████████████████████████████████▏                                                                         | 4/10 [21:21<32:02, 320.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2879702949170225\n",
      "Eval loss: 0.9919317364692688\n",
      "Best Threshold:  0.53\n",
      "Test F1 Accuracy:  0.45762711864406785\n",
      "Test Flat Accuracy:  0.9504643962848297 \n",
      "\n",
      "F1 Validation Accuracy:  44.26229508196721\n",
      "Flat Validation Accuracy:  94.73684210526315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████████████████████████████████████████████████████████████▌                                                             | 5/10 [26:41<26:41, 320.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2463386789609809\n",
      "Eval loss: 0.9908134937286377\n",
      "Best Threshold:  0.7\n",
      "Test F1 Accuracy:  0.48543689320388356\n",
      "Test Flat Accuracy:  0.9589783281733746 \n",
      "\n",
      "F1 Validation Accuracy:  44.800000000000004\n",
      "Flat Validation Accuracy:  94.65944272445822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|█████████████████████████████████████████████████████████████████████████▊                                                 | 6/10 [32:01<21:21, 320.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.21234343643664472\n",
      "Eval loss: 0.8788672685623169\n",
      "Best Threshold:  0.9\n",
      "Test F1 Accuracy:  0.5102040816326531\n",
      "Test Flat Accuracy:  0.9628482972136223 \n",
      "\n",
      "F1 Validation Accuracy:  41.29032258064516\n",
      "Flat Validation Accuracy:  92.95665634674923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|██████████████████████████████████████████████████████████████████████████████████████                                     | 7/10 [37:21<16:00, 320.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.17594246025125482\n",
      "Eval loss: 1.308672547340393\n",
      "Best Threshold:  0.7\n",
      "Test F1 Accuracy:  0.49484536082474223\n",
      "Test Flat Accuracy:  0.9620743034055728 \n",
      "\n",
      "F1 Validation Accuracy:  47.27272727272727\n",
      "Flat Validation Accuracy:  95.5108359133127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|██████████████████████████████████████████████████████████████████████████████████████████████████▍                        | 8/10 [42:42<10:40, 320.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.15927849195602828\n",
      "Eval loss: 1.2757447957992554\n",
      "Best Threshold:  0.61\n",
      "Test F1 Accuracy:  0.4954128440366973\n",
      "Test Flat Accuracy:  0.9574303405572755 \n",
      "\n",
      "F1 Validation Accuracy:  47.78761061946902\n",
      "Flat Validation Accuracy:  95.43343653250774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋            | 9/10 [48:02<05:20, 320.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.13070442532645885\n",
      "Eval loss: 1.7178659439086914\n",
      "Best Threshold:  0.18\n",
      "Test F1 Accuracy:  0.5\n",
      "Test Flat Accuracy:  0.9566563467492261 \n",
      "\n",
      "F1 Validation Accuracy:  48.35164835164835\n",
      "Flat Validation Accuracy:  96.36222910216719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [53:23<00:00, 320.32s/it]\n"
     ]
    }
   ],
   "source": [
    "train_loss_set = []\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch_num in trange(epochs, desc=\"Epoch\"):\n",
    "    model.train()\n",
    "\n",
    "    tr_loss = 0 \n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "        \n",
    "        loss_func = BCEWithLogitsLoss(pos_weight=torch.tensor([20]).to(device)) \n",
    "        loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels))\n",
    "\n",
    "        train_loss_set.append(loss.item())    \n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "    # Validation\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n",
    "\n",
    "    nb_eval_steps = 0\n",
    "    eval_loss = 0.0\n",
    "    for i, batch in enumerate(validation_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            b_logit_pred = outs[0]\n",
    "            \n",
    "            loss_func = BCEWithLogitsLoss(pos_weight=torch.tensor([20]).to(device)) \n",
    "            loss = loss_func(b_logit_pred.view(-1, num_labels), b_labels.type_as(b_logit_pred).view(-1, num_labels)) #convert labels to float for calculation\n",
    "        \n",
    "\n",
    "            pred_label = torch.sigmoid(b_logit_pred)\n",
    "\n",
    "            b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "            pred_label = pred_label.to('cpu').numpy()\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        nb_eval_steps += 1\n",
    "        eval_loss += loss\n",
    "        tokenized_texts.append(b_input_ids)\n",
    "        logit_preds.append(b_logit_pred)\n",
    "        true_labels.append(b_labels)\n",
    "        pred_labels.append(pred_label)\n",
    "    print(\"Eval loss: {}\".format(eval_loss/nb_eval_steps))\n",
    "        \n",
    "#         print(\"TRUE LABELS\", true_labels)\n",
    "#         print(\"PRED LABELS\", pred_labels)\n",
    "\n",
    "  # Flatten outputs\n",
    "    pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    threshold = 0.50\n",
    "    pred_bools = [pl>threshold for pl in pred_labels]\n",
    "    true_bools = [tl==1 for tl in true_labels]\n",
    "    \n",
    "    macro_thresholds = np.array(range(1,10))/10\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in macro_thresholds:\n",
    "        pred_bools_minmax = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools_minmax)\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools_minmax)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_macro_th = macro_thresholds[np.argmax(f1_results)] #best macro threshold value\n",
    "\n",
    "    micro_thresholds = (np.array(range(10))/100)+best_macro_th #calculating micro threshold values\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in micro_thresholds:\n",
    "        pred_bools_minmax = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools_minmax)\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools_minmax)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_f1_idx = np.argmax(f1_results) #best threshold value\n",
    "\n",
    "    # Printing and saving classification report\n",
    "    print('Best Threshold: ', micro_thresholds[best_f1_idx])\n",
    "    print('Test F1 Accuracy: ', f1_results[best_f1_idx])\n",
    "    print('Test Flat Accuracy: ', flat_acc_results[best_f1_idx], '\\n')\n",
    "\n",
    "    best_pred_bools = [pl>micro_thresholds[best_f1_idx] for pl in pred_labels]\n",
    "\n",
    "    \n",
    "    val_f1_accuracy = f1_score(true_bools,pred_bools)*100\n",
    "    val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
    "\n",
    "    print('F1 Validation Accuracy: ', val_f1_accuracy)\n",
    "    print('Flat Validation Accuracy: ', val_flat_accuracy)\n",
    "    torch.save(model.state_dict(), f'roberta_hallucination_1{epoch_num}_epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b74198",
   "metadata": {},
   "source": [
    "# Morphology Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6feb4570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                               text  is_correct  \\\n",
      "0           0  В 1929 году Ньюкомб переехал на Мальту в качес...           1   \n",
      "1           1  Вдруг решетка беззвучно поехала в сторону, и н...           1   \n",
      "2           2                       Этим летом не никуда ездили.           0   \n",
      "3           3  В 2011 году был выпущен документальный фильм «...           1   \n",
      "4           4  Поговорительница Пресли поссорилась с поп-звез...           0   \n",
      "\n",
      "  authored_by  error_type         source  \n",
      "0     Machine           0     WikiMatrix  \n",
      "1      Expert           0  Paducheva2004  \n",
      "2      Expert      Syntax        Rusgram  \n",
      "3     Machine           0     WikiMatrix  \n",
      "4     Machine  Morphology     WikiMatrix  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\n",
    "import pickle\n",
    "from transformers import *\n",
    "from tqdm import tqdm, trange\n",
    "from ast import literal_eval\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('./rucola/train.csv')\n",
    "#  './rucola/test.csv'\n",
    "print(train_df.head())\n",
    "device = 'cuda'\n",
    "\n",
    "train_df = train_df.assign(morphology=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(hallucination=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(lexis=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(semantics=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(syntax=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(commonsense=pd.Series(np.zeros(len(train_df), dtype=int)).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be43fba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Commonsense', 'Hallucination', 'Morphology', 'Lexis', 'Semantics', 'Syntax', '0'}\n",
      "{'Morphology'}\n",
      "Label columns:  ['morphology']\n"
     ]
    }
   ],
   "source": [
    "error_types = set()\n",
    "for _, row in train_df.iterrows():\n",
    "    error_types.update(row[\"error_type\"].split(\"|\"))\n",
    "print(error_types)\n",
    "\n",
    "# ВОТ ЗДЕСЬ ПОМЕНЯТЬ\n",
    "error_types = set([\"Morphology\"])\n",
    "print(error_types)\n",
    "\n",
    "cols = train_df.columns\n",
    "label_cols = list(cols[6:])\n",
    "num_labels = len(label_cols)\n",
    "print('Label columns: ', label_cols)\n",
    "\n",
    "for i, row in train_df.iterrows():\n",
    "    for error_type in error_types:\n",
    "        if error_type in row[\"error_type\"]:\n",
    "            train_df.at[i, error_type.lower()] = 1\n",
    "\n",
    "# И ВОТ ЗДЕСЬ ЕЩЕ\n",
    "labels = list(train_df.morphology.values)\n",
    "texts = list(train_df.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2431325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/vocab.json from cache at /home/bart/.cache/huggingface/transformers/01c769031f99defa27c759f85c0f21715d1506ed5c4ab01f7d7ef8c8b3b5674a.10df436f410c7709f54ad2bf7e973429f03c613012be0df709996d5aef3706af\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/merges.txt from cache at /home/bart/.cache/huggingface/transformers/6d742b9584f250f0088e6a6674f80ae706d8a390d6bc30f7cfaccb4292556f55.e4807b2d97082a2dfbf6623d7aa973a5057f87e44b329ec0b63daafd5d2ef593\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/config.json from cache at /home/bart/.cache/huggingface/transformers/191cf2edd2e8d2007f9a39479b45315ac1b79fe23fa4786511de1c8f9b9def46.ebd71ffa9cb74c7d1d39cf67343e2ae8465e2091f9fb70dbd5dec0cf58b1b60b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"sberbank-ai/ruRoberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer outputs:  dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoTokenizer, RobertaTokenizer\n",
    "\n",
    "\n",
    "# max_length = 100\n",
    "tokenizer = RobertaTokenizer.from_pretrained('sberbank-ai/ruRoberta-large', do_lower_case=False) # tokenizer\n",
    "encodings = tokenizer.batch_encode_plus(texts, padding='longest') # tokenizer's encoding method\n",
    "print('tokenizer outputs: ', encodings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2983a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df label indices with only one instance:  []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/config.json from cache at /home/bart/.cache/huggingface/transformers/191cf2edd2e8d2007f9a39479b45315ac1b79fe23fa4786511de1c8f9b9def46.ebd71ffa9cb74c7d1d39cf67343e2ae8465e2091f9fb70dbd5dec0cf58b1b60b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/home/jovyan/models/roberta/roberta_l\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/pytorch_model.bin from cache at /home/bart/.cache/huggingface/transformers/57935eccbb92bb4e5465411da844aeb075cac5b8c255c770a81463fa909d8ccc.f03af7b16e88082afbdd696d0647fe6bf624c0776d71dcece2b0e9973d081b26\n",
      "Some weights of the model checkpoint at sberbank-ai/ruRoberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sberbank-ai/ruRoberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/bart/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_ids = encodings['input_ids'] # tokenized and encoded sentences\n",
    "attention_masks = encodings['attention_mask'] # attention masks\n",
    "\n",
    "#ВОТ ЗДЕСЬ ПОМЕНЯТЬ\n",
    "label_counts = train_df.morphology.astype(str).value_counts()\n",
    "one_freq = label_counts[label_counts==1].keys()\n",
    "one_freq_idxs = sorted(list(train_df[train_df.morphology.astype(str).isin(one_freq)].index), reverse=True)\n",
    "print('df label indices with only one instance: ', one_freq_idxs)\n",
    "\n",
    "\n",
    "one_freq_input_ids = [input_ids.pop(i) for i in one_freq_idxs]\n",
    "one_freq_attention_masks = [attention_masks.pop(i) for i in one_freq_idxs]\n",
    "one_freq_labels = [labels.pop(i) for i in one_freq_idxs]\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels, attention_masks,\n",
    "        random_state=2020, test_size=0.10, stratify = labels)\n",
    "\n",
    "train_inputs.extend(one_freq_input_ids)\n",
    "train_labels.extend(one_freq_labels)\n",
    "train_masks.extend(one_freq_attention_masks)\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "\n",
    "batch_size = 14\n",
    "\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "torch.save(validation_dataloader, 'validation_data_loader')\n",
    "torch.save(train_dataloader, 'train_data_loader')\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"sberbank-ai/ruRoberta-large\", num_labels=num_labels)\n",
    "model.cuda()\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=5e-7, correct_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe8e9dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                                                                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.1662155651228523\n",
      "Eval loss: 1.1254162788391113\n",
      "Best Threshold:  0.58\n",
      "Test F1 Accuracy:  0.21674876847290642\n",
      "Test Flat Accuracy:  0.8769349845201239 \n",
      "\n",
      "F1 Validation Accuracy:  19.047619047619047\n",
      "Flat Validation Accuracy:  75.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|████████████▎                                                                                                              | 1/10 [05:11<46:44, 311.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.0220938740224184\n",
      "Eval loss: 1.0764213800430298\n",
      "Best Threshold:  0.54\n",
      "Test F1 Accuracy:  0.2734375\n",
      "Test Flat Accuracy:  0.8560371517027864 \n",
      "\n",
      "F1 Validation Accuracy:  26.537216828478964\n",
      "Flat Validation Accuracy:  82.43034055727554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|████████████████████████▌                                                                                                  | 2/10 [10:24<41:38, 312.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.8854018874403659\n",
      "Eval loss: 1.1322410106658936\n",
      "Best Threshold:  0.45\n",
      "Test F1 Accuracy:  0.2867383512544803\n",
      "Test Flat Accuracy:  0.8459752321981424 \n",
      "\n",
      "F1 Validation Accuracy:  25.941422594142256\n",
      "Flat Validation Accuracy:  86.30030959752321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|████████████████████████████████████▉                                                                                      | 3/10 [15:37<36:29, 312.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.8047839317618725\n",
      "Eval loss: 1.146601676940918\n",
      "Best Threshold:  0.42000000000000004\n",
      "Test F1 Accuracy:  0.2857142857142857\n",
      "Test Flat Accuracy:  0.8297213622291022 \n",
      "\n",
      "F1 Validation Accuracy:  26.923076923076923\n",
      "Flat Validation Accuracy:  85.29411764705883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|█████████████████████████████████████████████████▏                                                                         | 4/10 [20:51<31:18, 313.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.7222959090172169\n",
      "Eval loss: 1.3375475406646729\n",
      "Best Threshold:  0.8\n",
      "Test F1 Accuracy:  0.2956521739130435\n",
      "Test Flat Accuracy:  0.9373065015479877 \n",
      "\n",
      "F1 Validation Accuracy:  27.90697674418604\n",
      "Flat Validation Accuracy:  88.0030959752322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████████████████████████████████████████████████████████████▌                                                             | 5/10 [26:05<26:07, 313.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6499784738184026\n",
      "Eval loss: 1.5414661169052124\n",
      "Best Threshold:  0.41000000000000003\n",
      "Test F1 Accuracy:  0.30331753554502366\n",
      "Test Flat Accuracy:  0.8862229102167183 \n",
      "\n",
      "F1 Validation Accuracy:  27.77777777777778\n",
      "Flat Validation Accuracy:  89.93808049535603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|█████████████████████████████████████████████████████████████████████████▊                                                 | 6/10 [31:20<20:55, 313.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6004356919293584\n",
      "Eval loss: 1.4632333517074585\n",
      "Best Threshold:  0.64\n",
      "Test F1 Accuracy:  0.313953488372093\n",
      "Test Flat Accuracy:  0.9086687306501547 \n",
      "\n",
      "F1 Validation Accuracy:  30.3921568627451\n",
      "Flat Validation Accuracy:  89.00928792569658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|██████████████████████████████████████████████████████████████████████████████████████                                     | 7/10 [36:33<15:41, 313.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5405267878232701\n",
      "Eval loss: 1.530588984489441\n",
      "Best Threshold:  0.52\n",
      "Test F1 Accuracy:  0.3054187192118227\n",
      "Test Flat Accuracy:  0.8908668730650154 \n",
      "\n",
      "F1 Validation Accuracy:  30.476190476190478\n",
      "Flat Validation Accuracy:  88.69969040247679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|██████████████████████████████████████████████████████████████████████████████████████████████████▍                        | 8/10 [45:42<11:25, 342.87s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(b_input_ids, token_type_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, attention_mask\u001b[38;5;241m=\u001b[39mb_input_mask)\n\u001b[1;32m     19\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 21\u001b[0m loss_func \u001b[38;5;241m=\u001b[39m BCEWithLogitsLoss(pos_weight\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m) \n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,num_labels),b_labels\u001b[38;5;241m.\u001b[39mtype_as(logits)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,num_labels))\n\u001b[1;32m     24\u001b[0m train_loss_set\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())    \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss_set = []\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch_num in trange(epochs, desc=\"Epoch\"):\n",
    "    model.train()\n",
    "\n",
    "    tr_loss = 0 \n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "        \n",
    "        loss_func = BCEWithLogitsLoss(pos_weight=torch.tensor([15]).to(device)) \n",
    "        loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels))\n",
    "\n",
    "        train_loss_set.append(loss.item())    \n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "    # Validation\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n",
    "\n",
    "    nb_eval_steps = 0\n",
    "    eval_loss = 0.0\n",
    "    for i, batch in enumerate(validation_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            b_logit_pred = outs[0]\n",
    "            \n",
    "            loss_func = BCEWithLogitsLoss(pos_weight=torch.tensor([15]).to(device)) \n",
    "            loss = loss_func(b_logit_pred.view(-1, num_labels), b_labels.type_as(b_logit_pred).view(-1, num_labels)) #convert labels to float for calculation\n",
    "        \n",
    "\n",
    "            pred_label = torch.sigmoid(b_logit_pred)\n",
    "\n",
    "            b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "            pred_label = pred_label.to('cpu').numpy()\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        nb_eval_steps += 1\n",
    "        eval_loss += loss\n",
    "        tokenized_texts.append(b_input_ids)\n",
    "        logit_preds.append(b_logit_pred)\n",
    "        true_labels.append(b_labels)\n",
    "        pred_labels.append(pred_label)\n",
    "    print(\"Eval loss: {}\".format(eval_loss/nb_eval_steps))\n",
    "        \n",
    "#         print(\"TRUE LABELS\", true_labels)\n",
    "#         print(\"PRED LABELS\", pred_labels)\n",
    "\n",
    "  # Flatten outputs\n",
    "    pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    threshold = 0.50\n",
    "    pred_bools = [pl>threshold for pl in pred_labels]\n",
    "    true_bools = [tl==1 for tl in true_labels]\n",
    "    \n",
    "    macro_thresholds = np.array(range(1,10))/10\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in macro_thresholds:\n",
    "        pred_bools_minmax = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools_minmax)\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools_minmax)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_macro_th = macro_thresholds[np.argmax(f1_results)] #best macro threshold value\n",
    "\n",
    "    micro_thresholds = (np.array(range(10))/100)+best_macro_th #calculating micro threshold values\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in micro_thresholds:\n",
    "        pred_bools_minmax = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools_minmax)\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools_minmax)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_f1_idx = np.argmax(f1_results) #best threshold value\n",
    "\n",
    "    # Printing and saving classification report\n",
    "    print('Best Threshold: ', micro_thresholds[best_f1_idx])\n",
    "    print('Test F1 Accuracy: ', f1_results[best_f1_idx])\n",
    "    print('Test Flat Accuracy: ', flat_acc_results[best_f1_idx], '\\n')\n",
    "\n",
    "    best_pred_bools = [pl>micro_thresholds[best_f1_idx] for pl in pred_labels]\n",
    "\n",
    "    \n",
    "    val_f1_accuracy = f1_score(true_bools,pred_bools)*100\n",
    "    val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
    "\n",
    "    print('F1 Validation Accuracy: ', val_f1_accuracy)\n",
    "    print('Flat Validation Accuracy: ', val_flat_accuracy)\n",
    "    torch.save(model.state_dict(), f'roberta_morphology_{epoch_num}_epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a290458",
   "metadata": {},
   "source": [
    "# Commonsense classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aaefe7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-04 20:56:04.906356: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-04 20:56:04.906378: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                               text  is_correct  \\\n",
      "0           0  В 1929 году Ньюкомб переехал на Мальту в качес...           1   \n",
      "1           1  Вдруг решетка беззвучно поехала в сторону, и н...           1   \n",
      "2           2                       Этим летом не никуда ездили.           0   \n",
      "3           3  В 2011 году был выпущен документальный фильм «...           1   \n",
      "4           4  Поговорительница Пресли поссорилась с поп-звез...           0   \n",
      "\n",
      "  authored_by  error_type         source  \n",
      "0     Machine           0     WikiMatrix  \n",
      "1      Expert           0  Paducheva2004  \n",
      "2      Expert      Syntax        Rusgram  \n",
      "3     Machine           0     WikiMatrix  \n",
      "4     Machine  Morphology     WikiMatrix  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\n",
    "import pickle\n",
    "from transformers import *\n",
    "from tqdm import tqdm, trange\n",
    "from ast import literal_eval\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('./rucola/train.csv')\n",
    "#  './rucola/test.csv'\n",
    "print(train_df.head())\n",
    "device = 'cuda'\n",
    "\n",
    "# train_df = train_df.assign(morphology=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(hallucination=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(lexis=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(semantics=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "# train_df = train_df.assign(syntax=pd.Series(np.zeros(len(train_df), dtype=int)).values)\n",
    "train_df = train_df.assign(commonsense=pd.Series(np.zeros(len(train_df), dtype=int)).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d79ba80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Morphology', 'Lexis', 'Commonsense', 'Semantics', '0', 'Hallucination', 'Syntax'}\n",
      "{'Commonsense'}\n",
      "Label columns:  ['commonsense']\n"
     ]
    }
   ],
   "source": [
    "error_types = set()\n",
    "for _, row in train_df.iterrows():\n",
    "    error_types.update(row[\"error_type\"].split(\"|\"))\n",
    "print(error_types)\n",
    "\n",
    "# ВОТ ЗДЕСЬ ПОМЕНЯТЬ\n",
    "error_types = set([\"Commonsense\"])\n",
    "print(error_types)\n",
    "\n",
    "cols = train_df.columns\n",
    "label_cols = list(cols[6:])\n",
    "num_labels = len(label_cols)\n",
    "print('Label columns: ', label_cols)\n",
    "\n",
    "for i, row in train_df.iterrows():\n",
    "    for error_type in error_types:\n",
    "        if error_type in row[\"error_type\"]:\n",
    "            train_df.at[i, error_type.lower()] = 1\n",
    "\n",
    "# И ВОТ ЗДЕСЬ ЕЩЕ\n",
    "labels = list(train_df.commonsense.values)\n",
    "texts = list(train_df.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47b889f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/vocab.json from cache at /home/bart/.cache/huggingface/transformers/01c769031f99defa27c759f85c0f21715d1506ed5c4ab01f7d7ef8c8b3b5674a.10df436f410c7709f54ad2bf7e973429f03c613012be0df709996d5aef3706af\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/merges.txt from cache at /home/bart/.cache/huggingface/transformers/6d742b9584f250f0088e6a6674f80ae706d8a390d6bc30f7cfaccb4292556f55.e4807b2d97082a2dfbf6623d7aa973a5057f87e44b329ec0b63daafd5d2ef593\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/config.json from cache at /home/bart/.cache/huggingface/transformers/191cf2edd2e8d2007f9a39479b45315ac1b79fe23fa4786511de1c8f9b9def46.ebd71ffa9cb74c7d1d39cf67343e2ae8465e2091f9fb70dbd5dec0cf58b1b60b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"sberbank-ai/ruRoberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer outputs:  dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoTokenizer, RobertaTokenizer\n",
    "\n",
    "\n",
    "# max_length = 100\n",
    "tokenizer = RobertaTokenizer.from_pretrained('sberbank-ai/ruRoberta-large', do_lower_case=False) # tokenizer\n",
    "encodings = tokenizer.batch_encode_plus(texts, padding='longest') # tokenizer's encoding method\n",
    "print('tokenizer outputs: ', encodings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e3567fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df label indices with only one instance:  []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/config.json from cache at /home/bart/.cache/huggingface/transformers/191cf2edd2e8d2007f9a39479b45315ac1b79fe23fa4786511de1c8f9b9def46.ebd71ffa9cb74c7d1d39cf67343e2ae8465e2091f9fb70dbd5dec0cf58b1b60b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/home/jovyan/models/roberta/roberta_l\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/sberbank-ai/ruRoberta-large/resolve/main/pytorch_model.bin from cache at /home/bart/.cache/huggingface/transformers/57935eccbb92bb4e5465411da844aeb075cac5b8c255c770a81463fa909d8ccc.f03af7b16e88082afbdd696d0647fe6bf624c0776d71dcece2b0e9973d081b26\n",
      "Some weights of the model checkpoint at sberbank-ai/ruRoberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sberbank-ai/ruRoberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/bart/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_ids = encodings['input_ids'] # tokenized and encoded sentences\n",
    "attention_masks = encodings['attention_mask'] # attention masks\n",
    "\n",
    "#ВОТ ЗДЕСЬ ПОМЕНЯТЬ\n",
    "label_counts = train_df.commonsense.astype(str).value_counts()\n",
    "one_freq = label_counts[label_counts==1].keys()\n",
    "one_freq_idxs = sorted(list(train_df[train_df.commonsense.astype(str).isin(one_freq)].index), reverse=True)\n",
    "print('df label indices with only one instance: ', one_freq_idxs)\n",
    "\n",
    "\n",
    "one_freq_input_ids = [input_ids.pop(i) for i in one_freq_idxs]\n",
    "one_freq_attention_masks = [attention_masks.pop(i) for i in one_freq_idxs]\n",
    "one_freq_labels = [labels.pop(i) for i in one_freq_idxs]\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels, attention_masks,\n",
    "        random_state=2020, test_size=0.10, stratify = labels)\n",
    "\n",
    "train_inputs.extend(one_freq_input_ids)\n",
    "train_labels.extend(one_freq_labels)\n",
    "train_masks.extend(one_freq_attention_masks)\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "\n",
    "batch_size = 14\n",
    "\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "torch.save(validation_dataloader, 'validation_data_loader')\n",
    "torch.save(train_dataloader, 'train_data_loader')\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"sberbank-ai/ruRoberta-large\", num_labels=num_labels)\n",
    "model.cuda()\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=5e-7, correct_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e450e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                                                                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4265627301287995\n",
      "Eval loss: 0.40069252252578735\n",
      "Best Threshold:  0.32\n",
      "Test F1 Accuracy:  0.15384615384615388\n",
      "Test Flat Accuracy:  0.9744582043343654 \n",
      "\n",
      "F1 Validation Accuracy:  0.0\n",
      "Flat Validation Accuracy:  98.9938080495356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|████████████▎                                                                                                              | 1/10 [05:19<47:58, 319.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3595287222078991\n",
      "Eval loss: 0.4038943946361542\n",
      "Best Threshold:  0.45\n",
      "Test F1 Accuracy:  0.14814814814814817\n",
      "Test Flat Accuracy:  0.9821981424148607 \n",
      "\n",
      "F1 Validation Accuracy:  0.0\n",
      "Flat Validation Accuracy:  98.68421052631578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|████████████████████████▌                                                                                                  | 2/10 [10:40<42:41, 320.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3235697337257016\n",
      "Eval loss: 0.41697126626968384\n",
      "Best Threshold:  0.5\n",
      "Test F1 Accuracy:  0.13333333333333333\n",
      "Test Flat Accuracy:  0.9798761609907121 \n",
      "\n",
      "F1 Validation Accuracy:  13.333333333333334\n",
      "Flat Validation Accuracy:  97.9876160990712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|████████████████████████████████████▉                                                                                      | 3/10 [16:00<37:22, 320.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.28331246433687696\n",
      "Eval loss: 0.41090700030326843\n",
      "Best Threshold:  0.54\n",
      "Test F1 Accuracy:  0.17647058823529413\n",
      "Test Flat Accuracy:  0.978328173374613 \n",
      "\n",
      "F1 Validation Accuracy:  12.765957446808512\n",
      "Flat Validation Accuracy:  96.8266253869969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|█████████████████████████████████████████████████▏                                                                         | 4/10 [21:21<32:03, 320.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2523229755640532\n",
      "Eval loss: 0.43971630930900574\n",
      "Best Threshold:  0.51\n",
      "Test F1 Accuracy:  0.20689655172413793\n",
      "Test Flat Accuracy:  0.9821981424148607 \n",
      "\n",
      "F1 Validation Accuracy:  18.749999999999996\n",
      "Flat Validation Accuracy:  97.9876160990712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████████████████████████████████████████████████████████████▌                                                             | 5/10 [26:42<26:43, 320.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.21281405958256244\n",
      "Eval loss: 0.476386159658432\n",
      "Best Threshold:  0.58\n",
      "Test F1 Accuracy:  0.20689655172413793\n",
      "Test Flat Accuracy:  0.9821981424148607 \n",
      "\n",
      "F1 Validation Accuracy:  15.789473684210526\n",
      "Flat Validation Accuracy:  97.52321981424149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|█████████████████████████████████████████████████████████████████████████▊                                                 | 6/10 [32:03<21:23, 320.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1707890409657958\n",
      "Eval loss: 0.5824521780014038\n",
      "Best Threshold:  0.43000000000000005\n",
      "Test F1 Accuracy:  0.18749999999999997\n",
      "Test Flat Accuracy:  0.9798761609907121 \n",
      "\n",
      "F1 Validation Accuracy:  7.142857142857144\n",
      "Flat Validation Accuracy:  97.9876160990712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|██████████████████████████████████████████████████████████████████████████████████████                                     | 7/10 [37:24<16:02, 320.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.13665501320696516\n",
      "Eval loss: 0.5502740740776062\n",
      "Best Threshold:  0.5\n",
      "Test F1 Accuracy:  0.1714285714285714\n",
      "Test Flat Accuracy:  0.9775541795665634 \n",
      "\n",
      "F1 Validation Accuracy:  17.14285714285714\n",
      "Flat Validation Accuracy:  97.75541795665634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|██████████████████████████████████████████████████████████████████████████████████████████████████▍                        | 8/10 [42:45<10:41, 320.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10688691922697709\n",
      "Eval loss: 0.5780386328697205\n",
      "Best Threshold:  0.6\n",
      "Test F1 Accuracy:  0.1714285714285714\n",
      "Test Flat Accuracy:  0.9775541795665634 \n",
      "\n",
      "F1 Validation Accuracy:  15.0\n",
      "Flat Validation Accuracy:  97.36842105263158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋            | 9/10 [48:06<05:20, 320.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09386448997771087\n",
      "Eval loss: 0.6618751883506775\n",
      "Best Threshold:  0.32999999999999996\n",
      "Test F1 Accuracy:  0.14634146341463414\n",
      "Test Flat Accuracy:  0.9729102167182663 \n",
      "\n",
      "F1 Validation Accuracy:  6.666666666666667\n",
      "Flat Validation Accuracy:  97.8328173374613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [53:27<00:00, 320.72s/it]\n"
     ]
    }
   ],
   "source": [
    "train_loss_set = []\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch_num in trange(epochs, desc=\"Epoch\"):\n",
    "    model.train()\n",
    "\n",
    "    tr_loss = 0 \n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "        \n",
    "        loss_func = BCEWithLogitsLoss(pos_weight=torch.tensor([15]).to(device)) \n",
    "        loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels))\n",
    "\n",
    "        train_loss_set.append(loss.item())    \n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "    # Validation\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n",
    "\n",
    "    nb_eval_steps = 0\n",
    "    eval_loss = 0.0\n",
    "    for i, batch in enumerate(validation_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            b_logit_pred = outs[0]\n",
    "            \n",
    "            loss_func = BCEWithLogitsLoss(pos_weight=torch.tensor([15]).to(device)) \n",
    "            loss = loss_func(b_logit_pred.view(-1, num_labels), b_labels.type_as(b_logit_pred).view(-1, num_labels)) #convert labels to float for calculation\n",
    "        \n",
    "\n",
    "            pred_label = torch.sigmoid(b_logit_pred)\n",
    "\n",
    "            b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "            pred_label = pred_label.to('cpu').numpy()\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        nb_eval_steps += 1\n",
    "        eval_loss += loss\n",
    "        tokenized_texts.append(b_input_ids)\n",
    "        logit_preds.append(b_logit_pred)\n",
    "        true_labels.append(b_labels)\n",
    "        pred_labels.append(pred_label)\n",
    "    print(\"Eval loss: {}\".format(eval_loss/nb_eval_steps))\n",
    "    \n",
    "    pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "    threshold = 0.50\n",
    "    pred_bools = [pl>threshold for pl in pred_labels]\n",
    "    true_bools = [tl==1 for tl in true_labels]\n",
    "    \n",
    "    macro_thresholds = np.array(range(1,10))/10\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in macro_thresholds:\n",
    "        pred_bools_minmax = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools_minmax)\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools_minmax)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_macro_th = macro_thresholds[np.argmax(f1_results)] #best macro threshold value\n",
    "\n",
    "    micro_thresholds = (np.array(range(10))/100)+best_macro_th #calculating micro threshold values\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in micro_thresholds:\n",
    "        pred_bools_minmax = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools_minmax)\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools_minmax)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_f1_idx = np.argmax(f1_results) #best threshold value\n",
    "\n",
    "    print('Best Threshold: ', micro_thresholds[best_f1_idx])\n",
    "    print('Test F1 Accuracy: ', f1_results[best_f1_idx])\n",
    "    print('Test Flat Accuracy: ', flat_acc_results[best_f1_idx], '\\n')\n",
    "\n",
    "    best_pred_bools = [pl>micro_thresholds[best_f1_idx] for pl in pred_labels]\n",
    "\n",
    "    val_f1_accuracy = f1_score(true_bools,pred_bools)*100\n",
    "    val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
    "\n",
    "    print('F1 Validation Accuracy: ', val_f1_accuracy)\n",
    "    print('Flat Validation Accuracy: ', val_flat_accuracy)\n",
    "    torch.save(model.state_dict(), f'roberta_commonsense_{epoch_num}_epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d14d00f",
   "metadata": {},
   "source": [
    "# Fine tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54545d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e3740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cointegrated/rubert-tiny2\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b264bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels, attention_masks,\n",
    "        random_state=2020, test_size=0.10, stratify = labels)\n",
    "\n",
    "# Add one frequency data to train data\n",
    "train_inputs.extend(one_freq_input_ids)\n",
    "train_labels.extend(one_freq_labels)\n",
    "train_masks.extend(one_freq_attention_masks)\n",
    "\n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fa82d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
